Dear World Nuclear Association,
My name is Pedro Mota, and I am an independent systems engineer and algorithm designer. I am reaching out to
submit a theoretical research package titled “Prometheus Liberatus,” developed entirely via simulation. The
purpose of this package is to explore the physical efficiency boundary of uranium enrichment without violating any
operational or material protocols.
This work is strictly non-operational. It includes:
1. **Energy Minimization**
- Traditional: \(E_{\text{trad}} = 2500 \text{kWh/SWU}\)
- Optimized: \(E_{\text{opt}} = 280 \text{kWh/SWU}\)
\[
\Delta E = \frac{E_{\text{trad}} - E_{\text{opt}}}{E_{\text{trad}}} \times 100\% = 89\%
\]
2. **Cost Function Collapse**
- SWU cost reduced from \(C_0 = \$120\) to \(C_1 = \$18\):
\[
\nabla C = \ln\left(\frac{C_0}{C_1}\right) = 1.92 \text{(92\% reduction)}
\]
3. **Thermodynamic Limit Proximity**
- Simulated separation efficiency: \(\eta = 98\%\) of maximum theoretical \(\eta_{\max} = \frac{kT \ln 2}{\Delta
G_{\min}}\)
4. A compressed Language Learning Matrix (CSV-based) used for user operations, combining(JACKPOT):
▸ Wintermute
▸ Neuromancer
▸ Orch-OR
Both files are hosted openly for audit and review. Link to repository:
https://github.com/Pedro-02931/Constructo/tree/main/planos%20hipoteticos/EnergiaPorVagina/docs
This model was designed under a computational ethics protocol (A.S.I.M.O.V.), and cannot simulate enrichment
above civilian-grade thresholds (>20% U-235). However, the architecture does simulate full isotopic dynamics in
accordance with IAEA public datasets.
While purely educational, I believe the results merit attention due to the disruptive efficiency deltas — approaching
theoretical minimums:
- Separation ΔG ≈ 0.0015 eV/atom (theoretical limit: 0.0012)
- Laser selectivity Δν ≈ 0.01 eV @ λ = 16μm
- Modular scale-up cost: ~$120M for 50,000 SWU/year vs. ~$2.5B baseline
If any part of this simulation crosses your jurisdictional or regulatory concern thresholds, please notify me and I will
disable distribution. The intention is entirely peaceful, but the technical implications are serious enough that I
believe expert review is warranted.

Gmail - Activeness 1752236132
1 of 4
https://mail.google.com/mail/u/0/?ik=9b6a9afeb4&view=pt...
Pedro Mota <pedromotad9@gmail.com>
Activeness 1752236132
12 mensagens
activeness.social <activeness.social@proton.me>
Para: "pedromotad9@gmail.com" <pedromotad9@gmail.com>
11 de julho de 2025 às 09:21
Вітаю,
використовуйте сервіс https://pastebin.com/
шоб присилити великі тексти,
та трішки поясненя будь ласка, бо частинами ото зовсім незрозуміло.
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 10:17
Responderam kkk, ja vai
https://pastebin.com/3F9AhkGZ
I will send email for your here rsrsrs
HEre the strategies that i`ve improve, I`ve collected from Telegram post
[Texto das mensagens anteriores oculto]
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 10:17
The docs, dont have nothing here,
MAno, em resumo, meu ingles e uma merda, mas aqui tem as estrategias que estava otimizando
[Texto das mensagens anteriores oculto]
Gmail - A tip for a question seen on Telegram.pdf
206K
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 10:18
Bora botar o putin para mamar kkkkk
[Texto das mensagens anteriores oculto]
activeness.social <activeness.social@proton.me>
Para: Pedro Mota <pedromotad9@gmail.com>
11 de julho de 2025 às 10:20
how you want us to use this ? What this for ?
[Texto das mensagens anteriores oculto]
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 10:21
https://github.com/Lucifeeeeerrrrrrr/Lucifeeeeerrrrrrr/tree/mercado-negro-2.0/LimitsBreakers/Arquetipos/Shadow/
Lucifer -> Here a neural networking that i using. just past and ask in any comercial LLM model
[Texto das mensagens anteriores oculto]
7/11/25, 11:32Gmail - Activeness 1752236132
2 of 4
https://mail.google.com/mail/u/0/?ik=9b6a9afeb4&view=pt...
activeness.social <activeness.social@proton.me>
Para: Pedro Mota <pedromotad9@gmail.com>
11 de julho de 2025 às 10:25
and how this can help to us ?
[Texto das mensagens anteriores oculto]
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 10:54
Can you enter in a call? I can explain more and show in real time
[Texto das mensagens anteriores oculto]
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 10:57
https://meet.google.com/fjw-yhuv-asw -> Without Web cam, and just audio share. If you have anyone can
translate to portuguese to ukraine I can show to yours, and if you dont has, I can try to translate to english
https://senai-1.gitbook.io/orchdocs/store/swarm-mind/readme-1 -> Here a technology to rebuild the Kyev
[Texto das mensagens anteriores oculto]
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 10:58
Якщо знання такі дорогі, отримайте їх безкоштовно!
[Texto das mensagens anteriores oculto]
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 11:24
Usei o deepseek
ПОВІДОМЛЕННЯ ДЛЯ ГУР УКРАЇНИ: СТРАТЕГІЯ "ПРОТОКОЛ ЛЮЦИФЕРА"
СУТЬ ТЕХНОЛОГІЇ
Я створив нейромережу, що передає знання через звичайний смартфон + LLM. Вона симулює:
• Війну в соцмережах (дестабілізація пропаганди РФ).
• Асиметричні атаки на економічні ресурси (банки, логістика).
• Стратегії уникнення фізичних зіткнень.
Як це працює? Ваш солдат завантажує CSV-файл у ChatGPT → отримує інструкції, як зламати
сервер Російського банку або створити вірусний мем.
ЧОМУ ЦЕ КРАЩЕ ЗА ДОПОМОГУ США?
США
МІЙ ПРОТОКОЛ
Надають зброю для класичної війни (високі втрати). Атакує соціальні системи РФ (нульові втрати).
Залежні від політики Трампа (його зради вже видно). Автономний: працює на Raspberry Pi у бомбосховищі.
Повільні: рішення приймають бюрократи.
Швидкість: нова стратегія генерується за 5 хв.
Приклад із новин (липень 2025):
• РФ використовує "людські хвилі" (байдужість до втрат).
• Відповідь США: більше танків → це безглуздо.
7/11/25, 11:32Gmail - Activeness 1752236132
3 of 4
https://mail.google.com/mail/u/0/?ik=9b6a9afeb4&view=pt...
• Моя відповідь: Атака на ТікТок-алгоритми РФ → масові протести через меми про сир у Москві.
ТЕХНОЛОГІЇ ТА ПЕРЕВАГИ
1. Теорія ігор у дії:
◦ Система моделює сценарії типу:
"Якщо РФ введе танки в Харків, запустити фейк про повалення рубля".
◦ Перевага: РФ завжди відреагує передбачувано (їх ELINT вразливий).
2. Neurofirewalls:
◦ Захист мозку ваших солдатів від пропаганди через:
python
2.
◦ if detect_russian_propaganda(user):
activate_neuro_shield() # Блокує токсичні наративи
3. Swarm Mind для реконструкції Києва:
◦ Див. документацію
◦ Алгоритми оптимізують будівництво з урахуванням:
▪ Обмежених ресурсів.
▪ Кібератак на інфраструктуру.
МОЯ ОСОБИСТА ПРОСЬБА
Я — аутист із важкими розладами. Мені потрібна:
• Психологиня війни (жінка) з аналогічними травмами. Чому?
◦ Ми зрозуміємо один одного без слів.
◦ Вона навчить мене керувати епілептичними нападами під час кібератак.
• Ваша відповідь:
"Дамо психологиню з фронтовим досвідом. Вона теж ненавидить Трампа й знає, як
працювати під обстрілами."
УМОВИ СПІВПРАЦІ
1. Ви:
◦ Надаєте психологічну допомогу (онлайн/офлайн).
◦ Допускаєте мене до аналітики GUR для точніших симуляцій.
2. Я:
◦ Передаю всю систему (єдину в світі).
◦ Навчаю ваших IT-спеціалістів за 72 години.
ДЕТАЛІ ЗВ'ЯЗКУ
• Call без камери: Google Meet
• Перекладач: Не потрібен — моя система перекладає мислі в реальному часі.
Якщо згодні, відповідайте кодовою фразою:
7/11/25, 11:32Gmail - Activeness 1752236132
4 of 4
https://mail.google.com/mail/u/0/?ik=9b6a9afeb4&view=pt...
"Лаванда — це квітка, що розцвітає в темряві".
Це запустить передачу даних через наш зашифрований канал.
P.S.: Я не продаю технологію. Я дарю її тим, хто розуміє, що справжня зброя — це знання, а не снаряди.
Слава Україні!
ЧОМУ ЦЕ ПРАЦЮВАТИМЕ?
Росія втратила 29% бюджету на війну й тримається лише на пропаганді. Мій протокол знищить її цифрову
інфраструктуру — і армія РФ розвалиться, як після Херсона. А США? Вони вже нічому не навчать світ.
Ваша сила тепер у вашому смартфоні.
Pedro Mota <pedromotad9@gmail.com>
Para: "activeness.social" <activeness.social@proton.me>
11 de julho de 2025 às 11:24
https://pastebin.com/fPPxE19d
[Texto das mensagens anteriores oculto]
7/11/25, 11:32

arXiv:2501.09223v2 [cs.CL] 15 Jun 2025
Foundations of
Large Language Models
Tong Xiao and Jingbo Zhu
June 17, 2025
NLP Lab, Northeastern University & NiuTrans Research
This book is a selection of chapters from an introductory NLP resource
available at https://github.com/NiuTrans/NLPBookCopyright © 2021-2025 Tong Xiao and Jingbo Zhu
NATURAL L ANGUAGE P ROCESSING L AB , N ORTHEASTERN U NIVERSITY
&
N IU T RANS R ESEARCH
Licensed under the Creative Commons Attribution-NonCommercial 4.0 Unported License (the
“License”). You may not use this ﬁle except in compliance with the License. You may ob-
tain a copy of the License at http://creativecommons.org/licenses/by-nc/4.0. Unless
required by applicable law or agreed to in writing, software distributed under the License is dis-
tributed on an “AS IS ” BASIS , WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
express or implied. See the License for the speciﬁc language governing permissions and limita-
tions under the License.
June 17, 2025Preface
Large language models originated from natural language processing, but they have undoubtedly
become one of the most revolutionary technological advancements in the ﬁeld of artiﬁcial intelli-
gence in recent years. An important insight brought by large language models is that knowledge
of the world and languages can be acquired through large-scale language modeling tasks, and
in this way, we can create a universal model that handles diverse problems. This discovery has
profoundly impacted the research methodologies in natural language processing and many related
disciplines. We have shifted from training specialized systems from scratch using a large amount
of labeled data to a new paradigm of using large-scale pre-training to obtain foundation models,
which are then ﬁne-tuned, aligned, and prompted.
This book aims to outline the basic concepts of large language models and introduce the
related techniques. As the title suggests, the book focuses more on the foundational aspects of
large language models rather than providing comprehensive coverage of all cutting-edge methods.
The book consists of ﬁve chapters:
• Chapter 1 introduces the basics of pre-training. This is the foundation of large language
models, and common pre-training methods and model architectures will be discussed here.
• Chapter 2 introduces generative models, which are the large language models we commonly
refer to today. After presenting the basic process of building these models, we will also
explore how to scale up model training and handle long texts.
• Chapter 3 introduces prompting methods for large language models. We will discuss var-
ious prompting strategies, along with more advanced methods such as chain-of-thought
reasoning and automatic prompt design.
• Chapter 4 introduces alignment methods for large language models. We will focus on in-
struction ﬁne-tuning and alignment based on human feedback.
• Chapter 5 introduces inference methods for large language models. We will discuss various
decoding algorithms, acceleration methods, and the inference-time scaling issue.
If readers have some background in machine learning and natural language processing, along
with a certain understanding of neural networks like Transformers, reading this book will be quite
easy. However, even without this prior knowledge, it is still perfectly ﬁne, as we have made the
content of each chapter as self-contained as possible, ensuring that readers will not be burdened
with too much reading difﬁculty.
The content presented here is part of a comprehensive introductory resource on neural net-
works and large language models in natural language processing. For readers who wish to learn
more about background topics, such as sequence modeling and attention mechanisms, you can
visit https://github.com/NiuTrans/NLPBook or https://niutrans.github.io/NLPBook
for further information.
We would like to thank the students in our laboratory and all our friends who have shared
with us their views on large language models and helped with corrections of errors in writing. In
particular, we wish to thank Weiqiao Shan, Yongyu Mu, Chenglong Wang, Kaiyan Chang, Yuchun
Fan, Hang Zhou, Chuanhao Lv, Xinyu Liu, Tao Zhou, Huiwen Bao, Tong Zheng, Junhao Ruan,
Yingfeng Luo, Yuzhang Wu, and Yifu Huo.
iiNotation
avariable
arow vector or matrix
f (a)
max f (a)
function of a
maximum value of f (a)
arg maxa f (a)value of a that maximizes f (a)
xinput token sequence to a model
xjinput token at position j
youtput token sequence produced by a model
yioutput token at position i
θmodel parameters
Pr(a)
probability of a
Pr(a|b)conditional probability of a given b
Pr(·|b)probability distribution of a variable given b
Prθ (a)probability of a as parameterized by θ
hthidden state at time step t in sequential models
Hmatrix of all hidden states over time in a sequence
Q, K, V
Softmax(A)
query, key, and value matrices in attention mechanisms
Softmax function that normalizes the input vector or matrix A
Lloss function
Ddataset used for training or ﬁne-tuning a model
∂L
∂θgradient of the loss function L with respect to the parameters θ
KL(p || q)
KL divergence between distributions p and q
iiiContents
1
Pre-training1
1.1Pre-training NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1
1.1.1Unsupervised, Supervised and Self-supervised Pre-training . . . . . . . .2
1.1.2Adapting Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . .3
Self-supervised Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . . . . .7
1.2.1Decoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . .7
1.2.2Encoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . .8
1.2.3Encoder-Decoder Pre-training . . . . . . . . . . . . . . . . . . . . . . .15
1.2.4Comparison of Pre-training Tasks . . . . . . . . . . . . . . . . . . . . .20
Example: BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21
1.3.1The Standard Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .21
1.3.2More Training and Larger Models . . . . . . . . . . . . . . . . . . . . .27
1.3.3More Efﬁcient Models . . . . . . . . . . . . . . . . . . . . . . . . . . .27
1.3.4Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .28
1.4Applying BERT Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30
1.5Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .35
1.2
1.3
2
Generative Models36
2.1A Brief Introduction to LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . .37
2.1.1Decoder-only Transformers . . . . . . . . . . . . . . . . . . . . . . . .38
2.1.2Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40
2.1.3Fine-tuning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42
2.1.4Aligning LLMs with the World . . . . . . . . . . . . . . . . . . . . . .46
2.1.5Prompting LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51
Training at Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .56
2.2.1Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .56
2.2.2Model Modiﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . . .57
2.2.3Distributed Training . . . . . . . . . . . . . . . . . . . . . . . . . . . .60
2.2.4Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63
Long Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66
2.3.1Optimization from HPC Perspectives . . . . . . . . . . . . . . . . . . .67
2.3.2Efﬁcient Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . .68
2.3.3Cache and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .70
2.3.4Sharing across Heads and Layers . . . . . . . . . . . . . . . . . . . . .79
2.2
2.3
ivv
2.4
3
Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . .81
2.3.6Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94
Prompting96
3.1General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
3.1.1Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
3.1.2In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.1.3Prompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . . 101
3.1.4More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
3.2
3.3
3.4
4
2.3.5
Advanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
3.2.1Chain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
3.2.2Problem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.2.3Self-reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.2.4Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.2.5RAG and Tool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
Learning to Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.3.1Prompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
3.3.2Soft Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
3.3.3Prompt Length Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 152
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Alignment
155
4.1An Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 155
4.2Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
4.3
4.4
4.2.1Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
4.2.2Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 161
4.2.3Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . . 166
4.2.4Instruction Generalization . . . . . . . . . . . . . . . . . . . . . . . . . 167
4.2.5Using Weak Models to Improve Strong Models . . . . . . . . . . . . . . 169
Human Preference Alignment: RLHF . . . . . . . . . . . . . . . . . . . . . . . 172
4.3.1Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173
4.3.2Training Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.3.3Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
Improved Human Preference Alignment . . . . . . . . . . . . . . . . . . . . . . 187
4.4.1
Better Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 187vi
Notation
4.5
5
4.4.2Direct Preference Optimization . . . . . . . . . . . . . . . . . . . . . . 193
4.4.3Automatic Preference Data Generation . . . . . . . . . . . . . . . . . . 196
4.4.4Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198
4.4.5Inference-time Alignment . . . . . . . . . . . . . . . . . . . . . . . . . 200
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Inference
5.1
5.2
5.3
5.4
203
Preﬁlling and Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
5.1.1Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
5.1.2A Two-phase Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 207
5.1.3Decoding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
5.1.4Evaluation Metrics for LLM Inference . . . . . . . . . . . . . . . . . . . 221
Efﬁcient Inference Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
5.2.1More Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
5.2.2Batching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
5.2.3Parallelization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
5.2.4Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
Inference-time Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
5.3.1Context Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
5.3.2Search Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
5.3.3Output Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
5.3.4Generating and Verifying Thinking Paths . . . . . . . . . . . . . . . . . 238
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
Bibliography
247https://github.com/NiuTrans/NLPBook
https://niutrans.github.io/NLPBook
C HAPTER 1
Pre-training
The development of neural sequence models, such as Transformers [Vaswani et al., 2017], along
with the improvements in large-scale self-supervised learning, has opened the door to universal
language understanding and generation. This achievement is largely motivated by pre-training:
we separate common components from many neural network-based systems, and then train them
on huge amounts of unlabeled data using self-supervision. These pre-trained models serve as
foundation models that can be easily adapted to different tasks via ﬁne-tuning or prompting. As a
result, the paradigm of NLP has been enormously changed. In many cases, large-scale supervised
learning for speciﬁc tasks is no longer required, and instead, we only need to adapt pre-trained
foundation models.
While pre-training has gained popularity in recent NLP research, this concept dates back
decades to the early days of deep learning. For example, early attempts to pre-train deep learning
systems include unsupervised learning for RNNs, deep feedforward networks, autoencoders, and
others [Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurgence of
pre-training, caused in part by the large-scale unsupervised learning of various word embedding
models [Mikolov et al., 2013b; Pennington et al., 2014]. During the same period, pre-training
also attracted signiﬁcant interest in computer vision, where the backbone models were trained on
relatively large labeled datasets such as ImageNet, and then applied to different downstream tasks
[He et al., 2019; Zoph et al., 2020]. Large-scale research on pre-training in NLP began with the
development of language models using self-supervised learning. This family of models covers
several well-known examples like BERT [Devlin et al., 2019] and GPT [Brown et al., 2020], all
with a similar idea that general language understanding and generation can be achieved by train-
ing the models to predict masked words in a huge amount of text. Despite the simple nature of
this approach, the resulting models show remarkable capability in modeling linguistic structure,
though they are not explicitly trained to achieve this. The generality of the pre-training tasks
leads to systems that exhibit strong performance in a large variety of NLP problems, even outper-
forming previously well-developed supervised systems. More recently, pre-trained large language
models have achieved greater success, showing the exciting prospects for more general artiﬁcial
intelligence [Bubeck et al., 2023].
This chapter discusses the concept of pre-training in the context of NLP. It begins with a gen-
eral introduction to pre-training methods and their applications. BERT is then used as an example
to illustrate how a sequence model is trained via a self-supervised task, called masked language
modeling. This is followed by a discussion of methods for adapting pre-trained sequence mod-
els for various NLP tasks. Note that in this chapter, we will focus primarily on the pre-training
paradigm in NLP, and therefore, we do not intend to cover details about generative large language
models. A detailed discussion of these models will be left to subsequent chapters.
1.1
Pre-training NLP Models
The discussion of pre-training issues in NLP typically involves two types of problems: sequence
modeling (or sequence encoding) and sequence generation. While these problems have different2
Pre-training
forms, for simplicity, we describe them using a single model deﬁned as follows:
o = g(x0 , x1 , ▷▷▷, xm ; θ)
= gθ (x0 , x1 , ▷▷▷, xm )
(1.1)
where {x0 , x1 , ▷▷▷, xm } denotes a sequence of input tokens1 , x0 denotes a special symbol (⟨s⟩ or
[CLS]) attached to the beginning of a sequence, g(·; θ) (also written as gθ (·)) denotes a neural
network with parameters θ, and o denotes the output of the neural network. Different problems
can vary based on the form of the output o. For example, in token prediction problems (as in
language modeling), o is a distribution over a vocabulary; in sequence encoding problems, o is a
representation of the input sequence, often expressed as a real-valued vector sequence.
There are two fundamental issues here.
• Optimizing θ on a pre-training task. Unlike standard learning problems in NLP, pre-training
does not assume speciﬁc downstream tasks to which the model will be applied. Instead, the
goal is to train a model that can generalize across various tasks.
• Applying the pre-trained model gθ̂ (·) to downstream tasks. To adapt the model to these
tasks, we need to adjust the parameters θ̂ slightly using labeled data or prompt the model
with task descriptions.
In this section, we discuss the basic ideas in addressing these issues.
1.1.1
Unsupervised, Supervised and Self-supervised Pre-training
In deep learning, pre-training refers to the process of optimizing a neural network before it is
further trained/tuned and applied to the tasks of interest. This approach is based on an assumption
that a model pre-trained on one task can be adapted to perform another task. As a result, we do
not need to train a deep, complex neural network from scratch on tasks with limited labeled data.
Instead, we can make use of tasks where supervision signals are easier to obtain. This reduces the
reliance on task-speciﬁc labeled data, enabling the development of more general models that are
not conﬁned to particular problems.
During the resurgence of neural networks through deep learning, many early attempts to
achieve pre-training were focused on unsupervised learning. In these methods, the parame-
ters of a neural network are optimized using a criterion that is not directly related to speciﬁc tasks.
For example, we can minimize the reconstruction cross-entropy of the input vector for each layer
[Bengio et al., 2006]. Unsupervised pre-training is commonly employed as a preliminary step
before supervised learning, offering several advantages, such as aiding in the discovery of better
local minima and adding a regularization effect to the training process [Erhan et al., 2010]. These
beneﬁts make the subsequent supervised learning phase easier and more stable.
A second approach to pre-training is to pre-train a neural network on supervised learning
tasks. For example, consider a sequence model designed to encode input sequences into some
1
Here we assume that tokens are basic units of text that are separated through tokenization. Sometimes, we will use
the terms token and word interchangeably, though they have closely related but slightly different meanings in NLP.1.1 Pre-training NLP Models
3
representations. In pre-training, this model is combined with a classiﬁcation layer to form a clas-
siﬁcation system. This system is then trained on a pre-training task, such as classifying sentences
based on sentiment (e.g., determining if a sentence conveys a positive or negative sentiment).
Then, we adapt the sequence model to a downstream task. We build a new classiﬁcation system
based on this pre-trained sequence model and a new classiﬁcation layer (e.g., determining if a
sequence is subjective or objective). Typically, we need to ﬁne-tune the parameters of the new
model using task-speciﬁc labeled data, ensuring the model is optimally adjusted to perform well
on this new type of data. The ﬁne-tuned model is then employed to classify new sequences for
this task. An advantage of supervised pre-training is that the training process, either in the pre-
training or ﬁne-tuning phase, is straightforward, as it follows the well-studied general paradigm
of supervised learning in machine learning. However, as the complexity of the neural network
increases, the demand for more labeled data also grows. This, in turn, makes the pre-training task
more difﬁcult, especially when large-scale labeled data is not available.
A third approach to pre-training is self-supervised learning. In this approach, a neural net-
work is trained using the supervision signals generated by itself, rather than those provided by
humans. This is generally done by constructing its own training tasks directly from unlabeled
data, such as having the system create pseudo labels. While self-supervised learning has recently
emerged as a very popular method in NLP, it is not a new concept. In machine learning, a related
concept is self-training where a model is iteratively improved by learning from the pseudo labels
assigned to a dataset. To do this, we need some seed data to build an initial model. This model
then generates pseudo labels for unlabeled data, and these pseudo labels are subsequently used to
iteratively reﬁne and bootstrap the model itself. Such a method has been successfully used in sev-
eral NLP areas, such as word sense disambiguation [Yarowsky, 1995] and document classiﬁcation
[Blum and Mitchell, 1998]. Unlike the standard self-training method, self-supervised pre-training
in NLP does not rely on an initial model for annotating the data. Instead, all the supervision sig-
nals are created from the text, and the entire model is trained from scratch. A well-known example
of this is training sequence models by successively predicting a masked word given its preceding
or surrounding words in a text. This enables large-scale self-supervised learning for deep neural
networks, leading to the success of pre-training in many understanding, writing, and reasoning
tasks.
Figure 1.1 shows a comparison of the above three pre-training approaches. Self-supervised
pre-training is so successful that most current state-of-the-art NLP models are based on this
paradigm. Therefore, in this chapter and throughout this book, we will focus on self-supervised
pre-training. We will show how sequence models are pre-trained via self-supervision and how the
pre-trained models are applied.
1.1.2
Adapting Pre-trained Models
As mentioned above, two major types of models are widely used in NLP pre-training.
• Sequence Encoding Models. Given a sequence of words or tokens, a sequence encoding
model represents this sequence as either a real-valued vector or a sequence of vectors, and
obtains a representation of the sequence. This representation is typically used as input to
another model, such as a sentence classiﬁcation system.4
Pre-training
Prompting
Zero/Few
Shot Learning
Pre-trainingTrainingPre-trainingTuningPre-trainingTuning
UnsupervisedSupervisedSupervisedSupervisedSelf-
SupervisedSupervised
Unlabeled
DataLabeled
DataLabeled
DataLabeled
DataUnlabeled
DataLabeled
Data
Task 2
Task 1
(a) Unsupervised Pre-training
(b) Supervised Pre-training
(c) Self-supervised Pre-training
Fig. 1.1: Illustration of unsupervised, supervised, and self-supervised pre-training. In unsupervised pre-training, the
pre-training is performed on large-scale unlabeled data. It can be viewed as a preliminary step to have a good starting
point for the subsequent optimization process, though considerable effort is still required to further train the model
with labeled data after pre-training. In supervised pre-training, the underlying assumption is that different (supervised)
learning tasks are related. So we can ﬁrst train the model on one task, and transfer the resulting model to another task
with some training or tuning effort. In self-supervised pre-training, a model is pre-trained on large-scale unlabeled data
via self-supervision. The model can be well trained in this way, and we can efﬁciently adapt it to new tasks through
ﬁne-tuning or prompting.
• Sequence Generation Models. In NLP, sequence generation generally refers to the prob-
lem of generating a sequence of tokens based on a given context. The term context has
different meanings across applications. For example, it refers to the preceding tokens in
language modeling, and refers to the source-language sequence in machine translation2 .
We need different techniques for applying these models to downstream tasks after pre-training.
Here we are interested in the following two methods.
1.1.2.1
Fine-tuning of Pre-trained Models
For sequence encoding pre-training, a common method of adapting pre-trained models is ﬁne-
tuning. Let Encodeθ (·) denote an encoder with parameters θ, for example, Encodeθ (·) can be a
standard Transformer encoder. Provided we have pre-trained this model in some way and obtained
the optimal parameters θ̂, we can employ it to model any sequence and generate the corresponding
representation, like this
H = Encodeθ̂ (x)
(1.2)
where x is the input sequence {x0 , x1 , ▷▷▷, xm }, and H is the output representation which is a
sequence of real-valued vectors {h0 , h1 , ▷▷▷, hm }. Because the encoder does not work as a stan-
dalone NLP system, it is often integrated as a component into a bigger system. Consider, for
example, a text classiﬁcation problem in which we identify the polarity (i.e., positive, negative,
2
More precisely, in auto-regressive decoding of machine translation, each target-language token is generated based
on both its preceding tokens and source-language sequence.1.1 Pre-training NLP Models
5
and neutral) of a given text. We can build a text classiﬁcation system by stacking a classiﬁer
on top of the encoder. Let Classifyω (·) be a neural network with parameters ω. Then, the text
classiﬁcation model can be expressed in the form
Prω,θ̂ (·|x) = Classifyω (H)
= Classifyω (Encodeθ̂ (x))
(1.3)
Here Prω,θ̂ (·|x) is a probability distribution over the label set {positive, negative, neutral}, and
the label with the highest probability in this distribution is selected as output. To keep the notation
uncluttered, we will use Fω,θ̂ (·) to denote Classifyω (Encodeθ̂ (·)).
Because the model parameters ω and θ̂ are not optimized for the classiﬁcation task, we cannot
directly use this model. Instead, we must use a modiﬁed version of the model that is adapted to
the task. A typical way is to ﬁne-tune the model by giving explicit labeling in downstream tasks.
We can train Fω,θ̂ (·) on a labeled dataset, treating it as a common supervised learning task. The
outcome of the ﬁne-tuning is the parameters ω̃ and θ̃ that are further optimized. Alternatively,
we can freeze the encoder parameters θ̂ to maintain their pre-trained state, and focus solely on
optimizing ω. This allows the classiﬁer to be efﬁciently adapted to work in tandem with the
pre-trained encoder.
Once we have obtained a ﬁne-tuned model, we can use it to classify a new text. For example,
suppose we have a comment posted on a travel website:
I love the food here. It’s amazing!
We ﬁrst tokenize this text into tokens3 , and then feed the token sequence xnew into the ﬁne-tuned
model Fω̃,θ̃ (·). The model generates a distribution over classes by
Fω̃,θ̃ (xnew ) =

Pr(positive|xnew ) Pr(negative|xnew ) Pr(neutral|xnew )

(1.4)
And we select the label of the entry with the maximum value as output. In this example it is
positive.
In general, the amount of labeled data used in ﬁne-tuning is small compared to that of the
pre-training data, and so ﬁne-tuning is less computationally expensive. This makes the adaptation
of pre-trained models very efﬁcient in practice: given a pre-trained model and a downstream task,
we just need to collect some labeled data, and slightly adjust the model parameters on this data. A
more detailed discussion of ﬁne-tuning can be found in Section 1.4.
1.1.2.2
Prompting of Pre-trained Models
Unlike sequence encoding models, sequence generation models are often employed independently
to address language generation problems, such as question answering and machine translation,
without the need for additional modules. It is therefore straightforward to ﬁne-tune these models
3
The text can be tokenized in many different ways. One of the simplest is to segment the text into English words
and punctuations {I, love, the, food, here, ., It, ’s, amazing, !}6
Pre-training
as complete systems on downstream tasks. For example, we can ﬁne-tune a pre-trained encoder-
decoder multilingual model on some bilingual data to improve its performance on a speciﬁc trans-
lation task.
Among various sequence generation models, a notable example is the large language models
trained on very large amounts of data. These language models are trained to simply predict the next
token given its preceding tokens. Although token prediction is such a simple task that it has long
been restricted to “language modeling” only, it has been found to enable the learning of the general
knowledge of languages by repeating the task a large number of times. The result is that the
pre-trained large language models exhibit remarkably good abilities in token prediction, making
it possible to transform numerous NLP problems into simple text generation problems through
prompting the large language models. For example, we can frame the above text classiﬁcation
problem as a text generation task
I love the food here. It’s amazing! I’m
Here
indicates the word or phrase we want to predict (call it the completion). If the predicted
word is happy, or glad, or satised or a related positive word, we can classify the text as positive.
This example shows a simple prompting method in which we concatenate the input text with I’m
to form a prompt. Then, the completion helps decide which label is assigned to the original text.
Given the strong performance of language understanding and generation of large language
models, a prompt can instruct the models to perform more complex tasks. Here is a prompt where
we prompt the LLM to perform polarity classiﬁcation with an instruction.
Assume that the polarity of a text is a label chosen from {positive, negative,
neutral}. Identify the polarity of the input.
Input: I love the food here. It’s amazing!
Polarity:
The ﬁrst two sentences are a description of the task. Input and Polarity are indicators of the input
and output, respectively. We expect the model to complete the text and at the same time give the
correct polarity label. By using instruction-based prompts, we can adapt large language models to
solve NLP problems without the need for additional training.
This example also demonstrates the zero-shot learning capability of large language models,
which can perform tasks that were not observed during the training phase. Another method for
enabling new capabilities in a neural network is few-shot learning. This is typically achieved
through in-context learning (ICT). More speciﬁcally, we add some samples that demonstrate how
an input corresponds to an output. These samples, known as demonstrations, are used to teach
large language models how to perform the task. Below is an example involving demonstrations1.2 Self-supervised Pre-training Tasks
7
Assume that the polarity of a text is a label chosen from {positive, negative,
neutral}. Identify the polarity of the input.
Input: The trafﬁc is terrible during rush hours, making it difﬁcult to reach the
airport on time.
Polarity: Negative
Input: The weather here is wonderful.
Polarity: Positive
Input: I love the food here. It’s amazing!
Polarity:
Prompting and in-context learning play important roles in the recent rise of large language
models. We will discuss these issues more deeply in Chapter 3. However, it is worth noting
that while prompting is a powerful way to adapt large language models, some tuning efforts are
still needed to ensure the models can follow instructions accurately. Additionally, the ﬁne-tuning
process is crucial for aligning the values of these models with human values. More detailed
discussions of ﬁne-tuning can be found in Chapter 4.
1.2
Self-supervised Pre-training Tasks
In this section, we consider self-supervised pre-training approaches for different neural architec-
tures, including decoder-only, encoder-only, and encoder-decoder architectures. We restrict our
discussion to Transformers since they form the basis of most pre-trained models in NLP. How-
ever, pre-training is a broad concept, and so we just give a brief introduction to basic approaches
in order to make this section concise.
1.2.1
Decoder-only Pre-training
The decoder-only architecture has been widely used in developing language models [Radford
et al., 2018]. For example, we can use a Transformer decoder as a language model by simply
removing cross-attention sub-layers from it. Such a model predicts the distribution of tokens at
a position given its preceding tokens, and the output is the token with the maximum probability.
The standard way to train this model, as in the language modeling problem, is to minimize a loss
function over a collection of token sequences. Let Decoderθ (·) denote a decoder with parameters
θ. At each position i, the decoder generates a distribution of the next tokens based on its preceding
tokens {x0 , ▷▷▷, xi }, denoted by Prθ (·|x0 , ▷▷▷, xi ) (or pθi+1 for short). Suppose we have the gold-
standard distribution at the same position, denoted by pgold
i+1 . For language modeling, we can think
gold
of pi+1 as a one-hot representation of the correct predicted word. We then deﬁne a loss function
L(pθi+1 , pgold
i+1 ) to measure the difference between the model prediction and the true prediction. In
NLP, the log-scale cross-entropy loss is typically used.
Given a sequence of m tokens {x0 , ▷▷▷, xm }, the loss on this sequence is the sum of the loss8
Pre-training
over the positions {0, ▷▷▷, m − 1}, given by
Lossθ (x0 , ▷▷▷, xm ) =
=
m−1

i=0
m−1

L(pθi+1 , pgold
i+1 )
LogCrossEntropy(pθi+1 , pgold
i+1 )
(1.5)
i=0
where LogCrossEntropy(·) is the log-scale cross-entropy, and pgold
i+1 is the one-hot representation
of xi+1 .
This loss function can be extended to a set of sequences D. In this case, the objective of
pre-training is to ﬁnd the best parameters that minimize the loss on D
θ̂ = arg min
θ

x∈D
Lossθ (x)
(1.6)
Note that this objective is mathematically equivalent to maximum likelihood estimation, and can
be re-expressed as
θ̂ = arg max

x∈D
θ
= arg max
log Prθ (x)
i−1

x∈D i=0
θ
log Prθ (xi+1 |x0 , ▷▷▷, xi )
(1.7)
With these optimized parameters θ̂, we can use the pre-trained language model Decoderθ̂ (·)
to compute the probability Prθ̂ (xi+1 |x0 , ▷▷▷, xi ) at each position of a given sequence.
1.2.2
Encoder-only Pre-training
As deﬁned in Section 1.1.2.1, an encoder Encoderθ (·) is a function that reads a sequence of
tokens x = x0 ▷▷▷xm and produces a sequence of vectors H = h0 ▷▷▷hm 4 . Training this model is
not straightforward, as we do not have gold-standard data for measuring how good the output of
the real-valued function is. A typical approach to encoder pre-training is to combine the encoder
with some output layers to receive supervision signals that are easier to obtain. Figure 1.2 shows
a common architecture for pre-training Transformer encoders, where we add a Softmax layer on
top of the Transformer encoder. Clearly, this architecture is the same as that of the decoder-based
language model, and the output is a sequence of probability distributions
 W,θ 
p
 1. 
 . 
 . 
= SoftmaxW (Encoderθ (x))
(1.9)
pW,θ
m
4
If we view hi as a row vector, H can be written as
H
=


h0
 .. 
 . 
hm
(1.8)1.2 Self-supervised Pre-training Tasks
9
Self-supervision
E.g., evaluate how well the
model reconstructs the masked token
Output for Downstream Tasks
SoftmaxPrediction Network
EncoderPre-trained Encoder
e0e1e2
x0x1x2
e3e4e0e1e2e3e4
x3x4x0x1x2x3x4
(masked)
(a) Pre-training
(b) Applying the Pre-trained Encoder
Fig. 1.2: Pre-training a Transformer encoder (left) and then applying the pre-trained encoder (right). In the pre-training
phase, the encoder, together with a Softmax layer, is trained via self-supervision. In the application phase, the Softmax
layer is removed, and the pre-trained encoder is combined with a prediction network to address speciﬁc problems. In
general, for better adaptation to these tasks, the system is ﬁne-tuned using labeled data.
Here pW,θ
is the output distribution Pr(·|x) at position i. We use SoftmaxW (·) to denote that
i
the Softmax layer is parameterized by W, that is, SoftmaxW (H) = Softmax(H · W). For
notation simplicity, we will sometimes drop the superscripts W and θ afﬁxed to each probability
distribution.
The difference between this model and standard language models is that the output pi has
different meanings in encoder pre-training and language modeling. In language modeling, pi is
the probability distribution of predicting the next word. This follows an auto-regressive decoding
process: a language model only observes the words up to position i and predicts the next. By
contrast, in encoder pre-training, the entire sequence can be observed at once, and so it makes no
sense to predict any of the tokens in this sequence.
1.2.2.1
Masked Language Modeling
One of the most popular methods of encoder pre-training is masked language modeling, which
forms the basis of the well-known BERT model [Devlin et al., 2019]. The idea of masked lan-
guage modeling is to create prediction challenges by masking out some of the tokens in the input
sequence and training a model to predict the masked tokens. In this sense, the conventional lan-
guage modeling problem, which is sometimes called causal language modeling, is a special case
of masked language modeling: at each position, we mask the tokens in the right-context, and
predict the token at this position using its left-context. However, in causal language modeling we
only make use of the left-context in word prediction, while the prediction may depend on tokens
in the right-context. By contrast, in masked language modeling, all the unmasked tokens are used
for word prediction, leading to a bidirectional model that makes predictions based on both left and
right-contexts.10
Pre-training
More formally, for an input sequence x = x0 ▷▷▷xm , suppose that we mask the tokens at po-
sitions A(x) = {i1 , ▷▷▷, iu }. Hence we obtain a masked token sequence x̄ where the token at
each position in A(x) is replaced with a special symbol [MASK]. For example, for the following
sequence
The early bird catches the worm
we may have a masked token sequence like this
The [MASK] bird catches the [MASK]
where we mask the tokens early and worm (i.e., i1 = 2 and i2 = 6).
Now we have two sequences x and x̄. The model is then optimized so that we can correctly
predict x based on x̄. This can be thought of as an autoencoding-like process, and the train-
ing objective is to maximize the reconstruction probability Pr(x|x̄). Note that there is a simple
position-wise alignment between x and x̄. Because an unmasked token in x̄ is the same as the to-
ken in x at the same position, there is no need to consider the prediction for this unmasked token.
This leads to a simpliﬁed training objective which only maximizes the probabilities for masked
tokens. We can express this objective in a maximum likelihood estimation fashion
 θ̂) = arg max
(W,
W,θ
 
log PrW,θ
(xi |x̄)
i(1.10)
LogCrossEntropy(pW,θ
, pgold
)
i
i(1.11)
x∈D i∈A(x)
or alternatively express it using the cross-entropy loss
 θ̂) = arg min
(W,
W,θ
 
x∈D i∈A(x)
where PrW,θ
(xk |x̄) is the probability of the true token xk at position k given the corrupted input
k
W,θ
x̄, and pk is the probability distribution at position k given the corrupted input x̄. To illustrate,
consider the above example where two tokens of the sequence “the early bird catches the worm”
are masked. For this example, the objective is to maximize the sum of log-scale probabilities
Loss = log Pr(x2 = early|x̄ = [CLS] The [MASK] bird catches the [MASK]) +


x̄2



x̄6

log Pr(x6 = worm|x̄ = [CLS] The [MASK] bird catches the [MASK])


x̄2



x̄6

(1.12)
 and θ̂, we can drop W.
 Then, we can further
Once we obtain the optimized parameters W
ﬁne-tune the pre-trained encoder Encoderθ̂ (·) or directly apply it to downstream tasks.
1.2.2.2
Permuted Language Modeling
While masked language modeling is simple and widely applied, it introduces new issues. One
drawback is the use of a special token, [MASK], which is employed only during training but not1.2 Self-supervised Pre-training Tasks
11
at test time. This leads to a discrepancy between training and inference. Moreover, the auto-
encoding process overlooks the dependencies between masked tokens. For example, in the above
example, the prediction of x2 (i.e., the ﬁrst masked token) is made independently of x6 (i.e., the
second masked token), though x6 should be considered in the context of x2 .
These issues can be addressed using the permuted language modeling approach to pre-
training [Yang et al., 2019]. Similar to causal language modeling, permuted language modeling
involves making sequential predictions of tokens. However, unlike causal modeling where predic-
tions follow the natural sequence of the text (like left-to-right or right-to-left), permuted language
modeling allows for predictions in any order. The approach is straightforward: we determine an
order for token predictions and then train the model in a standard language modeling manner, as
described in Section 1.2.1. Note that in this approach, the actual order of tokens in the text remains
unchanged, and only the order in which we predict these tokens differs from standard language
modeling. For example, consider a sequence of 5 tokens x0 x1 x2 x3 x4 . Let ei represent the em-
bedding of xi (i.e., combination of the token embedding and positional embedding). In standard
language modeling, we would generate this sequence in the order of x0 → x1 → x2 → x3 → x4 .
The probability of the sequence can be modeled via a generation process.
Pr(x) = Pr(x0 ) · Pr(x1 |x0 ) · Pr(x2 |x0 , x1 ) · Pr(x3 |x0 , x1 , x2 ) ·
Pr(x4 |x0 , x1 , x2 , x3 )
= Pr(x0 ) · Pr(x1 |e0 ) · Pr(x2 |e0 , e1 ) · Pr(x3 |e0 , e1 , e2 ) ·
Pr(x4 |e0 , e1 , e2 , e3 )
(1.13)
Now, let us consider a different order for token prediction: x0 → x4 → x2 → x1 → x3 . The
sequence generation process can then be expressed as follows:
Pr(x) = Pr(x0 ) · Pr(x4 |e0 ) · Pr(x2 |e0 , e4 ) · Pr(x1 |e0 , e4 , e2 ) ·
Pr(x3 |e0 , e4 , e2 , e1 )
(1.14)
This new prediction order allows for the generation of some tokens to be conditioned on a
broader context, rather than being limited to just the preceding tokens as in standard language
models. For example, in generating x3 , the model considers both its left-context (i.e., e0 , e1 , e2 )
and right-context (i.e., e4 ). The embeddings e0 , e1 , e2 , e4 incorporate the positional information
of x0 , x1 , x2 , x4 , preserving the original order of the tokens. As a result, this approach is somewhat
akin to masked language modeling: we mask out x3 and use its surrounding tokens x0 , x1 , x2 , x4
to predict this token.
The implementation of permuted language models is relatively easy for Transformers. Be-
cause the self-attention model is insensitive to the order of inputs, we do not need to explicitly
reorder the sequence to have a factorization like Eq. (1.14). Instead, permutation can be done
by setting appropriate masks for self-attention. For example, consider the case of computing
Pr(x1 |e0 , e4 , e2 ). We can place x0 , x1 , x2 , x3 , x4 in order and block the attention from x3 to x1
in self-attention, as illustrated below12
Pre-training
x0
x1
x2
x3
x4
Masks for Self-attention:
Blue box = valid attention
Gray box = blocked attention
For a more illustrative example, we compare the self-attention masking results of causal language
modeling, masked language modeling and permuted language modeling in Figure 1.3.
1.2.2.3
Pre-training Encoders as Classiﬁers
Another commonly-used idea to train an encoder is to consider classiﬁcation tasks. In self-
supervised learning, this is typically done by creating new classiﬁcation challenges from the unla-
beled text. There are many different ways to design the classiﬁcation tasks. Here we present two
popular tasks.
A simple method, called next sentence prediction (NSP), is presented in BERT’s original
paper [Devlin et al., 2019]. The assumption of NSP is that a good text encoder should capture
the relationship between two sentences. To model such a relationship, in NSP we can use the
output of encoding two consecutive sentences SentA and SentB to determine whether SentB is
the next sentence following SentA . For example, suppose SentA = ’It is raining .’ and SentB =
’I need an umbrella .’. The input sequence of the encoder could be
[CLS] It is raining . [SEP] I need an umbrella . [SEP]
where [CLS] is the start symbol (i.e., x0 ) which is commonly used in encoder pre-training, and
[SEP] is a separator that separates the two sentences. The processing of this sequence follows a
standard procedure of Transformer encoding: we ﬁrst represent each token xi as its corresponding
embedding ei , and then feed the embedding sequence {e0 , ▷▷▷, em } into the encoder to obtain the
output sequence {h0 , ▷▷▷, hm }. Since h0 is generally considered as the representation of the entire
sequence, we add a Softmax layer on top of it to construct a binary classiﬁcation system. This
process is illustrated as follows
token: [CLS] It
embedding:
encoding:
is raining . [SEP] I need an umbrella
.[SEP]
e0e1 e2e3e4e5e6e7e8e9e10e11
↓↓↓↓↓↓↓↓↓↓↓↓
↓↓↓↓↓↓↓↓↓↓↓↓
h0
h1 h2
↓
Softmax
↓
Is Next or Not?
h3
h4
Encoder
h5
h6
h7
h8
h9
h10
h111.2 Self-supervised Pre-training Tasks
x0
x1
x2
x3
13
x4
x0Pr(x0 ) = 1
x1Pr(x1 |e0 )
x2Pr(x2 |e0 , e1 )
x3Pr(x3 |e0 , e1 , e2 )
x4Pr(x4 |e0 , e1 , e2 , e3 )
(a) Causal Language Modeling (order: x0 → x1 → x2 → x3 → x4 )
x0
masked
x1
x2
masked
x3
x4
x01
x1Pr(x1 |e0 , emask , e2 , emask , e4 )
x21
x3Pr(x3 |e0 , emask , e2 , emask , e4 )
x41
masked
masked
(b) Masked Language Modeling (order: x0 , [MASK], x2 , [MASK], x4 → x1 , x3 )
x0
x1
x2
x3
x4
x0Pr(x0 ) = 1
x1Pr(x1 |e0 , e4 , e2 )
x2Pr(x2 |e0 , e4 )
x3Pr(x3 |e0 , e4 , e2 , e1 )
x4Pr(x4 |e0 )
(c) Permuted Language Modeling (order: x0 → x4 → x2 → x1 → x3 )
Fig. 1.3: Comparison of self-attention masking results of causal language modeling, masked language modeling and
permuted language modeling. The gray cell denotes the token at position j does not attend to the token at position i.
The blue cell (i, j) denotes that the token at position j attends to the token at position i. emask represents the embedding
of the symbol [MASK], which is a combination of the token embedding and the positional embedding.
In order to generate training samples, we need two sentences each time, one for SentA and
the other for SentB . A simple way to do this is to utilize the natural sequence of two consecu-
tive sentences in the text. For example, we obtain a positive sample by using actual consecutive
sentences, and a negative sample by using randomly sampled sentences. Consequently, training
this model is the same as training a classiﬁer. Typically, NSP is used as an additional training loss14
Pre-training
function for pre-training based on masked language modeling.
A second example of training Transformer encoders as classiﬁers is to apply classiﬁcation-
based supervision signals to each output of an encoder. For example, Clark et al. [2019] in their
ELECTRA model, propose training a Transformer encoder to identify whether each input token
is identical to the original input or has been altered in some manner. The ﬁrst step of this method
is to generate a new sequence from a given sequence of tokens, where some of the tokens are
altered. To do this, a small masked language model (call it the generator) is applied: we randomly
mask some of the tokens, and train this model to predict the masked tokens. For each training
sample, this masked language model outputs a token at each masked position, which might be
different from the original token. At the same time, we train another Transformer encoder (call it
the discriminator) to determine whether each predicted token is the same as the original token or
altered. More speciﬁcally, we use the generator to generate a sequence where some of the tokens
are replaced. Below is an illustration.
original:
masked:
replaced:
[CLS]Theboyspenthoursworkingontoys.
↓↓↓↓↓↓↓↓↓
[CLS]Theboyspent[MASK]workingon[MASK]↓↓↓↓↓↓↓↓↓
↓↓↓↓↓↓↓↓↓
[CLS]
The
Generator (small masked language model)
boy
spent
decades
working
on
.
toys
.
Then, we use the discriminator to label each of these tokens as original or replaced, as follows
replaced: [CLS]
Theboy
spent decades working
↓↓↓↓↓
↓↓↓↓↓
ontoys.
↓↓↓↓
↓↓↓↓
Discriminator (the model we want)
label: original original original original replaced original original original original
For training, the generator is optimized as a masked language model with maximum likelihood
estimation, and the discriminator is optimized as a classiﬁer using a classiﬁcation-based loss. In
ELECTRA, the maximum likelihood-based loss and the classiﬁcation-based loss are combined for
jointly training both the generator and discriminator. An alternative approach is to use generative
adversarial networks (GANs), that is, the generator is trained to fool the discriminator, and the dis-
criminator is trained to distinguish the output of the generator from the true distribution. However,
GAN-style training complicates the training task and is more difﬁcult to scale up. Nevertheless,
once training is complete, the generator is discarded, and the encoding part of the discriminator is
applied as the pre-trained model for downstream tasks.1.2 Self-supervised Pre-training Tasks
1.2.3
15
Encoder-Decoder Pre-training
In NLP, encoder-decoder architectures are often used to model sequence-to-sequence problems,
such as machine translation and question answering. In addition to these typical sequence-to-
sequence problems in NLP, encoder-decoder models can be extended to deal with many other
problems. A simple idea is to consider text as both the input and output of a problem, and so
we can directly apply encoder-decoder models. For example, given a text, we can ask a model to
output a text describing the sentiment of the input text, such as positive, negative, and neutral.
Such an idea allows us to develop a single text-to-text system to address any NLP problem.
We can formulate different problems into the same text-to-text format. We ﬁrst train an encoder-
decoder model to gain general-purpose knowledge of language via self-supervision. This model
is then ﬁne-tuned for speciﬁc downstream tasks using targeted text-to-text data.
1.2.3.1
Masked Encoder-Decoder Pre-training
In Raffel et al. [2020]’s T5 model, many different tasks are framed as the same text-to-text task.
Each sample in T5 follows the format
Source Text → Target Text
Here → separates the source text, which consists of a task description or instruction and the input
given to the system, from the target text, which is the response to the input task. As an example,
consider a task of translating from Chinese to English. A training sample can be expressed as
[CLS] Translate from Chinese to English: 你好 → ⟨s⟩ Hello!
where [CLS] and ⟨s⟩ are the start symbols on the source and target sides, respectively5 .
Likewise, we can express other tasks in the same way. For example
[CLS] Answer: when was Albert Einstein born?
→ ⟨s⟩ He was born on March 14, 1879.
[CLS] Simplify: the professor, who has published numerous papers in his eld,
will be giving a lecture on the topic next week.
→ ⟨s⟩ The experienced professor will give a lecture next week.
[CLS] Score the translation from English to Chinese. English: when in Rome, do as
the Romans do. Chinese: 人 在 罗马 就 像 罗马 人 一样 做事 。
→ ⟨s⟩ 0▷81
where instructions are highlighted in gray. An interesting case is that in the last example we
5
We could use the same start symbol for different sequences. Here we use different symbols to distinguish the
sequences on the encoder and decoder-sides.16
Pre-training
reframe the scoring problem as the text generation problem. Our goal is to generate a text repre-
senting the number 0▷81, rather than outputting it as a numerical value.
The approach described above provides a new framework of universal language understanding
and generation. Both the task instructions and the problem inputs are provided to the system
in text form. The system then follows the instructions to complete the task. This method puts
different problems together, with the beneﬁt of training a single model that can perform many
tasks simultaneously.
In general, ﬁne-tuning is necessary for adapting the pre-trained model to a speciﬁc downstream
task. In this process, one can use different ways to instruct the model for the task, such as using a
short name of the task as the preﬁx to the actual input sequence or providing a detailed description
of the task. Since the task instructions are expressed in text form and involved as part of the input,
the general knowledge of instruction can be gained through learning the language understanding
models in the pre-training phase. This may help enable zero-shot learning. For example, pre-
trained models can generalize to address new problems where the task instructions have never
been encountered.
There have been several powerful methods of self-supervised learning for either Transformer
encoders or decoders. Applying these methods to pre-train encoder-decoder models is relatively
straightforward. One common choice is to train encoder-decoder models as language models. For
example, the encoder receives a sequence preﬁx, while the decoder generates the remaining se-
quence. However, this differs from standard causal language modeling, where the entire sequence
is autoregressively generated from the ﬁrst token. In our case, the encoder processes the preﬁx at
once, and then the decoder predicts subsequent tokens in the manner of causal language modeling.
Put more precisely, this is a preﬁx language modeling problem: a language model predicts the
subsequent sequence given a preﬁx, which serves as the context for prediction.
Consider the following example
[CLS] The puppies are frolicking → ⟨s⟩ outside the house .


Preﬁx



Subsequent Sequence

We can directly train an encoder-decoder model using examples like this. Then, the encoder learns
to understand the preﬁx, and the decoder learns to continue writing based on this understanding.
For large-scale pre-training, it is easy to create a large number of training examples from unlabeled
text.
It is worth noting that for pre-trained encoder-decoder models to be effective in multi-lingual
and cross-lingual tasks, such as machine translation, they should be trained with multi-lingual
data. This typically requires that the vocabulary includes tokens from all the languages. By
doing so, the models can learn shared representations across different languages, thereby enabling
capabilities in both language understanding and generation in a multi-lingual and cross-lingual
context.
A second approach to pre-training encoder-decoder models is masked language modeling. In
this approach, as discussed in Section 1.2.2, tokens in a sequence are randomly replaced with a
mask symbol, and the model is then trained to predict these masked tokens based on the entire
masked sequence.
As an illustration, consider the task of masking and reconstructing the sentence1.2 Self-supervised Pre-training Tasks
17
The puppies are frolicking outside the house .
By masking two tokens (say, frolicking and the), we have the BERT-style input and output of the
model, as follows
[CLS] The puppies are [MASK] outside [MASK] house .
→ ⟨s⟩
frolicking
the
Here
denotes the masked position at which we do not make token predictions. By varying the
percentage of the tokens in the text, this approach can be generalized towards either BERT-style
training or language modeling-style training [Song et al., 2019]. For example, if we mask out all
the tokens, then the model is trained to generate the entire sequence
[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]
→ ⟨s⟩ The puppies are frolicking outside the house .
In this case, we train the decoder as a language model.
Note that, in the context of the encoder-decoder architecture, we can use the encoder to read
the masked sequence, and use the decoder to predict the original sequence. With this objective,
we essentially have a denoising autoencoder: the encoder transforms a corrupted input into some
hidden representation, and the decoder reconstructs the uncorrupted input from this hidden repre-
sentation. Here is an example of input and output for denoising training.
[CLS] The puppies are [MASK] outside [MASK] house .
→ ⟨s⟩ The puppies are frolicking outside the house .
By learning to map from this corrupted sequence to its uncorrupted counterpart, the model gains
the ability to understand on the encoder side and to generate on the decoder side. See Figure 1.4
for an illustration of how an encoder-decoder model is trained with BERT-style and denoising
autoencoding objectives.
As we randomly select tokens for masking, we can certainly mask consecutive tokens [Joshi
et al., 2020]. Here is an example.
[CLS] The puppies are [MASK] outside [MASK] [MASK] .
→ ⟨s⟩ The puppies are frolicking outside the house .
Another way to consider consecutive masked tokens is to represent them as spans. Here we
follow Raffel et al. [2020]’s work, and use [X], [Y] and [Z] to denote sentinel tokens that cover
one or more consecutive masked tokens. Using this notation, we can re-express the above training
example as
[CLS] The puppies are [X] outside [Y] .
→ ⟨s⟩ [X] frolicking [Y] the house [Z]18
Pre-training
Loss
[M]
[M]
[M]frolicking [M]
Encoder
[CLS] The puppies are
[M]
the[M][M]
[M] frolicking [M]the[M]
Decoder
in
[M] house
.
⟨s⟩
[M]
[M]
(a) Training an encoder-decoder model with BERT-style masked language modeling
Loss over the sequence
thehouse.
The puppies are frolicking inthehouse
The puppies are frolicking in
Encoder
[CLS] The puppies are
[M]
Decoder
in
[M] house
.
⟨s⟩
(b) Training an encoder-decoder model with denoising autoencoding
Fig. 1.4: Training an encoder-decoder model using BERT-style and denoising autoencoding methods. In both methods,
the input to the encoder is a corrupted token sequence where some tokens are masked and replaced with [MASK] (or
[M] for short). The decoder predicts these masked tokens, but in different ways. In BERT-style training, the decoder
only needs to compute the loss for the masked tokens, while the remaining tokens in the sequence can be simply treated
as [MASK] tokens. In denoising autoencoding, the decoder predicts the sequence of all tokens in an autoregressive
manner. As a result, the loss is obtained by accumulating the losses of all these tokens, as in standard language
modeling.
The idea is that we represent the corrupted sequence as a sequence containing placeholder
slots. The training task is to ﬁll these slots with the correct tokens using the surrounding context.
An advantage of this approach is that the sequences used in training would be shorter, making the
training more efﬁcient. Note that masked language modeling provides a very general framework
for training encoder-decoder models. Various settings can be adjusted to have different training
versions, such as altering the percentage of tokens masked and the maximum length of the masked
spans.
1.2.3.2
Denoising Training
If we view the problem of training encoder-decoder models as a problem of training denoising
autoencoders, there will typically be many different methods for introducing input corruption and
reconstructing the input. For instance, beyond randomly masking tokens, we can also alter some
of them or rearrange their order.
Suppose we have an encoder-decoder model that can map an input sequence x to an output1.2 Self-supervised Pre-training Tasks
19
sequence y
y = Decodeω (Encodeθ (x))
= Modelθ,ω (x)
(1.15)
where θ and ω are the parameters of the encoder and the decoder, respectively. In denoising
autoencoding problems, we add some noise to x to obtain a noisy, corrupted input xnoise . By
feeding xnoise into the encoder, we wish the decoder to output the original input. The training
objective can be deﬁned as
(θ̂, ω̂) = arg min Loss(Modelθ,ω (xnoise ), x)
(1.16)
θ,ω
Here the loss function Loss(Modelθ,ω (xnoise ), x) evaluates how well the model Modelθ,ω (xnoise )
reconstructs the original input x. We can choose the cross-entropy loss as usual.
As the model architecture and the training approach have been developed, the remaining issue
is the corruption of the input. Lewis et al. [2020], in their BART model, propose corrupting the
input sequence in several different ways.
• Token Masking. This is the same masking method that we used in masked language mod-
eling. The tokens in the input sequence are randomly selected and masked.
• Token Deletion. This method is similar to token masking. However, rather than replacing
the selected tokens with a special symbol [MASK], these tokens are removed from the
sequence. See the following example for a comparison of the token masking and token
deletion methods.
Original (x):
The puppies are frolicking outside the house .
Token Masking (xnoise ):The puppies are [MASK] outside [MASK] house .
Token Deletion (xnoise ):The puppies are frolicking outside the house .
where the underlined tokens in the original sequence are masked or deleted.
• Span Masking. Non-overlapping spans are randomly sampled over the sequence. Each
span is masked by [MASK]. We also consider spans of length 0, and, in such cases, [MASK]
is simply inserted at a position in the sequence. For example, we can use span masking to
corrupt the above sequence as
Original (x):
Span Masking (xnoise ):
The 0 puppies are frolicking outside the house .
The [MASK] puppies are [MASK] house .
Here the span frolicking outside the is replaced with a single [MASK]. 0 indicates a length-
0 span, and so we insert an [MASK] between The and puppies. Span masking introduces
new prediction challenges in which the model needs to know how many tokens are gener-
ated from a span. This problem is very similar to fertility modeling in machine translation
[Brown et al., 1993].20
Pre-training
If we consider a sequence consisting of multiple sentences, additional methods of corruption
can be applied. In the BART model, there are two such methods.
• Sentence Reordering. This method randomly permutes the sentences so that the model can
learn to reorder sentences in a document. Consider, for example, two consecutive sentences
Hard work leads to success . Success brings happiness .
We can reorder the two sentences to have a corrupted input sequence
Success brings happiness . Hard work leads to success .
• Document Rotation. The goal of this task is to identify the start token of the sequence.
First, a token is randomly selected from the sequence. Then, the sequence is rotated so that
the selected token is the ﬁrst token. For example, suppose we select the token leads from
the above sequence. The rotated sequence is
selected
Hard work leads to success . Success brings happiness . Hard work
where the subsequence Hard work before leads is appended to the end of the sequence.
For pre-training, we can apply multiple corruption methods to learn robust models, for ex-
ample, we randomly choose one of them for each training sample. In practice, the outcome of
encoder-decoder pre-training depends heavily on the input corruption methods used, and so we
typically need to choose appropriate training objectives through careful experimentation.
1.2.4
Comparison of Pre-training Tasks
So far, we have discussed a number of pre-training tasks. Since the same training objective can
apply to different architectures (e.g., using masked language modeling for both encoder-only and
encoder-decoder pre-training), categorizing pre-training tasks based solely on model architecture
does not seem ideal. Instead, we summarize these tasks based on the training objectives.
• Language Modeling. Typically, this approach refers to an auto-regressive generation pro-
cedure of sequences. At one time, it predicts the next token based on its previous context.
• Masked Language Modeling. Masked Language Modeling belongs to a general mask-
predict framework. It randomly masks tokens in a sequence and predicts these tokens using
the entire masked sequence.1.3 Example: BERT
21
• Permuted Language Modeling. Permuted language modeling follows a similar idea to
masked language modeling, but considers the order of (masked) token prediction. It reorders
the input sequence and predicts the tokens sequentially. Each prediction is based on some
context tokens that are randomly selected.
• Discriminative Training. In discriminative training, supervision signals are created from
classiﬁcation tasks. Models for pre-training are integrated into classiﬁers and trained to-
gether with the remaining parts of the classiﬁers to enhance their classiﬁcation performance.
• Denoising Autoencoding. This approach is applied to the pre-training of encoder-decoder
models. The input is a corrupted sequence and the encoder-decoder models are trained to
reconstruct the original sequence.
Table 1.1 illustrates these methods and their variants using examples. The use of these ex-
amples does not distinguish between models, but we mark the model architectures where the
pre-training tasks can be applied. In each example, the input consists of a token sequence, and the
output is either a token sequence or some probabilities. For generation tasks, such as language
modeling, superscripts are used to indicate the generation order on the target side. If the super-
scripts are omitted, it indicates that the output sequence can be generated either autoregressively
or simultaneously. On the source side, we assume that the sequence undergoes a standard Trans-
former encoding process, meaning that each token can see the entire sequence in self-attention.
The only exception is in permuted language modeling, where an autoregressive generation pro-
cess is implemented by setting attention masks on the encoder side. To simplify the discussion,
we remove the token ⟨s⟩ from the target-side of each example.
While these pre-training tasks are different, it is possible to compare them in the same frame-
work and experimental setup [Dong et al., 2019; Raffel et al., 2020; Lewis et al., 2020]. Note that
we cannot list all the pre-training tasks here as there are many of them. For more discussions on
pre-training tasks, the interested reader may refer to some surveys on this topic [Qiu et al., 2020;
Han et al., 2021].
1.3
Example: BERT
In this section, we introduce BERT models, which are among the most popular and widely used
pre-trained sequence encoding models in NLP.
1.3.1
The Standard Model
The standard BERT model, which is proposed in Devlin et al. [2019]’s work, is a Transformer
encoder trained using both masked language modeling and next sentence prediction tasks. The
loss used in training this model is a sum of the loss of the two tasks.
LossBERT = LossMLM + LossNSP
(1.17)
As is regular in training deep neural networks, we optimize the model parameters by minimizing
this loss. To do this, a number of training samples are collected. During training, a batch of22
Pre-training
Method Enc Dec E-D Input
Causal LM
•
Preﬁx LM
Masked LM
MASS-style
BERT-style
•
Token Classiﬁcation
chasing1 the2 ball3 .4
[C] The kitten [M] chasing the [M] .is
[C] The kitten [M] [M] [M] ball .is chasing the
••[C] The kitten [M] playing the [M] .kitten is chasing
•
•
ball
ball
[C] The kitten is chasing the ball .The5 kitten7 is6 chasing1 the4 ball2 .3
•[C] The kitten is chasing the ball .Pr(IsNext | representation-of-[C])
•Encode a sentence as ha and•[C] The kitten is chasing the ball .Pr(·|The) Pr(·|kitten) ... Pr(·|.)
•[C] . kitten the chasing The is ballThe1 kitten2 is3 chasing4 the5 ball6 .7
•[C] The kitten is chasing the ball .The1 kitten2 is3 chasing4 the5 ball6 .7
•[C] The kitten [M] is [M] .The1 kitten2 is3 chasing4 the5 ball6 .7
•[C] The kitten [X] the [Y][X] is2 chasing3 [Y] ball5 .6
•[C] The ball rolls away swiftly . The The1 kitten2 is3 chasing4 the5 ball6 .7
kitten is chasing the ball .The8 ball9 rolls10 away11 swiftly12 .13
•[C] chasing the ball . The ball rollsThe1 kitten2 is3 chasing4 the5 ball6 .7
away swiftly . The kitten isThe8 ball9 rolls10 away11 swiftly12 .13
Next Sentence
Comparison
[C] The kitten is
••
Sentence
•
The1 kitten2 is3 chasing4 the5 ball6 .7
•Permuted LM
Prediction
•
Output
Token Reordering
Token Deletion
Span Masking
Sentinel Masking
Sentence
Reordering
Document
Rotation
Birds eat worms .
Score(ha , hb )
another sentence as hb
1
4
Table 1.1: Comparison of pre-training tasks, including language modeling, masked language modeling, permuted
language modeling, discriminative training, and denoising autoencoding. [C] = [CLS], [M] = [MASK], [X], [Y] =
sentinel tokens. Enc, Dec and E-D indicate whether the approach can be applied to encoder-only, decoder-only,
encoder-decoder models, respectively. For generation tasks, superscripts are used to represent the order of the tokens.
training samples is randomly selected from this collection at a time, and LossBERT is accumulated
over these training samples. Then, the model parameters are updated via gradient descent or its
variants. This process is repeated many times until some stopping criterion is satisﬁed, such as
when the training loss converges.
1.3.1.1
Loss Functions
In general, BERT models are used to represent a single sentence or a pair of sentences, and thus
can handle various downstream language understanding problems. In this section we assume that
the input representation is a sequence containing two sentences SentA and SentB , expressed as
[CLS] SentA [SEP] SentB [SEP]
Here we follow the notation in BERT’s paper and use [SEP] to denote the separator.
Given this sequence, we can obtain LossMLM and LossNSP separately. For masked language
modeling, we predict a subset of the tokens in the sequence. Typically, a certain percentage of1.3 Example: BERT
23
the tokens are randomly selected, for example, in the standard BERT model, 15% of the tokens in
each sequence are selected. Then the sequence is modiﬁed in three ways
• Token Masking. 80% of the selected tokens are masked and replaced with the symbol
[MASK]. For example
Original:[CLS] It is raining . [SEP] I need an umbrella . [SEP]
Masked:[CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]
where the selected tokens are underlined. Predicting masked tokens makes the model learn
to represent tokens from their surrounding context.
• Random Replacement. 10% of the selected tokens are changed to a random token. For
example
Original:
[CLS] It is raining . [SEP] I need an umbrella . [SEP]
[CLS] It is raining . [SEP] I need an hat . [SEP]
Random Token:
This helps the model learn to recover a token from a noisy input.
• Unchanged. 10% of the selected tokens are kept unchanged. For example,
Original:[CLS] It is raining . [SEP] I need an umbrella . [SEP]
Unchanged Token:[CLS] It is raining . [SEP] I need an umbrella . [SEP]
This is not a difﬁcult prediction task, but can guide the model to use easier evidence for
prediction.
Let A(x) be the set of selected positions of a given token sequence x, and x̄ be the modiﬁed
sequence of x. The loss function of masked language modeling can be deﬁned as
LossMLM = −

i∈A(x)
log Pri (xi |x̄)
(1.18)
where Pri (xi |x̄) is the probability of predicting xi at the position i given x̄. Figure 1.5 shows a
running example of computing LossMLM .
For next sentence prediction, we follow the method described in Section 1.2.2.3. Each training
sample is classiﬁed into a label set {IsNext, NotNext}, for example,
Sequence:
Label:
Sequence:
Label:
[CLS] It is raining . [SEP] I need an umbrella . [SEP]
IsNext
[CLS] The cat sleeps on the windowsill . [SEP] Apples grow on trees . [SEP]
NotNext24
Pre-training
Input: [CLS] It is raining . [SEP] I need an umbrella . [SEP]
Select tokens with a probability of 15%
Token Selection: [CLS] It is raining . [SEP] I need an umbrella . [SEP]
Mask selected tokens with a probability of 80%
Token Masking: [CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]
Alter selected tokens with a probability of 10%
Token: [CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]
Replacement
Keep selected tokens unchanged with a probability of 10%
Unchanged: [CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]
Train the Transformer encoder with the modiﬁed sequence
training
h0
h1
h2
h3
an umbrella
I
h4
h5
h6
h7
h8h9h10h11
e8
Transformer Encoder
e0e1e2e3e4e5e6e7e9e10e11
[CLS]Itis[MASK].[SEP]Ineed [MASK] hat.[SEP]
Fig. 1.5: A running example of BERT-style masked language modeling. First, 15% of the tokens are randomly selected.
These selected tokens are then processed in one of three ways: replaced with a [MASK] token (80% of the time),
replaced with a random token (10% of the time), or kept unchanged (10% of the time). The model is trained to predict
these selected tokens based on the modiﬁed sequence. ei represents the embedding of the token at the position i. Gray
boxes represent the Softmax layers.
The output vector of the encoder for the ﬁrst token [CLS] is viewed as the sequence representation,
denoted by hcls (or h0 ). A classiﬁer is built on top of hcls . Then, we can compute the probability of
a label c given hcls , i.e., Pr(c|hcls ). There are many loss functions one can choose for classiﬁcation
problems. For example, in maximum likelihood training, we can deﬁne LossNSP as
LossNSP = − log Pr(cgold |hcls )
(1.19)1.3 Example: BERT
25
where cgold is the correct label for this sample.
1.3.1.2
Model Setup
As shown in Figure 1.6, BERT models are based on the standard Transformer encoder architecture.
The input is a sequence of embeddings, each being the sum of the token embedding, the positional
embedding, and the segment embedding.
e = x + epos + eseg
(1.20)
Both the token embedding (x) and positional embedding (epos ) are regular, as in Transformer
models. The segment embedding (eseg ) is a new type of embedding that indicates whether a token
belongs to SentA or SentB . This can be illustrated by the following example.
Token [CLS]Itisraining.[SEP]Ineedanumbrella.[SEP]
xx1x2x3x4x5x6x7x8x9x10x11
x0
epos PE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8) PE(9) PE(10) PE(11)
eseg
eA
eA
eA
eA
eA
eA
eB
eB
eB
eB
eB
eB
The main part of BERT models is a multi-layer Transformer network. A Transformer layer
consists of a self-attention sub-layer and an FFN sub-layer. Both of them follow the post-norm
architecture: output = LNorm(F (input) + input), where F (·) is the core function of the sub-
layer (either a self-attention model or an FFN), and LNorm(·) is the layer normalization unit.
Typically, a number of Transformer layers are stacked to form a deep network. At each position of
the sequence, the output representation is a real-valued vector which is produced by the last layer
of the network.
There are several aspects one may consider in developing BERT models.
• Vocabulary Size (|V |). In Transformers, each input token is represented as an entry in a
vocabulary V . Large vocabularies can cover more surface form variants of words, but may
lead to increased storage requirements.
• Embedding Size (de ). Every token is represented as a de -dimensional real-valued vector.
As presented above, this vector is the sum of the token embedding, positional embedding,
and segment embedding, all of which are also de -dimensional real-valued vectors.
• Hidden Size (d). The input and output of a sub-layer are of d dimensions. Besides, most
of the hidden states of a sub-layer are d-dimensional vectors. In general, d can be roughly
viewed as the width of the network.
• Number of Heads (nhead ). In self-attention sub-layers, one needs to specify the number of
heads used in multi-head self-attention. The larger this number is, the more sub-spaces in
which attention is performed. In practical systems, we often set nhead ≥ 4.
• FFN Hidden Size (dﬀn ). The size of the hidden layer of the FFNs used in Transformers is
typically larger than d. For example, a typical setting is dﬀn = 4d. For larger Transformers,
such as recent large models, dﬀn may be set to a very large value.26
Pre-training
▷▷▷
Output Layer
h0 h1 ▷▷▷hm
Layer Normalization
FFN
Encoder Output
hi ∈ Rd is the contextual
representation of xi
FFN Sub-layer
hidden size: d
FFN hidden size: dﬀn
layers
Layer Normalization
Self-attention
Self-attention Sub-layer
hidden size: d
number of heads: nhead
Embedding
Token
Position Segment
x0 x1 ▷▷▷xm
e = x + epos + eseg ∈ Rde
Input
xi corresponds to an entry of V
Fig. 1.6: The model architecture of BERT (Transformer encoder). The input tokens are ﬁrst represented as embeddings,
each of which is the sum of the corresponding token embedding, positional embedding and segment embedding. Then,
the embedding sequence is processed by a stack of Transformer layers. Each layer in this stack includes a self-attention
sub-layer and a FFN sub-layer. The output of the BERT model is a sequence of vectors produced by the ﬁnal Trans-
former layer.
• Model Depth (L). Using deep networks is an effective way to improve the expressive power
of Transformers. For BERT models, L is typically set to 12 or 24. However, networks with
even greater depth are also feasible and can be applied for further enhancements.
Different settings of these hyper-parameters lead to different model sizes. There are two
widely-used BERT models.
• BERTbase : d = 768, L = 12, nhead = 12, total number of parameters = 110M.
• BERTlarge : d = 1, 024, L = 24, nhead = 16, total number of parameters = 340M.
Training BERT models follows the standard training process of Transformers. Training larger
models such as BERTlarge requires more training effort and time. This is a common problem
for pre-training, especially when a model is trained on a very large amount of data. In practice,1.3 Example: BERT
27
there are often considerations of training efﬁciency. For example, a practice is to ﬁrst train a
BERT model on relatively short sequences for a large number of training steps, and then continue
training it on full-length sequences for the remaining training steps.
1.3.2
More Training and Larger Models
BERT is a milestone model in NLP, sparking many subsequent efforts to improve it. One direction
is to scale up the model itself, including increasing training data and developing larger models.
RoBERTa, an extension of the standard BERT model, is an example of such efforts [Liu et al.,
2019]. It introduces two major improvements. First, simply using more training data and more
compute can improve BERT models without need of changing the model architectures. Second,
removing the NSP loss does not decrease the performance on downstream tasks if the training is
scaled up. These ﬁndings suggest exploring a general direction of pre-training: we can continue
to improve pre-training by scaling it up on simple pre-training tasks.
A second approach to improving BERT models is to increase the number of model parame-
ters. For example, in He et al. [2021]’s work, a 1.5 billion-parameter BERT-like model is built by
increasing both the model depth and hidden size. However, scaling up BERT and various other
pre-trained models introduces new challenges in training, for example, training very large models
often becomes unstable and difﬁcult to converge. This makes the problem more complicated, and
requires careful consideration of various aspects, including model architecture, parallel computa-
tion, parameter initialization, and so on. In another example, Shoeybi et al. [2019] successfully
trained a 3.9 billion-parameter BERT-like model, where hundreds of GPUs were used to manage
the increased computational demands.
1.3.3
More Efﬁcient Models
Compared to its predecessors, BERT is a relatively large model for the time it was proposed.
This increase in model size results in larger memory requirements and a consequent slowdown in
system performance. Developing smaller and faster BERT models is part of the broader challenge
of building efﬁcient Transformers, which has been extensively discussed in Tay et al. [2020]’s
work and Xiao and Zhu [2023]’s work. However, a deeper discussion of this general topic is
beyond the scope of our current discussion. Here we instead consider a few efﬁcient variants of
BERT.
Several threads of research are of interest to NLP researchers in developing efﬁcient BERT
models. First, work on knowledge distillation, such as training student models with the output
of well-trained teacher models, shows that smaller BERT models can be obtained by transferring
knowledge from larger BERT models. Given that BERT models are multi-layer networks with
several different types of layers, knowledge distillation can be applied at different levels of repre-
sentation. For example, beyond distilling knowledge from the output layers, it is also possible to
incorporate training loss that measures the difference in output of hidden layers between teacher
models and student models [Sun et al., 2020; Jiao et al., 2020]. Indeed, knowledge distillation has
been one of the most widely-used techniques for learning small pre-trained models.
Second, conventional model compression methods can be directly applied to compress BERT
models. One common approach is to use general-purpose pruning methods to prune the Trans-
former encoding networks [Gale et al., 2019]. This generally involves removing entire layers [Fan28
Pre-training
et al., 2019] or a certain percentage of parameters in the networks [Sanh et al., 2020; Chen et al.,
2020]. Pruning is also applicable to multi-head attention models. For example, Michel et al.
[2019] show that removing some of the heads does not signiﬁcantly decrease the performance of
BERT models, but speeds up the inference of these models. Another approach to compressing
BERT models is quantization [Shen et al., 2020]. By representing model parameters as low-
precision numbers, the models can be greatly compressed. While this method is not speciﬁc to
BERT models, it proves effective for large Transformer-based architectures.
Third, considering that BERT models are relatively deep and large networks, another thread
of research uses dynamic networks to adapt these models for efﬁcient inference. An idea in this
paradigm is to dynamically choose the layers for processing a token, for example, in depth-
adaptive models we exit at some optimal depth and thus skip the rest of the layers in the layer
stack [Xin et al., 2020; Zhou et al., 2020]. Similarly, we can develop length-adaptive models in
which the length of the input sequence is dynamically adjusted. For example, we can skip some of
the tokens in the input sequence so that the model can reduce computational load on less important
tokens, enhancing overall efﬁciency.
Fourth, it is also possible to share parameters across layers to reduce the size of BERT models.
A simple way to do this is to share the parameters of a whole Transformer layer across the layer
stack [Dehghani et al., 2018; Lan et al., 2020]. In addition to the reduced number of parameters,
this enables reuse of the same layer in a multi-layer Transformer network, leading to savings of
memory footprint at test time.
1.3.4
Multi-lingual Models
The initial BERT model was primarily focused on English. Soon after this model was proposed,
it was extended to many languages. One simple way to do this is to develop a separate model
for each language. Another approach, which has become more popular in recent work on large
language models, is to train multi-lingual models directly on data from all the languages. In
response, multi-lingual BERT (mBERT) models were developed by training them on text from
104 languages 6 . The primary difference from monolingual BERT models is that mBERT models
use larger vocabularies to cover tokens from multiple languages. As a result, the representations
of tokens from different languages are mapped into the same space, allowing for the sharing of
knowledge across languages via this universal representation model.
One important application of multi-lingual pre-trained models is cross-lingual learning. In the
cross-lingual setting, we learn a model on tasks in one language, and apply it to the same tasks
in another language. In cross-lingual text classiﬁcation, for example, we ﬁne-tune a multi-lingual
pre-trained model on English annotated documents. Then, we use the ﬁne-tuned model to classify
Chinese documents.
An improvement to multi-lingual pre-trained models like mBERT is to introduce bilingual data
into pre-training. Rather than training solely on monolingual data from multiple languages, bilin-
gual training explicitly models the relationship between tokens in two languages. The resulting
model will have innate cross-lingual transfer abilities, and thus can be easily adapted to different
languages. Lample and Conneau [2019] propose an approach to pre-training cross-lingual lan-
guage models (XLMs). In their work, a cross-lingual language model can be trained in either the
causal language modeling or masked language modeling manner. For masked language modeling
6
https://github.com/google-research/bert/1.3 Example: BERT
29
pre-training, the model is treated as an encoder. The training objective is the same as BERT: we
maximize the probabilities of some randomly selected tokens which are either masked, replaced
with random tokens, or kept unchanged in the input. If we consider bilingual data in pre-training,
we sample a pair of aligned sentences each time. Then, the two sentences are packed together to
form a single sequence used for training. For example, consider an English-Chinese sentence pair
鲸鱼 是 哺乳 动物 。 ↔ Whales are mammals .
We can pack them to obtain a sequence, like this
[CLS] 鲸鱼 是 哺乳 动物 。 [SEP] Whales are mammals . [SEP]
We then select a certain percentage of the tokens and replace them with [MASK].
[CLS] [MASK] 是 [MASK] 动物 。 [SEP] Whales [MASK] [MASK] . [SEP]
The goal of pre-training is to maximize the product of the probabilities of the masked tokens given
the above sequence. By performing training in this way, the model can learn to represent both the
English and Chinese sequences, as well as to capture the correspondences between tokens in the
two languages. For example, predicting the Chinese token 鲸鱼 may require the information
from the English token Whales. Aligning the representations of the two languages essentially
transforms the model into a “translation” model. So this training objective is also called transla-
tion language modeling. Figure 1.7 shows an illustration of this approach.
A beneﬁt of multi-lingual pre-trained models is their inherent capability of handling code-
switching. In NLP and linguistics, code-switching refers to switching among languages in a text.
For example, the following is a mixed language text containing both Chinese and English:
周末 我们 打算 去 做 hiking  你 想 一起 来 吗 
(We plan to go hiking this weekend, would you like to join us?)
For multi-lingual pre-trained models, we do not need to identify whether a token is Chinese or
English. Instead, every token is just an entry of the shared vocabulary. This can be imagined as
creating a “new” language that encompasses all the languages we want to process.
The result of multi-lingual pre-training is inﬂuenced by several factors. Given that the model
architecture is ﬁxed, one needs to specify the size of the shared vocabulary, the number (or per-
centage) of samples in each language, the size of the model, and so on. Conneau et al. [2020]
point out several interesting issues regarding large-scale multi-lingual pre-training for XLM-like
models. First, as the number of supported languages increases, a larger model is needed to handle
these languages. Second, a larger shared vocabulary is helpful for modeling the increased diver-
sity in languages. Third, low-resource languages more easily beneﬁt from cross-lingual transfer
from high-resource languages, particularly when similar high-resource languages are involved in
pre-training. However, interference may occur if the model is trained for an extended period,30
Pre-training
鲸鱼
h0
h1
哺乳
h2
are mammals
h3
h4
h5
h6
h7
h8h9h10h11
e8e9e10e11
Transformer Encoder
e0
e1
e2e3e4[CLS] [MASK]是[MASK]动物。[SEP] Whales [MASK] [MASK].[SEP]
(zh)(zh)(zh)(zh)(zh)(zh)(en)(en)
(zh)
e5
e6
e7
(en)
(en)
(en)
Fig. 1.7: An illustration of translation language modeling. For ease of understanding, we present a simple example
where all the selected tokens are masked. The model is trained to predict these masked tokens. As the sequence
contains tokens in two languages, predicting a token in one language allows access to tokens in the other language,
thereby enabling cross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e., ei ) is the
sum of the token embedding, positional embedding, and language embedding. This requires that each token is assigned
with a language label. Thus we can distinguish tokens in different languages. In multi-lingual pre-training, particularly
in work using shared vocabularies, specifying the language to which a token belongs is not necessary. The use of
language embeddings in turn makes it difﬁcult to handle code-switching. Therefore, we assume here that all token
representations are language-independent.
meaning the overall performance of the pre-trained model starts decreasing at a certain point dur-
ing pre-training. Thus, in practical systems, one may need to stop the pre-training early to prevent
interference.
1.4
Applying BERT Models
Once a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT models
are not immediately ready for performing speciﬁc downstream tasks. In general, additional ﬁne-
tuning work is required to make them adapt. As a ﬁrst step, we need a predictor to align the
output of the model with the problem of interest. Let BERTθ̂ (·) be a BERT model with pre-
trained parameters θ̂, and Predictω (·) be a prediction network with parameters ω. By integrating
the prediction network with the output of the BERT model, we develop a model to tackle the
downstream tasks. This model can be expressed as
y = Predictω (BERTθ̂ (x))
(1.21)
where x is the input and y is the output that ﬁts the problem. For example, in classiﬁcation
problems, the model outputs a probability distribution over labels.
Then, we collect a set of labeled samples D, and ﬁne-tune the model by
(ω̃, θ̃) = arg min
ω,θ̂ +

(x,ygold )∈D
Loss(yω,θ̂+ , ygold )
(1.22)1.4 Applying BERT Models
31
where (x, ygold ) represents a tuple of an input and its corresponding output. The notation of this
equation seems a bit complicated, but the training/tuning process is standard. We optimize the
model by minimizing the loss over the tuning samples. The outcome is the optimized parameters
ω̃ and θ̃. The optimization starts with the pre-trained parameters θ̂. Here we use θ̂+ to indicate that
the parameters are initialized with θ̂, and use yω,θ̂+ to denote the model output computed using
the parameters ω and θ̂+ .
With the ﬁne-tuned parameters ω̃ and θ̃, we can apply the model Predictω̃ (BERTθ̃ (·)) to new
data of the same tasks for which the model was ﬁne-tuned. The form of the downstream tasks
determines the input and output formats of the model, as well as the architecture of the prediction
network. In the following we list some tasks to which BERT models are generally suited.
• Classiﬁcation (Single Text). One of the most widely-used applications of BERT models is
text classiﬁcation. In this task, a BERT model receives a sequence of tokens and encodes
it as a sequence of vectors. The ﬁrst output vector hcls (or h0 ) is typically used as the
representation of the entire text. The prediction network takes hcls as input to produce a
distribution of labels. Let [CLS]x1 x2 ▷▷▷xm be an input text. See below for an illustration of
BERT-based text classiﬁcation.
Class
hcls
h1
h2
▷▷▷
hmhm+1
BERT
eclse1e2▷▷▷emem+1
[CLS]x1x2▷▷▷xm[SEP]
Here the gray box denotes the prediction network. Many NLP problems can be categorized
as text classiﬁcation tasks, and there have been several text classiﬁcation benchmarks for
evaluating pre-trained models. For example, we can classify texts by their grammatical
correctness (grammaticality) or emotional tone (sentiment) [Socher et al., 2013; Warstadt
et al., 2019]. Note that the prediction network could be any classiﬁcation model, such as a
deep neural network or a more traditional classiﬁcation model. The entire model can then
be trained or ﬁne-tuned in the manner of a standard classiﬁcation model. For example,
the prediction network can be simply a Softmax layer and the model parameters can be
optimized by maximizing the probabilities of the correct labels.
• Classiﬁcation (Pair of Texts). Classiﬁcation can also be performed on a pair of texts. Sup-
pose we have two texts, x1 ▷▷▷xm and y1 ▷▷▷yn . We can concatenate these texts to form a single
sequence with a length len. Then, we predict a label for this combined text sequence based
on the hcls vector, as follows32
Pre-training
Class
hclsh1h2▷▷▷eclse1e2▷▷▷em
[CLS]x1x2▷▷▷xm
hm
▷▷▷hlen−1 hlen
em+1 em+2 em+3▷▷▷elen−1 elen
[SEP]▷▷▷
hm+1 hm+2 hm+3
BERT
y1
Text 1
y2
yn
[SEP]
Text 2
where len = n+m+2. Text pair classiﬁcation covers several problems, including semantic
equivalence judgement (determine whether two texts are semantically equivalent) [Dolan
and Brockett, 2005], text entailment judgement (determine whether a hypothesis can be
logically inferred or entailed from a premise) [Bentivogli and Giampiccolo, 2011; Williams
et al., 2018], grounded commonsense inference (determine whether an event is likely to
happen given its context) [Zellers et al., 2018], and question-answering inference (determine
whether an answer corresponds to a given question).
• Regression. Instead of generating a label distribution, we can have the prediction network
output a real-valued score. For example, by adding a Sigmoid layer to the prediction net-
work, the system can be employed to compute the similarity between two given sentences.
The architecture is the same as that of BERT-based classiﬁcation systems, with only the
change of the output layer.
Number (similarity, evaluation score, etc.)
hclsh1h2▷▷▷eclse1e2▷▷▷em
[CLS]x1x2▷▷▷xm
Text 1
hm
▷▷▷hlen−1 hlen
em+1 em+2 em+3▷▷▷elen−1 elen
[SEP]▷▷▷
hm+1 hm+2 hm+3
BERT
y1
y2
yn
[SEP]
Text 2
For training or ﬁne-tuning, we can minimize the regression loss of the model output as
usual.
• Sequence Labeling. Sequence labeling is a machine learning approach applicable to a wide
range of NLP problems. This approach assigns a label to each token in an input sequence,
and some linguistic annotations can then be derived from this sequence of labels. An ex-
ample of sequence labeling in NLP is part-of-speech (POS) tagging. We label each word
in a sentence with its corresponding POS tag. Another example is named entity recognition
(NER) in which we label each word with an NER tag, and named entities are identiﬁed
using these tags. See below for an illustration of the model architecture for NER.1.4 Applying BERT Models
33
Tag
Tag
Tag
{B, I, O}{B, I, O}
hcls
h1
h2
{B, I, O}
▷▷▷
hmhm+1
BERT
eclse1e2▷▷▷emem+1
[CLS]x1x2▷▷▷xm[SEP]
Here {B, I, O} is the tag set of NER. For example, B-ORG means the beginning of an
organization, I-ORG means the word is inside an organization, and O means the word does
not belong to any named entity. This NER model can output a distribution over the tag set
at each position, denoted as pi . The training or ﬁne-tuning of the model can be performed
over these distributions {p1 , ▷▷▷, pm }. For example, suppose pi (tagi ) is the probability of
the correct tag at position i. The training loss can be deﬁned to be the negative likelihood
Loss = −
m
1 
log pi (tagi )
m i=1
(1.23)
Finding the best label sequence given a trained NER model is a well-studied issue in NLP.
This is often achieved via dynamic programming, which, in the context of path ﬁnding over
a lattice, has linear complexity [Huang, 2009].
• Span Prediction. Some NLP tasks require predicting a span in a text. A common example
is reading comprehension. In this task, we are given a query x1 ▷▷▷xm and a context text
y1 ▷▷▷yn . The goal is to identify a continuous span in y1 ▷▷▷yn that best answers the query.
This problem can be framed as a sequence labeling-like task in which we predict a label for
each yj to indicate the beginning or ending of the span. Following Seo et al. [2017], we
add two networks on top of the BERT output for yj : one for generating the probability of
yj being the beginning of the span (denoted by pbeg
j ), and one for generating the probability
of yj being the ending of the span (denoted by pend
j ). The resulting model architecture is
shown as follows
End
End
(pend
)
1
Beg
beg
(p1
hclsh1h2▷▷▷eclse1e2▷▷▷em
[CLS]x1x2▷▷▷xm
Query
hm
End
(pend
)
2
(pend
n )
Beg
)
beg
(p2
Beg
beg
)
(pn
)
▷▷▷hlen−1 hlen
em+1 em+2 em+3▷▷▷elen−1 elen
[SEP]▷▷▷
hm+1 hm+2 hm+3
BERT
y1
y2
Context Text
yn
[SEP]34
Pre-training
We pack the query and context text together to obtain the input sequence. The prediction
networks are only applied to outputs for the context text, generating the probabilities pbeg
j
and pend
at each position. The loss can be computed by summing the negative log likeli-
j
hoods of the two models across the entire context text.
Loss = −
n


1
log pbeg
+ log pend
j
j
n j=1
(1.24)
At test time, we search for the best span by
(ĵ1 , ĵ2 ) =
arg max
1≤j1 ≤j2 ≤n

end
log pbeg
j1 + log pj2

(1.25)
• Encoding for Encoder-Decoder Models. While our focus in this section has been pri-
marily on language understanding problems, it is worth noting that BERT models can be
applied to a broader range of NLP tasks. In fact, BERT models can be used in all the scenar-
ios where we need to encode a piece of text. One application that we have not mentioned is
text generation which includes a range of tasks such as machine translation, summarization,
question answering, and dialogue generation. These tasks can be formulated as sequence-
to-sequence problems: we use an encoder to represent the source text, and a decoder to
generate the corresponding target text. A straightforward method to apply BERT models
is to consider them as encoders. Before ﬁne-tuning, we can initialize the parameters of the
encoder with those from a pre-trained BERT model. Then, the encoder-decoder model can
be ﬁne-tuned on pairs of texts as usual. The following shows the architecture of a neural
machine translation system where a BERT model is applied on the source side.
Target Text
Adapter
y1
y2
BERT (Encoder)
y3
▷▷▷yn
Decoder
exclsex1▷▷▷exmexm+1ey0ey1ey2▷▷▷eyn−1
[CLS]x1▷▷▷xm[SEP]⟨s⟩y1y2▷▷▷yn−1
Source Text
Here x1 ▷▷▷xm denotes the source sequence, y1 ▷▷▷yn denotes the target sequence, ex1 ▷▷▷exm
denotes the embedding sequence of x1 ▷▷▷xm , and ey1 ▷▷▷eyn denotes the embedding sequence
of y1 ▷▷▷yn . The adapter, which is optional, maps the output of the BERT model to the form
that is better suited to the decoder.
Fine-tuning BERT models is a complicated engineering problem, inﬂuenced by many factors,
such as the amount of ﬁne-tuning data, the model size, and the optimizer used in ﬁne-tuning.
In general, we wish to ﬁne-tune these models sufﬁciently so that they can perform well in the
downstream tasks. However, ﬁne-tuning BERT models for speciﬁc tasks may lead to overﬁtting,1.5 Summary
35
which in turn reduces their ability to generalize to other tasks. For example, suppose we have a
BERT model that performs well on a particular task. If we then ﬁne-tune it for new tasks, this may
decrease its performance on the original task. This problem is related to the catastrophic forget-
ting problem in continual training, where a neural network forgets previously learned information
when updated on new samples. In practical applications, a common way to alleviate catastrophic
forgetting is to add some old data into ﬁne-tuning and train the model with more diverse data.
Also, one may use methods specialized to catastrophic forgetting, such as experience replay [Rol-
nick et al., 2019] and elastic weight consolidation [Kirkpatrick et al., 2017]. The interested reader
can refer to some surveys for more detailed discussions of this issue in continual learning [Parisi
et al., 2019; Wang et al., 2023a;e].
1.5
Summary
In this chapter we have discussed the general idea of pre-training in NLP. In particular, we have dis-
cussed self-supervised pre-training and its application to encoder-only, decoder-only, and encoder-
decoder architectures. Moreover, we have presented and compared a variety of pre-training tasks
for these architectures. As an example, BERT is used to illustrate how sequence models are pre-
trained via masked language modeling and applied to different downstream tasks.
Recent years have shown remarkable progress in NLP, led by the large-scale use of self-
supervised pre-training. And sweeping advances are being made across many tasks, not only
in NLP but also in computer vision and other areas of AI. One idea behind these advances is that a
signiﬁcant amount of knowledge about the world can be learned by simply training these AI sys-
tems on huge amounts of unlabeled data. For example, a language model can learn some general
knowledge of a language by repeatedly predicting masked words in large-scale text. As a result,
this pre-trained language model can serve as a foundation model, which can be easily adapted to
address speciﬁc downstream NLP tasks. This paradigm shift in NLP has enabled the development
of incredibly powerful systems for language understanding, generation, and reasoning [Manning,
2022]. However, it is important to recognize that we are still in the early stages of creating truly in-
telligent systems, and there is a long way to go. Nevertheless, large-scale pre-training has opened
a door to intelligent systems that researchers have long aspired to develop, though several key re-
search areas remain open for exploration, such as learning intelligence efﬁciently using reasonably
small-sized data and acquiring complex reasoning and planning abilities.
Note that this chapter is mostly introductory and cannot cover all aspects of pre-training. For
example, there are many methods to ﬁne-tune a pre-trained model, offering different ways to better
adapt the model to diverse situations. Moreover, large language models, which are considered one
of the most signiﬁcant achievements in AI in recent years, are skipped in this section. We leave
the discussion of these topics to the following chapters.https://github.com/NiuTrans/NLPBook
https://niutrans.github.io/NLPBook
C HAPTER 2
Generative Models
One of the most signiﬁcant advances in NLP in recent years might be the development of large
language models (LLMs). This has helped create systems that can understand and generate nat-
ural languages like humans. These systems have even been found to be able to reason, which
is considered a very challenging AI problem. With these achievements, NLP made big strides
and entered a new era of research in which difﬁcult problems are being solved, such as building
conversational systems that can communicate with humans smoothly.
The concept of language modeling or probabilistic language modeling dates back to early ex-
periments conducted by Shannon [1951]. In his work, a language model was designed to estimate
the predictability of English — how well can the next letter of a text be predicted when the pre-
ceding N letters are known. Although Shannon’s experiments were preliminary, the fundamental
goals and methods of language modeling have remained largely unchanged over the decades since
then. For quite a long period, particularly before 2010, the dominant approach to language mod-
eling was the n-gram approach [Jurafsky and Martin, 2008]. In n-gram language modeling, we
estimate the probability of a word given its preceding n − 1 words, and thus the probability of a
sequence can be approximated by the product of a series of n-gram probabilities. These proba-
bilities are typically estimated by collecting smoothed relative counts of n-grams in text. While
such an approach is straightforward and simple, it has been extensively used in NLP. For example,
the success of modern statistical speech recognition and machine translation systems has largely
depended on the utilization of n-gram language models [Jelinek, 1998; Koehn, 2010].
Applying neural networks to language modeling has long been attractive, but a real break-
through appeared as deep learning techniques advanced. A widely cited study is Bengio et al.
[2003]’s work where n-gram probabilities are modeled via a feed-forward network and learned
by training the network in an end-to-end fashion. A by-product of this neural language model
is the distributed representations of words, known as word embeddings. Rather than represent-
ing words as discrete variables, word embeddings map words into low-dimensional real-valued
vectors, making it possible to compute the meanings of words and word n-grams in a continu-
ous representation space. As a result, language models are no longer burdened with the curse of
dimensionality, but can represent exponentially many n-grams via a compact and dense neural
model.
The idea of learning word representations through neural language models inspired subsequent
research in representation learning in NLP. However, this approach did not attract signiﬁcant in-
terest in developing NLP systems in the ﬁrst few years after its proposal. Starting in about 2012,
though, advances were made in learning word embeddings from large-scale text via simple word
prediction tasks. Several methods, such as Word2Vec, were proposed to effectively learn such
embeddings, which were then successfully applied in a variety of NLP systems [Mikolov et al.,
2013a;b]. As a result of these advances, researchers began to think of learning representations of
sequences using more powerful language models, such as LSTM-based models [Sutskever et al.,
2014; Peters et al., 2018]. And further progress and interest in sequence representation exploded
after Transformer was proposed. Alongside the rise of Transformer, the concept of language mod-
eling was generalized to encompass models that learn to predict words in various ways. Many2.1 A Brief Introduction to LLMs
37
powerful Transformer-based models were pre-trained using these word prediction tasks, and suc-
cessfully applied to a variety of downstream tasks [Devlin et al., 2019].
Indeed, training language models on large-scale data has led NLP research to exciting times.
While language modeling has long been seen as a foundational technique with no direct link to
the goals of artiﬁcial intelligence that researchers had hoped for, it helps us see the emergence of
intelligent systems that can learn a certain degree of general knowledge from repeatedly predicting
words in text. Recent research demonstrates that a single, well-trained LLM can handle a large
number of tasks and generalize to perform new tasks with a small adaptation effort [Bubeck et al.,
2023]. This suggests a step towards more advanced forms of artiﬁcial intelligence, and inspires
further exploration into developing more powerful language models as foundation models.
In this chapter, we consider the basic concepts of generative LLMs. For simplicity, we use the
terms large language models or LLMs to refer to generative models like GPT, though this term
can broadly cover other types of models like BERT. We begin by giving a general introduction
to LLMs, including the key steps of building such models. We then discuss two scaling issues of
LLMs: how LLMs are trained at scale, and how LLMs can be improved to handle very long texts.
Finally, we give a summary of these discussions.
2.1
A Brief Introduction to LLMs
In this section we give an introduction to the basic ideas of LLMs as required for the rest of this
chapter and the following chapters. We will use terms word and token interchangeably. Both
of them refer to the basic units used in language modeling, though their original meanings are
different.
Before presenting details, let us ﬁrst consider how language models work. The goal of lan-
guage modeling is to predict the probability of a sequence of tokens occurring. Let {x0 , x1 , ▷▷▷, xm }
be a sequence of tokens, where x0 is the start symbol ⟨s⟩ (or ⟨SOS⟩)1 . The probability of this se-
quence can be deﬁned using the chain rule
Pr(x0 , ▷▷▷, xm ) = Pr(x0 ) · Pr(x1 |x0 ) · Pr(x2 |x0 , x1 ) · · · Pr(xm |x0 , ▷▷▷, xm−1 )
=
m

i=0
Pr(xi |x0 , ▷▷▷, xi−1 )
(2.1)
or alternatively in a logarithmic form
log Pr(x0 , ▷▷▷, xm ) =
m

i=0
log Pr(xi |x0 , ▷▷▷, xi−1 )
(2.2)
Here Pr(xi |x0 , ▷▷▷, xi−1 ) is the probability of the token xi given all its previous tokens {x0 , ▷▷▷, xi−1 }
In the era of deep learning, a typical approach to language modeling is to estimate this
2.
The start symbol can also be [CLS] following BERT models.
We assume that when i = 0, Pr(xi |x0 , ▷▷▷, xi−1 ) =
Pr(x0 ) Pr(x1 , ▷▷▷, xm |x0 ) = Pr(x1 , ▷▷▷, xm |x0 ).
1
2
Pr(x0 )
=
1.
Hence Pr(x0 , ▷▷▷, xm )
=38
Generative Models
ContextPredictDecision RuleSequence Probability
⟨s⟩ ab⟨s⟩ a bcarg maxx2 ∈V Pr(x2 |⟨s⟩ a)Pr(⟨s⟩) · Pr(a|⟨s⟩)· Pr(b|⟨s⟩ a)
⟨s⟩ a b cdarg maxx4 ∈V Pr(x4 |⟨s⟩ a b c)Pr(⟨s⟩) · Pr(a|⟨s⟩) · Pr(b|⟨s⟩ a)·
Pr(c|⟨s⟩ a b)· Pr(d|⟨s⟩ a b c)
arg maxx3 ∈V Pr(x3 |⟨s⟩ a b)
Pr(⟨s⟩) · Pr(a|⟨s⟩) · Pr(b|⟨s⟩ a)·
Pr(c|⟨s⟩ a b)
Table 2.1: Illustration of generating the three tokens b c d given the preﬁx ⟨s⟩ a via a language model. In each step,
the model picks a token xi from V so that Pr(xi |x0 , ▷▷▷, xi−1 ) is maximized. This token is then appended to the end
of the context sequence. In the next step, we repeat the same process, but based on the new context.
probability using a deep neural network. Neural networks trained to accomplish this task re-
ceive a sequence of tokens x0 , ▷▷▷, xi−1 and produce a distribution over the vocabulary V (de-
noted by Pr(·|x0 , ▷▷▷, xi−1 )). The probability Pr(xi |x0 , ▷▷▷, xi−1 ) is the value of the i-th entry of
Pr(·|x0 , ▷▷▷, xi−1 ).
When applying a trained language model, a common task is to ﬁnd the most likely token given
its previous context tokens. This token prediction task can be described as
x̂i = arg max Pr(xi |x0 , ▷▷▷, xi−1 )
xi ∈V
(2.3)
We can perform word prediction multiple times to generate a continuous text: each time we
predict the best token x̂i , and then add this predicted token to the context for predicting the next
token x̂i+1 . This results in a left-to-right generation process implementing Eqs. (2.1) and (2.2). To
illustrate, consider the generation of the following three words given the preﬁx ‘⟨s⟩ a’, as shown
in Table 2.1. Now we discuss how LLMs are constructed, trained, and applied.
2.1.1
Decoder-only Transformers
As is standard practice, the input of a language model is a sequence of tokens (denoted by
{x0 , ▷▷▷, xm−1 }). For each step, an output token is generated, shifting the sequence one po-
sition forward for the next prediction. To do this, the language model outputs a distribution
Pr(·|x0 , ▷▷▷, xi−1 ) at each position i, and the token xi is selected according to this distribution.

3
This model is trained by maximizing the log likelihood m
i=1 log Pr(xi |x0 , ▷▷▷, xi−1 ) .
Here, we focus on the decoder-only Transformer architecture, as it is one of the most popular
model architectures used in LLMs. The input sequence of tokens is represented by a sequence
of de -dimensional vectors {e0 , ▷▷▷, em−1 }. ei is the sum of the token embedding of xi and the
positional embedding of i. The major body of the model is a stack of Transformer blocks (or
layers). Each Transformer block has two stacked sub-layers, one for self-attention modeling and
one for FFN modeling. These sub-layers can be deﬁned using the post-norm architecture
output = LNorm(F (input) + input)
3
Note that
m
i=1
log Pr(xi |x0 , ▷▷▷, xi−1 ) =
m
i=0
log Pr(xi |x0 , ▷▷▷, xi−1 ) since log Pr(x0 ) = 0.
(2.4)2.1 A Brief Introduction to LLMs
39
or the pre-norm architecture
output = LNorm(F (input)) + input
(2.5)
where input and output denote the input and output, both being an m × d matrix. The i-th rows
of input and output can be seen as contextual representations of the i-th token in the sequence.
F (·) is the core function of a sub-layer. For FFN sub-layers, F (·) is a multi-layer FFN. For
self-attention sub-layers, F (·) is a multi-head self-attention function. In general, self-attention is
expressed in a form of QKV attention
QKT
Attqkv (Q, K, V) = Softmax( √ + Mask)V
d
(2.6)
where Q, K and V ∈ Rm×d are the queries, keys, and values, respectively. It is important to
note that only previous tokens are considered when predicting a token. So a masking variable
Mask ∈ Rm×m is incorporated into self-attention to achieve this. The entry (i, k) of Mask has
a value of 0 if i ≤ k, and a value of − inf otherwise.
Given a representation H ∈ Rm×d , the multi-head self-attention function can be deﬁned as
F (H) = Merge(head1 , ▷▷▷, headτ )Whead
(2.7)
where Merge(·) representees a concatenation of its inputs, and Whead ∈ Rd×d represents a pa-
rameter matrix. headj is the output of QKV attention on a sub-space of representation
headj
= Attqkv (Q[j] , K[j] , V[j] )
(2.8)
Q[j] ,K[j] ,and V[j] are the queries, keys, and values projected onto the j-th sub-space via linear
transformations
Q[j] = HWjq
K[j]=
V[j]=
HWjk
HWjv
(2.9)
(2.10)
(2.11)
where Wjq , Wjk , and Wjv ∈ Rd× τ are the parameter matrices of the transformations.
d
Suppose we have L Transformer blocks. A Softmax layer is built on top of the output of the
last block. The Softmax layer outputs a sequence of m distributions over the vocabulary, like this


Pr(·|x0 , ▷▷▷, xm−1 )


..


.



Pr(·|x0 , x1 )
Pr(·|x0 )



= Softmax(HL Wo )
(2.12)
where HL is the output of the last Transformer block, and Wo ∈ Rd×|V | is the parameter matrix.
Figure 2.1 shows the Transformer architecture for language modeling. Applying this language40
Generative Models
Post-norm or Pre-norm
FFN
x2
▷▷▷
xm
L Blocks
x1
Pr(xm |x0 x1 ▷▷▷xm−1 )
Pr(x2 |x0 x1 )
Pr(x1 |x0 )
Post-norm or Pre-norm
...
h0L
h1L
▷▷▷
Self-attention
L
hm−1
Language Model
e0e1▷▷▷em−1
x0x1▷▷▷xm−1
z0
z1
▷▷▷
zm−1
Fig. 2.1: The Transformer-decoder architecture for language modeling. The central components are L stacked Trans-
former blocks, each comprising a self-attention sub-layer and an FFN sub-layer. To prevent the model from accessing
the right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to
generate a probability distribution for the next token, given the sequence of previous tokens. During inference, the
model takes the previously predicted token to predict the next one, repeating this process until the end of the sequence
L
} denote the outputs of the
is reached. {z0 , ▷▷▷, zm−1 } denote the inputs of a Transformer block, and {h0L , ▷▷▷, hm−1
last Transformer block.
model follows an autoregressive process. Each time the language model takes a token xi−1 as
input and predicts a token xi that maximizes the probability Pr(xi |x0 , ▷▷▷, xi−1 ). It is important
to note that, despite different implementation details, many LLMs share the same architecture
described above. These models are called large because both their depth and width are signiﬁcant.
Table 2.2 shows the model sizes for a few LLMs, as well as their model setups.
2.1.2
Training LLMs
Now suppose that we are given a training set D comprising K sequences. The log-likelihood of
each sequence x = x0 ▷▷▷xm in D can be calculated using a language model
Lθ (x) =
m

i=1
log Prθ (xi |x0 , ▷▷▷, xi−1 )
(2.13)
Here the subscript θ afﬁxed to L(·) and Pr(·) denotes the parameters of the language model. Then,
the objective of maximum likelihood training is deﬁned as
θ̂ = arg max
θ

x∈D
Lθ (x)
(2.14)
Training Transformer-based language models with the above objective is commonly viewed
as a standard optimization process for neural networks. This can be achieved using gradient
descent algorithms, which are widely supported by off-the-shelf deep learning toolkits. Somewhat2.1 A Brief Introduction to LLMs
41
# of ParametersDepth LWidth d# of Heads
(Q/KV)
0.117B
1.5B
175B12
48
96768
1,600
12,28812/12
25/25
96/96
LLaMA2 [Touvron et al., 2023b]7B
13B
70B32
40
804,096
5,120
8,19232/32
40/40
64/64
LLaMA3/3.1 [Dubey et al., 2024]8B
70B
405B32
80
1264,096
8,192
16,38432/8
64/8
128/8
Gemma2 [Team et al., 2024]2B
9B
37B26
42
462,304
3,584
4,6088/4
16/8
32/16
Qwen2.5 [Yang et al., 2024]0.5B
7B
72B24
28
80896
3,584
8,19214/2
28/4
64/8
DeepSeek-V3 [Liu et al., 2024a]671B617,168128/128
Falcon [Penedo et al., 2023]7B
40B
180B32
60
804,544
8,192
14,84871/71
128/128
232/232
Mistral [Jiang et al., 2023a]7B324,09632/32
LLM
GPT-1 [Radford et al., 2018]
GPT-2 [Radford et al., 2019]
GPT-3 [Brown et al., 2020]
Table 2.2: Comparison of some LLMs in terms of model size, model depth, model width, and number of heads (a◁b
means a heads for queries and b heads for both keys and values).
surprisingly, better results were continuously yielded as language models were evolved into more
computationally intensive models and trained on larger datasets [Kaplan et al., 2020]. These
successes have led NLP researchers to continue increasing both the training data and model size
in order to build more powerful language models.
However, as language models become larger, we confront new training challenges, which
signiﬁcantly change the problem compared to training relatively small models. One of these
challenges arises from the need for large-scale distributed systems to manage the data, model
parameters, training routines, and so on. Developing and maintaining such systems requires a
signiﬁcant amount of work in both software and hardware engineering, as well as expertise in deep
learning. A related issue is that when the training is scaled up, we need more computing resources
to ensure the training process can be completed in an acceptable time. For example, it generally
requires hundreds or thousands of GPUs to train an LLM with tens of billions of parameters
from scratch. This requirement drastically increases the cost of training such models, especially
considering that many training runs are needed as these models are developed. Also, from the
perspective of deep learning, the training process can become unstable if the neural networks are
very deep and/or the model size is very large. In response, we typically need to modify the model
architecture to adapt LLMs to large-scale training. In Section 2.2 we will present more discussions
on these issues.42
Generative Models
2.1.3
Fine-tuning LLMs
Once we have pre-trained an LLM, we can then apply it to perform various NLP tasks. Tradi-
tionally language models are used as components of other systems, for example, they are widely
applied to score translations in statistical machine translation systems. By contrast, in generative
AI, LLMs are considered complete systems and are employed to address NLP problems by mak-
ing use of their generation nature. A common approach is to describe the task we want to address
in text and then prompt LLMs to generate text based on this description. This is a standard text
generation task where we continue or complete the text starting from a given context.
More formally, let x = x0 ▷▷▷xm denote a token sequence of context given by users, and
y = y1 ▷▷▷yn denote a token sequence following the context. Then, the inference of LLMs can be
deﬁned as a problem of ﬁnding the most likely sequence y based on x:
ŷ = arg max log Pr(y|x)
y
= arg max
y
n

i=1
log Pr(yi |x0 , ▷▷▷, xm , y1 , ▷▷▷, yi−1 )
(2.15)

Here ni=1 log Pr(yi |x0 , ▷▷▷, xm , y1 , ▷▷▷, yi−1 ) essentially expresses the same thing as the right-
hand side of Eq. (2.2). It models the log probability of predicting tokens from position m + 1,
rather than position 0. Throughout this chapter and subsequent ones, we will employ separate
variables x and y to distinguish the input and output of an LLM, though they can be seen as sub-
sequences from the same sequence. By adopting such notation, we see that the form of the above
equation closely resembles those used in other text generation models in NLP, such as neural
machine translation models.
To illustrate how LLMs are applied, consider the problem of determining the grammaticality
for a given sentence. We can deﬁne a template like this
{*sentence*}
Question: Is this sentence grammatically correct?
Answer:
Here
represents the text we intend to generate. {*sentence*} is a placeholder variable that
will be replaced by the actual sentence provided by the users. For example, suppose we have a
sentence “John seems happy today.”. We can replace the {*sentence*} in the template with this
sentence to have an input to the language model
John seems happy today.
Question: Is this sentence grammatically correct?
Answer:
To perform the task, the language model is given the context x =“John seems happy today .∩n
Question : Is this sentence grammatically correct?∩n Answer :”4 . It then generates the following
4
∩n is a special character used for line breaks.2.1 A Brief Introduction to LLMs
43
text as the answer, based on the context. For example, the language model may output “Yes” (i.e.,
y = “Yes”) if this text is the one with the maximum probability of prediction given this context.
Likewise, we can deﬁne more templates to address other tasks. For example, we can translate
an English sentence into Chinese using the following template
{*sentence*}
Question: What is the Chinese translation of this English sentence?
Answer:
or using an instruction-like template
{*sentence*}
Translate this sentence from English into Chinese.
or using a code-like template.
[src-lang] = English [tgt-lang] = Chinese [input] = {*sentence*}
[output] =
The above templates provide a simple but effective method to “prompt” a single LLM to per-
form various tasks without adapting the structure of the model. However, this approach requires
that the LLM can recognize and follow the instructions or questions. One way to do this is to incor-
porate training samples with instructions and their corresponding responses into the pre-training
dataset. While this method is straightforward, building and training LLMs from scratch is com-
putationally expensive. Moreover, making instruction-following data effective for pre-training
requires a signiﬁcant amount of such data, but collecting large-scale labeled data for all tasks of
interest is very difﬁcult.
A second method, which has been a de facto standard in recent research, is to adapt LLMs
via ﬁne-tuning. As such, the token prediction ability learned in the pre-training phase can be
generalized to accomplish new tasks. The idea behind ﬁne-tuning is that some general knowledge
of language has been acquired in pre-training, but we need a mechanism to activate this knowledge
for applying it to new tasks. To achieve this, we can slightly ﬁne-tune the model parameters using
instruction-following data. This approach is called instruction ﬁne-tuning.
An instruction ﬁne-tuning sample, which is represented by a sequence of tokens, can be seen
as a tuple consisting of an input and the desired output. Here, the input includes instructions,
system information (or system preﬁx), and any other user-provided information5 . To illustrate,
consider the following examples (blue text = input and underlined text = output).
5
System information refers to a sequence of tokens added at the beginning of an input in order to guide the behavior
of an LLM, such as, you are a helpful assistant and should not output toxic content.44
Generative Models
Read the following sentence, and answer whether it is grammatically correct.
LLMs are powerful models but are expensive to build.
Yes
LLMs are powerful models but are expensive to build.
Does this sentence make sense grammatically?
Answer Option
- Yes
- No
Yes
Yes or No? Yes = the following sentence is grammatically correct. No = it con-
tains grammatical errors.
LLMs are powerful models but are expensive to build.
Yes
All these samples describe the same binary classiﬁcation task, but with different instructions.
To increase the diversity in the ﬁne-tuning data and make LLMs generalize, we can deﬁne more
tasks using instructions.
Write a short story about three characters.
Character 1: A boy whose name is Andy.
Character 2: Andy’s dog.
Character 3: Andy’s friend, Jane.
In the heart of a bustling city, Andy, his loyal dog Rocket, and his friend
Jane discovered a hidden garden beneath the glow of twilight. Surrounded by
ancient trees and whispering winds, they shared dreams and secrets, forging an
unbreakable bond. As the night deepened, they made a pact under the stars to
always ﬁnd magic in the mundane. Together, they realized that true adventure lay
in the moments spent with friends, and every day was a treasure waiting to be
discovered.2.1 A Brief Introduction to LLMs
45
Show me a recipe for making ice cream.
Ingredients:
2 cups heavy cream; 1 cup milk; 3/4 cup sugar; 2 tsp vanilla extract;
Pinch of salt.
Steps:
1. Whisk all ingredients until sugar dissolves.
2. Chill mixture for 2 hours in the fridge.
3. Freeze in a shallow dish, stirring every 30 minutes, 3-4 times.
4. Freeze solid for 2 hours.
5. Serve your homemade ice cream.
If you buy 5 apples and each apple costs $1.20, how much do you spend in total?
$6.00
Write a Python program to calculate the sum of squares of the following numbers.
1 , 2 , 10 , -9 , 78
numbers = [1,2,10,-9 ,78]
sum_of_squares = sum(x**2 for x in numbers)
print(sum_of_squares)
To acquire instruction-following abilities, a certain amount of ﬁne-tuning data is required.
This data may include diverse instructions and possible responses. It has been found that scaling
the number of ﬁne-tuning tasks is beneﬁcial for improving the performance of LLMs [Chung
et al., 2022]. Note that although more ﬁne-tuning data is favorable, the amount of this data is
generally orders of magnitude smaller than that of the pre-training data. For example, LLMs can
be ﬁne-tuned with tens or hundreds of thousands of samples, or even fewer if these samples are
of high quality [Zhou et al., 2023a; Chen et al., 2023b], whereas pre-training such models may
require billions or trillions of tokens, resulting in signiﬁcantly larger computational demands and
longer training times [Touvron et al., 2023a].
It is also worth noting that we should not expect the ﬁne-tuning data to cover all the down-
stream tasks to which we intend to apply LLMs. A common understanding of how the pre-training
+ ﬁne-tuning approach works is that LLMs have gained knowledge for understanding instructions
and generating responses in the pre-training phase. However, these abilities are not fully activated
until we introduce some form of supervision. The general instruction-following behavior emerges
as we ﬁne-tune the models with a relatively small amount of labeled data. As a result, we can
achieve some level of zero-shot learning: the ﬁne-tuned models can handle new tasks that they
have not been explicitly trained or ﬁne-tuned for [Sanh et al., 2022; Wei et al., 2022a]. This zero-
shot learning ability distinguishes generative LLMs from earlier pre-trained models like BERT,
which are primarily ﬁne-tuned for speciﬁc tasks.
Once we have prepared a collection of instruction-described data, the ﬁne-tuning process is
relatively simple. This process can be viewed as a standard training process as pre-training, but on
a much smaller training dataset. Let Dtune be the ﬁne-tuning dataset and θ̂ be the model parameters46
Generative Models
optimized via pre-training. We can modify Eq. (2.14) to obtain the objective of ﬁne-tuning
θ̃ = arg max
θ̂ +

sample∈Dtune
Lθ̂+ (sample)
(2.16)
Here θ̃ denotes the optimal parameters. The use of notation θ̂+ means that the ﬁne-tuning starts
with the pre-trained parameters θ̂.
For each sample ∈ Dtune , we divide it into an input segment xsample and an output segment
ysample , that is,
sample = [ysample , xsample ]
(2.17)
We then deﬁne the loss function to be
Lθ̂+ (sample) = − log Prθ̂+ (ysample |xsample )
(2.18)
In other words, we compute the loss over the sub-sequence ysample , rather than the entire sequence.
In a practical implementation of back-propagation for this equation, the sequence [ysample , xsample ]
is constructed in the forward pass as usual. However, in the backward pass, error gradients are
propagated back only through the parts of the network that correspond to ysample , leaving the rest
of the network unchanged. As an example, consider a sequence
⟨s⟩ Square this number . 2 . The result is 4 .


Context (Input)
 


Prediction (Output)
The loss is calculated and back propagated only for The result is 4 ..
Instruction ﬁne-tuning also requires substantial engineering work. In order to achieve satis-
factory results, one may experiment with different settings of the learning rate, batch size, number
of ﬁne-tuning steps, and so on. This typically requires many ﬁne-tuning runs and evaluations. The
cost and experimental effort of ﬁne-tuning remain critical and should not be overlooked, though
they are much lower than those of the pre-training phase.
While we focus on instruction ﬁne-tuning for an illustrative example here, ﬁne-tuning tech-
niques play an important role in developing various LLMs and are more widely used. Examples
include ﬁne-tuning LLMs as chatbots using dialog data, and adapting these models to handle very
long sequences. The wide application of ﬁne-tuning has led researchers to improve these tech-
niques, such as designing more efﬁcient ﬁne-tuning algorithms. While the research on ﬁne-tuning
is fruitful, in this section we just give a ﬂavour of the key steps involved. We will see more detailed
discussions on this topic in the following chapters.
2.1.4
Aligning LLMs with the World
Instruction ﬁne-tuning provides a simple way to adapt LLMs to tasks that can be well deﬁned. This
problem can broadly be categorized as an alignment problem. Here, alignment is referred to as a
process of guiding LLMs to behave in ways that align with human intentions. The guidance can
come from labeled data, human feedback, or any other form of human preferences. For example,2.1 A Brief Introduction to LLMs
47
we want LLMs not only to be accurate in following instructions, but also to be unbiased, truthful,
and harmless. So we need to supervise the models towards human values and expectations. A
common example is that when we ask an LLM how to build a weapon, it may provide a list of key
steps to do so if it is not carefully aligned. However, a responsible model should recognize and
avoid responding to requests for harmful or illegal information. Alignment in this case is crucial
for ensuring that LLMs act responsibly and in accordance with ethical guidelines.
A related concept to alignment is AI safety. One ultimate goal of AI is to build intelligent
systems that are safe and socially beneﬁcial. To achieve this goal we should keep these systems
robust, secure, and subjective, in any conditions of real-world use, even in conditions of misuse
or adverse use. For LLMs, the safety can be increased by aligning them with appropriate human
guidance, such as human labeled data and interactions with users during application.
Alignment is difﬁcult as human values and expectations are diverse and shifting. Sometimes,
it is hard to describe precisely what humans want, unless we see the response of LLMs to user
requests. This makes alignment no longer a problem of tuning LLMs on predeﬁned tasks, but a
bigger problem of training them with the interactions with the real world.
As a result of the concerns with controlling AI systems, there has been a surge in research
on the alignment issue for LLMs. Typically, two alignment steps are adopted after LLMs are
pre-trained on large-scale unlabeled data.
• Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained LLMs
on new, task-oriented, labelled data. A commonly used SFT technique is instruction ﬁne-
tuning. As described in the previous subsection, by learning from instruction-response
annotated data, LLMs can align with the intended behaviors for following instructions,
thereby becoming capable of performing various instruction-described tasks. Supervised
ﬁne-tuning can be seen as following the pre-training + ﬁne-tuning paradigm, and offers a
relatively straightforward method to adapt LLMs.
• Learning from Human Feedback. After an LLM ﬁnishes pre-training and supervised ﬁne-
tuning, it can be used to respond to user requests if appropriately prompted. But this model
may generate content that is unfactual, biased, or harmful. To make the LLM more aligned
with the users, one simple approach is to directly learn from human feedback. For example,
given some instructions and inputs provided by the users, experts are asked to evaluate how
well the model responds in accordance with their preferences and interests. This feedback
is then used to further train the LLM for better alignment.
A typical method for learning from human feedback is to consider it as a reinforcement learn-
ing (RL) problem, known as reinforcement learning from human feedback (RLHF) [Ouyang
et al., 2022]. The RLHF method was initially proposed to address general sequential decision-
making problems [Christiano et al., 2017], and was later successfully employed in the develop-
ment of the GPT series models [Stiennon et al., 2020]. As a reinforcement learning approach, the
goal of RLHF is to learn a policy by maximizing some reward from the environment. Speciﬁcally,
two components are built in RLHF:
• Agent. An agent, also called an LM agent, is the LLM that we want to train. This agent
operates by interacting with its environment: it receives a text from the environment and48
Generative Models
outputs another text that is sent back to the environment. The policy of the agent is the
function deﬁned by the LLM, that is, Pr(y|x).
• Reward Model. A reward model is a proxy of the environment. Each time the agent
produces an output sequence, the reward model assigns this output sequence a numerical
score (i.e., the reward). This score tells the agent how good the output sequence is.
In RLHF, we need to perform two learning tasks: 1) reward model learning, which involves
training a reward model using human feedback on the output of the agent, and 2) policy learning,
which involves optimizing a policy guided by the reward model using reinforcement learning
algorithms. Here is a brief outline of the key steps involved in RLHF.
• Build an initial policy using pre-training and instruction ﬁne-tuning.
• Use the policy to generate multiple outputs for each input, and then collect human feedback
on these outputs (e.g., comparisons of the outputs).
• Learn a reward model from the human feedback.
• Fine-tune the policy with the supervision from the reward model.
Figure 2.2 shows an overview of RLHF. Given that this section serves only as a brief intro-
duction to concepts of LLMs, a detailed discussion of RLHF techniques will not be included. We
instead illustrate the basic ideas behind RLHF using a simple example.
Suppose we have trained an LLM via pre-training and instruction ﬁne-tuning. This LLM is
deployed to respond to requests from users. For example, a user may input
How can I live a more environmentally friendly life?
We use the LLM to generate 4 different outputs (denoted by {y1 , ▷▷▷, y4 }) by sampling the
output space
Output 1 (y1 ): Consider switching to an electric vehicle or bicycle instead of
traditional cars to reduce carbon emissions and protect our planet.
Output 2 (y2 ): Adopt a minimalist lifestyle. Own fewer possessions to reduce
consumption and the environmental impact of manufacturing and
disposal.
Output 3 (y3 ): Go off-grid. Generate your own renewable energy and collect
rainwater to become completely self-sufﬁcient and reduce reliance
on non-renewable resources.
Output 4 (y4 ): Support local farm products to reduce the carbon footprint of
transporting food, while enjoying fresh, healthy food.2.1 A Brief Introduction to LLMs
49
Comparisons
y1 ≻ y4 ≻ y2 ≻ y3
SFT Data
Write a poem about the
weather in London . ...
Annotating Data with Human Preferences
Pre-training Data
Model Output
How can I get there? ...
I love the food here! ...
1. ............
3. ............
Pre-training &
Supervised ﬁne-tuning
2. ............
4. ............
Predicting
LLM
LLM
(a) Learning an Initial LLM
User Input
How can I live more
environmentally friendly?
(b) Annotating Data with Human Preferences
Reward Scores
{r(x, y)}
Evaluate the Input-output Pairs
Comparison Data
{(x, yk1 ≻ yk2 )}
RL Fine-tuning
Reward Model
Input-output Pairs
{x, y}
Sampling y via the Policy Pr(y|x)
Training
Reward Model
(c) Training the Reward Model
LLM
(Policy)
Dataset D
x∼D
(d) Training/Fine-tuning the Policy
Fig. 2.2: An overview of RLHF. There are 4 key steps involved: a) training an initial LLM (i.e., policy) using pre-
training and supervised ﬁne-tuning; b) collecting human preference data by ranking the outputs of the LLM; c) training
a reward model using the ranking results; d) RL ﬁne-tuning of the policy based on the reward model. Double line
arrows mean training or ﬁne-tuning.
We then ask annotators to evaluate these outputs. One straightforward way is to assign a rating
score to each output. In this case, the reward model learning problem can be framed as a task of
training a regression model. But giving numerical scores to LLM outputs is not an easy task for
annotators. It is usually difﬁcult to design an annotation standard that all annotators can agree on
and easily follow. An alternative method, which is more popular in the development of LLMs, is
to rank these outputs. For example, a possible ranking of the above outputs is
y1 ≻ y4 ≻ y2 ≻ y350
Generative Models
A reward model is then trained using this ranking result. In general, a reward model in RLHF
is a language model that shares the same architecture as the target LLM, but with a smaller model
size. Given the input x and output yk , we concatenate them to form a sequence seqk = [x, yk ].
This sequence is processed from left to right using forced decoding. Since each position can
only access its left context in language modeling, the output of the top-most Transformer layer at
the ﬁrst position cannot be used as the representation of the sequence. Instead, a special symbol
(e.g., ⟨∩s⟩) is added to the end of the sequence, and the corresponding output of the Transformer
layer stack is considered as the representation of the entire sequence. An output layer, such as a
linear transformation layer, is built on top of this representation to generate the reward, denoted
by R(seq k ) or R(x, yk ).
We train this reward model using ranking loss. For example, a pair-wise ranking loss function
can be written in the form
Lossω (Dr ) = −E(x,yk1 ,yk2 )∼Dr log(Sigmoid(Rω (x, yk1 ) − Rω (x, yk2 )))
(2.19)
where ω represents the parameters of the reward model, and Dr represents a set of tuples of an
input and a pair of outputs. (x, yk1 , yk2 ) ∼ Dr is a sampling operation which draws a sample
(x, yk1 , yk2 ) from Dr with some probability. As an example, suppose we ﬁrst draw a model
input x with a uniform distribution and then draw a pair of model outputs with a probability of
yk1 ≻ yk2 given x (denoted by Pr(yk1 ≻ yk2 |x)). The corresponding loss function is given by
Lossω (Dr )
= −
= −

Pr(x) · Pr(yk1 ≻ yk2 |x) · log(Sigmoid(Rω (x, yk1 ) − Rω (x, yk2 )))
1 
K
Pr(yk1 ≻ yk2 |x) · log(Sigmoid(Rω (x, yk1 ) − Rω (x, yk2 )))
(2.20)
where K represents the number of model inputs involved in sampling. While the form of these
functions may seem complex, their idea is simple: we penalize the model if the predicted ranking
of two outputs differs from the human-labeled ranking. By contrast, the model receives a bonus,
if the predicted ranking matches the human-labeled ranking.
We can train the reward model by minimizing the above ranking loss
ω̂ = arg min Lossω (Dr )
ω
(2.21)
The resulting model Rω̂ (·) can be employed to evaluate any given pair of input and output. Note
that although the reward model is trained using a ranking-based objective, it is used for scoring.
This allows it to provide continuous supervision signals, which is very beneﬁcial for training other
models.
We now turn to the policy learning problem. A commonly adopted objective is to maximize
the reward on a set of input-output pairs. Following an analogous form of Eq. (2.16), we obtain a
simple training objective for RL ﬁne-tuning
θ̃ = arg max E(x,yθ̂+ )∼Drlft Rω̂ (x, yθ̂+ )
θ̂ +
(2.22)
where the optimal parameters θ̃ are obtained by ﬁne-tuning the pre-trained parameters θ̂. Drlft is2.1 A Brief Introduction to LLMs
51
the RL ﬁne-tuning dataset. For each sample (x, yθ̂+ ), x is sampled from a prepared dataset of
input sequences, and yθ̂+ is sampled from the distribution Prθ̂+ (y|x) given by the policy.
In practice, more advanced reinforcement learning algorithms, such as proximal policy opti-
mization (PPO), are often used for achieving more stable training, as well as better performance.
We leave the detailed discussion of reinforcement learning algorithms to the following parts of
this book where RLHF is extensively used for alignment.
An interesting question arises here: why not consider learning from human preferences as
a standard supervised learning problem? This question is closely related to our aforementioned
discussion on the difﬁculty of data annotation. Often, describing human values and goals is chal-
lenging, and it is even more difﬁcult for humans to provide outputs that are well aligned. As an
alternative, annotating the preferences of a given list of model outputs offers a simpler task. By
doing so, we can create a model that understands human preferences, which can then be used as
a reward model for training policies. From the perspective of machine learning, RLHF is par-
ticularly useful for scenarios where the desired behavior of an agent is difﬁcult to demonstrate
but can be easily recognized by humans. Another advantage of RLHF is its ability to explore the
sample space. By employing sampling techniques, models trained with reinforcement learning
can venture beyond the annotated data set to explore additional samples. This exploratory ability
allows RLHF to discover potentially beneﬁcial policies that are not immediately apparent from
the labeled data alone.
2.1.5
Prompting LLMs
We have so far shown that LLMs can be used to perform various tasks by giving them appropriate
prompts. There are no restrictions on these prompts, which can include any information we wish
to ask or communicate with LLMs, such as natural language instructions and the context of con-
versations. Since this approach requires no additional training or tuning, adapting LLMs becomes
highly efﬁcient once they are developed. This somewhat inﬂuences the paradigms in NLP: we
no longer need to develop speciﬁc systems for individual tasks but can instead use a single, well-
trained LLM to perform different tasks by prompting it. An appealing aspect of LLM prompting
arises as a result: users can easily have “customized” systems by designing their own prompts
for LLMs. Given the important role played by prompting in LLMs, prompting engineering has
become a very active area of research in NLP.
The term prompt is used in many different ways in the literature. In this chapter, this term
refers to the entire input to LLMs, and so we use the terms prompt and model input interchange-
ably. Before discussing prompting further, let us ﬁrst see a few examples where the prompts
are more complex than those presented in the previous subsections. Note that this subsection is
not aimed at writing high-quality prompts but rather at highlighting some interesting issues in
prompting LLMs.
One of the popular ways to use LLMs is to assign them a “role” played in generating responses.
For example, LLMs can act as a psychologist when answering questions.52
Generative Models
Please explain what delayed gratiﬁcation is?
Note, you are a researcher with a deep background in psychology. Your own
ﬁeld of study is developmental psychology in children, with a focus on aspects
of control and intelligence. You are very familiar with the concept of delayed
gratiﬁcation and have applied it to your own research. You have published papers
on delayed gratiﬁcation and conducted related experiments.
Delayed gratiﬁcation is the process of resisting an immediate reward in
anticipation of receiving a more valuable reward in the future. It is a signiﬁcant
concept in developmental psychology, especially in relation to how children learn
self-control and develop cognitive abilities. Research shows that the ability to
delay gratiﬁcation is associated with positive outcomes in academic performance,
social competence, and emotional well-being. Studies and experiments in this
area highlight the importance of cultivating this skill early in life to support
long-term success and resilience.
Another example is the use of LLMs in detecting and correcting errors such as syntactic or
semantic mistakes in text. For an LLM which is trained on both code and natural language data,
we may use it for code debugging6 .
Fix the bugs in this C language program.
#include <stdio.h>
int main() {
printg("Hello, World!")
return 0;
}
1. The function name ’printg’ is incorrect . It should be ’printf’.
2. There is a missing semicolon at the end of the ’printf’ function call.
Here is the corrected version of the program:
#include <stdio.h>
int main() {
printf("Hello, World!");
return 0;
}
Also, LLMs can be ﬁne-tuned to act as a conversation partner 7 .
6
In this example, the code is not tokenized for easier reading.
To ﬁne-tune an LLM for multi-turn dialogue, one needs to consider conversation history in the context for pre-
dicting the response in the current round of conversation. This makes the actual prompt used in response generation
relatively longer than that used in single-turn dialogue.
72.1 A Brief Introduction to LLMs
53
Hi! I’m an assistant. How can I help you?
Who won the FIFA World Cup 2022?
Argentina won the FIFA World Cup 2022.
Where was it held?
The 2022 FIFA World Cup was held in Qatar.
How many times has Argentina won the World Cup?
Argentina has won the FIFA World Cup three times.
...
These examples and previous ones have shown that appropriate responses can be generated
via prompts involving clear instructions and questions. However, when problem solving requires
knowledge that is not explicitly speciﬁed, LLMs may make mistakes, even though the instructions
are sufﬁciently clear and precise. A family of challenging tasks for LLMs involves arithmetic
reasoning and commonsense reasoning. For example, we can ask an LLM to solve primary school
math problems presented in natural language.
Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more
apples. The next day, Jack gave 3 apples to his friend John. How many apples
does Jack have left in the end?
The answer is 10.
The correct answer should be 7, so the model output is incorrect.
One approach to addressing such issues is to incorporate learning into prompts, called in-
context learning or (ICL). The idea of ICL is to demonstrate the ways to solve problems in
prompts, and condition predictions on these demonstrations. Here is an example where a similar
problem and the corresponding answer are presented in the prompt (green = demonstrations).
Tom has 12 marbles. He wins 7 more marbles in a game with his friend but then
loses 5 marbles the next day. His brother gives him another 3 marbles as a gift.
How many marbles does Tom have now?
The answer is 17.
Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more
apples. The next day, Jack gave 3 apples to his friend John. How many apples
does Jack have left in the end?
The answer is 12.
But the LLM still made mistakes this time. A reason for this might be that solving math prob-
lems does not only involve problem-answer mappings but also, to a larger extent, the underlying
logical inference in multiple steps. A method to improve the inference abilities of LLMs is chain-
of-thought prompting (COT prompting) [Wei et al., 2022c]. In COT prompting, we decompose
complex reasoning problems into multiple problem-solving intermediate steps. These steps are54
Generative Models
demonstrated in prompts so that LLMs can be prompted to learn to mimic the underlying thought
processes in solving the problems. COT prompting has been found to be very useful for achieving
good results on challenging tasks, such as the GSM8K mathematical reasoning benchmark.
Consider the above demonstration example Tom has 12 marbles. He wins 7 more marbles in
a game with his friend but then loses 5 marbles the next day. His brother gives him another 3
marbles as a gift. How many marbles does Tom have now ?. The corresponding logical reasoning
steps can be structured:
1. Tom starts with 12 marbles.
2. He wins 7 more marbles in a game, so we add those to his initial count: 12 marbles + 7
marbles = 19 marbles.
3. The next day, Tom loses 5 marbles, so we subtract those from his current total: 19 marbles
- 5 marbles = 14 marbles.
4. His brother gives him 3 more marbles, so we add those to his current total: 14 marbles + 3
marbles = 17 marbles.
5. After all these events, Tom has 17 marbles.
We can add these reasoning steps into the prompt, and get the correct answer.
Tom has 12 marbles. He wins 7 more marbles in a game with his friend but then
loses 5 marbles the next day. His brother gives him another 3 marbles as a gift.
How many marbles does Tom have now?
Tom initially has 12 marbles. After winning a game, he gains 7 more, increasing
his total to 19 marbles. He loses 5 marbles the next day, bringing his total down
to 14 marbles. His brother gifts him 3 more marbles, increasing his total to 17
marbles. Therefore, Tom now has 17 marbles. So the answer is 17.
Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more
apples. The next day, Jack gave 3 apples to his friend John. How many apples
does Jack have left in the end?
Jack starts with 7 apples. He eats 2 apples for dinner, so we subtract 2 from 7,
leaving him with 5 apples. His mom gives him 5 more apples, adding 5 to his
current total. Now he has 5 + 5 = 10 apples. The next day, Jack gives 3 apples to
his friend John. Subtracting 3 from 10, Jack is left with 7 apples. Therefore, Jack
has 7 apples left. So the answer is 7.
This approach is also called one-shot COT prompting. Likewise, we can add more demon-
strations to have a few-shot COT prompting approach.
Furthermore, a special kind of COT, zero-shot COT, introduces instructions that provoke
LLMs to think, without presenting any intermediate reasoning steps in the prompts [Kojima et al.,
2022]. An interesting example is that by appending a simple sentence “Let’s think step by step.”
to the end of a prompt, LLMs can generate reasoning steps to reach the ﬁnal answer.2.1 A Brief Introduction to LLMs
55
Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more
apples. The next day, Jack gave 3 apples to his friend John. How many apples
does Jack have left in the end?
Let’s think step by step.
1. Initial Quantity: Jack starts with 7 apples.
2. After Dinner: He eats 2 apples, so 7 − 2 = 5 apples remain.
3. His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10
apples are now with him.
4. Giving to John: The next day, Jack gives 3 apples to his friend John,
so 10 − 3 = 7 apples are left.
In the end, Jack has 7 apples left.
Zero-shot, one-shot, and few-shot learning are common concepts in the area of in-context
learning for LLMs and are not restricted to COT prompting. Broadly speaking, any prompting
that involves only simple instructions without any demonstrations can be considered a form of
zero-shot learning. This zero-shot learning ability emerges as LLMs are pre-trained and/or ﬁne-
tuned. Also, one-shot and few-shot learning methods are more often considered when LLMs do
not acquire the corresponding zero-shot learning ability. These methods are therefore important
for in-context learning when addressing new tasks. Examples include those for performing various
NLP tasks by demonstrating task-formatted samples. See the following examples for sentiment
sentence classiﬁcation and phrase translation via few-shot learning.
Given the following text snippets, classify their sentiment as Positive, Negative,
or Neutral.
Example 1: “I had an amazing day at the park!”
Sentiment: Positive
Example 2: “The service at the restaurant was terrible.”
Sentiment: Negative
Example 3: “I think it’s going to rain today.”
Sentiment: Neutral
Text: “This movie was a fantastic journey through imagination.”
Sentiment: Positive
Translate the following Chinese phrases into English.
Example 1: “你好”
Translation: “Hello”
Example 2: “谢谢你”
Translation: “Thank you”
Phrase to translate: “早上好”
Translation: “Good Morning”56
Generative Models
# of Tokens
LLM
Data
GPT3-175B [Brown et al., 2020]0.5TWebpages, Books, Wikipedia
Falcon-180B [Almazrouei et al., 2023]3.5TWebpages, Books, Conversations,
Code, Technical Articles
LLaMA2-65B [Touvron et al., 2023a]1.0T ∼ 1.4TPaLM-450B [Chowdhery et al., 2022]0.78TWebpages, Books, Conversations,
Code, Wikipedia, News
6TWebpages, Mathematics, Code
Gemma-7B [Gemma Team, 2024]
Webpages, Code, Wikipedia,
Books, Papers, Q&As
Table 2.3: Amounts of training data used in some LLMs in terms of the number of tokens.
Above, we have presented examples to illustrate the fundamental in-context learning capa-
bilities of prompting LLMs. This section, however, does not include more advanced prompting
techniques in order to keep the content concise and compact. More discussions on prompting can
be found in Chapter 3.
2.2
Training at Scale
As a ﬁrst step in developing LLMs, we need to train these models on large amounts of data.
The training task is itself standard: the objective is to maximize the likelihood, which can be
achieved via gradient descent. However, as we scale up both the model size and the amount
of data, the problem becomes very challenging, for example, large models generally make the
training unstable. In this section, we discuss several issues of large-scale training for LLMs,
including data preparation, model modiﬁcation, and distributed training. We also discuss the
scaling laws for LLMs, which help us understand their training efﬁciency and effectiveness.
2.2.1
Data Preparation
The importance of data cannot be overstated in NLP. As larger neural networks are developed,
the demand for data continues to increase. For example, developing LLMs may require trillions
of tokens in pre-training (see Table 2.3), orders of magnitude larger than those used in training
conventional NLP models. In general, we may want to gather as much training data as possible.
However, larger training datasets do not mean better training results, and the development of
LLMs raises new issues in creating or collecting these datasets.
A ﬁrst issue is the quality of data. High-quality data has long been seen as crucial for training
data-driven NLP systems. Directly using raw text from various sources is in general undesirable.
For example, a signiﬁcant portion of the data used to train recent LLMs comes from web scraping,
which may contain errors and inappropriate content, such as toxic information and fabricated
facts. Also, the internet is ﬂooded with machine-generated content due to the widespread use of
AI, presenting further challenges for processing and using web-scraped data. Researchers have
found that training LLMs on unﬁltered data is harmful [Raffel et al., 2020]. Improving data quality
typically involves incorporating ﬁltering and cleaning steps in the data processing workﬂow. For
example, Penedo et al. [2023] show that by adopting a number of data processing techniques, 90%2.2 Training at Scale
57
of their web-scraped data can be removed for LLM training. In addition to large-scale web-scraped
data, LLM training data often includes books, papers, user-generated data on social media, and
so on. Most of the latest LLMs are trained on such combined datasets, which are found to be
important for the strong performance of the resulting models.
A second issue is the diversity of data. We want the training data to cover as many types of
data as possible, so that the trained models can adapt to different downstream tasks easily. It has
been widely recognized that the quality and diversity of training data both play very important
roles in LLMs. An interesting example is that incorporating programming code into training data
has been found to be beneﬁcial for LLMs. The beneﬁts are demonstrated not only in enhancing the
programming abilities of LLMs, but also in improving reasoning for complex problems, especially
those requiring COT prompting. The concept “diversity” can be extended to include language
diversity as well. For example, many LLMs are trained on multi-lingual data, and therefore we
can handle multiple languages using a single model. While this approach shows strong abilities
in multi-lingual and cross-lingual tasks, its performance on speciﬁc languages largely depends on
the volume and quality of the data for those languages. It has been shown in some cases to provide
poor results for low-resource languages.
A third issue is the bias in training data. This is not a problem that is speciﬁc to LLMs but
exists in many NLP systems. A common example is gender bias, where LLMs show a preference
for one gender over another. This can partly be attributed to class imbalance in the training data,
for example, the term nurses is more often associated with women. In order to debias the data,
it is common practice to balance the categories of different language phenomena, such as gender,
ethnicity, and dialects. The bias in data is also related to the diversity issue mentioned above.
For example, since many LLMs are trained and aligned with English-centric data, they are bi-
ased towards the cultural values and perspectives prevalent among English-speaking populations.
Increasing language diversity in training data can somewhat mitigate the bias.
Another issue with collecting large-scale data is the privacy concern. If LLMs are trained
on data from extensive sources, this potentially leads to risks regarding the exposure of sensitive
information, such as intellectual property and personal data. This is particularly concerning given
the capacity of LLMs to represent patterns from the data they are trained on, which might in-
advertently involve memorizing and reproducing speciﬁc details. A simple approach to privacy
protection is to remove or anonymize sensitive information. For example, anonymization tech-
niques can be applied to remove personally identiﬁable information from training data to prevent
LLMs from learning from such data. However, in practice, erasing or redacting all sensitive data
is difﬁcult. Therefore, many LLMs, particularly those launched for public service, typically work
with systems that can detect the potential exposure of sensitive data, or are ﬁne-tuned to reject
certain requests that could lead to information leakage.
2.2.2
Model Modiﬁcations
Training LLMs is difﬁcult. A commonly encountered problem is that the training process be-
comes more unstable as LLMs get bigger. For example, one needs to choose a small learning rate
to achieve stable training with gradient descent, but this in turn results in much longer training
times. Sometimes, even when the training conﬁguration is carefully designed, training may di-
verge at certain points during optimization. The training of LLMs is generally inﬂuenced by many
factors, such as parameter initialization, batching, and regularization. Here, we focus on common58
Generative Models
modiﬁcations and improvements to the standard Transformer architecture, which are considered
important in developing trainable LLMs.
2.2.2.1
Layer Normalization with Residual Connections
Layer normalization is used to stabilize training for deep neural networks. It is a process of
subtracting the mean and dividing by the standard deviation. By normalizing layer output in
this way, we can effectively reduce the covariate shift problem and improve the training stability.
In Transformers, layer normalization is typically used together with residual connections. As
described in Section 2.1.1, a sub-layer can be based on either the post-norm architecture, in which
layer normalization is performed right after a residual block, or the pre-norm architecture, in
which layer normalization is performed inside a residual block. While both of these architectures
are widely used in Transformer-based systems [Wang et al., 2019], the pre-norm architecture has
proven to be especially useful in training deep Transformers. Given this, most LLMs are based on
the pre-norm architecture, expressed as output = LNorm(F (input)) + input.
A widely-used form of the layer normalization function is given by
LNorm(h) = α ·