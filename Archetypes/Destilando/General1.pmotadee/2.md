
h−µ
+β
σ+ϵ
(2.23)
where h is a d-dimensional real-valued vector, µ is the mean of all the entries of h, and σ is the
corresponding standard deviation. ϵ is introduced for the sake of numerical stability. α ∈ Rd and
β ∈ Rd are the gain and bias terms.
A variant of layer normalization, called root mean square (RMS) layer normalization, only
re-scales the input vector but does not re-center it [Zhang and Sennrich, 2019]. The RMS layer
normalization function is given by
LNorm(h) = α ·
h
σrms + ϵ
where σrms is the root mean square of h, that is, σrms = ( d1
function is used in LLMs like the LLaMA series.
2.2.2.2
+β
(2.24)
d
2 1
k=1 hk ) 2 . This layer normalization
Activation Functions in FFNs
In Transformers, FFN sub-layers are designed to introduce non-linearities into representation
learning, and are found to be useful for preventing the representations learned by self-attention
from degeneration8 [Dong et al., 2021]. A standard form of the FFNs used in these sub-layers can
be expressed as
FFN(h) = σ(hWh + bh )Wf + bf
(2.25)
where Wh ∈ Rd×dh , bh ∈ Rdh , Wf ∈ Rdh ×d , and bf ∈ Rd are the parameters, and dh is the
hidden size. σ(·) is the activation function of the hidden layer. A common choice for σ(·) is the
8
Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.2.2 Training at Scale
59
rectiﬁed linear unit (ReLU), given by
σrelu (h) = max(0, h)
(2.26)
In practical implementations, increasing dh is helpful and thus it is often set to a larger number
in LLMs. But a very large hidden size poses challenges for both training and deployment. In this
case, the design of the activation function plays a relatively more important role in wide FFNs.
There are several alternatives to the ReLU in LLMs. One of these is the gaussian error linear
unit (GeLU) which can be seen as a smoothed version of the ReLU. Rather than controlling the
output by the sign of the input, the GeLU function weights its input by the percentile Pr(h ≤ h).
Here h is a d-dimensional vector whose entries are drawn from the standard normal distribution
Gaussian(0, 1)9 . Speciﬁcally, the GeLU function is deﬁned to be
σgelu (h) = h Pr(h ≤ h)
= hΦ(h)
(2.27)
where Φ(h) is the cumulative distribution function of Gaussian(0, 1), which can be implemented
in convenient ways [Hendrycks and Gimpel, 2016]. The GeLU function has been adopted in
several LLMs, such as BERT, GPT-3, and BLOOM.
Another family of activation functions which is popular in LLMs is gated linear unit (GLU)-
based functions. The basic form of GLUs is given by
σglu (h) = σ(hW1 + b1 ) ⊙ (W2 + b2 )
(2.28)
where W1 ∈ Rd×d , b1 ∈ Rd , W2 ∈ Rd×d , and b2 ∈ Rd are model parameters. Different choices
of σ(·) result in different versions of GLU functions. For example, if σ(·) is deﬁned to be the
GeLU function, we will have the GeGLU function
σgeglu (h) = σgelu (hW1 + b1 ) ⊙ (W2 + b2 )
(2.29)
This activation function has been successfully applied in LLMs like Gemma.
As another example, consider σ(·) to be the Swish function σswish (h) = h ⊙ Sigmoid(ch)
[Ramachandran et al., 2017]. Then, the SwiGLU function is given by
σswiglu (h) = σswish (hW1 + b1 ) ⊙ (W2 + b2 )
(2.30)
Both the PaLM and LLaMA series are based on the SwiGLU function. For more discussions of
GLUs, the reader can refer to Shazeer [2020]’s work.
9
Pr(h ≤ h) is an informal notation. It refers to a vector, with each entry representing the percentile for the
corresponding entry of h.60
Generative Models
2.2.2.3
Removing Bias Terms
Another popular model design is to remove the bias terms in afﬁne transformations used in LLMs.
This treatment can be applied to layer normalization, transformations of the inputs to QKV atten-
tion, and FFNs. For example, we can modify Eq. (2.25) to obtain an FFN with no bias terms
FFN(h) = σ(hWh )Wf
(2.31)
Chowdhery et al. [2022] report that removing bias terms helps improve the training stability
of LLMs. This method has been used in several recent LLMs, such as LLaMA and Gemma.
2.2.2.4
Other Issues
Many LLMs also involve modiﬁcations to their positional embedding models. For example, one
can replace sinusoidal positional encodings with rotary position embeddings so that the learned
LLMs can handle long sequences better. These models will be discussed in Section 2.3.
Note that while model modiﬁcations are common in training LLMs, the stability of training
can be improved in many different ways. For example, increasing the batch size as the training
proceeds has been found to be useful for some LLMs. In general, achieving stable and efﬁcient
large-scale LLM training requires carefully designed setups, including learning schedules, opti-
mizer choices, training parallelism, mixed precision training, and so on. Some of these issues are
highly engineered, and therefore, we typically need a number of training runs to obtain satisfactory
LLMs.
2.2.3
Distributed Training
Training LLMs requires signiﬁcant amounts of computational resources. A common approach to
improving training efﬁciency is to use large-scale distributed systems. Fortunately, alongside the
rise of neural networks in AI, deep learning-oriented software and hardware have been developed,
making it easier to implement LLMs and perform computations. For example, one can now easily
ﬁne-tune an LLM using deep learning software frameworks and a machine with multiple GPUs.
However, scaling up the training of LLMs is still challenging, and requires signiﬁcant efforts in
developing hardware and software systems for stable and efﬁcient distributed training.
An important consideration of distributed training is parallelism. There are several forms of
parallelism: data parallelism, model parallelism, tensor parallelism, and pipeline parallelism. De-
spite different ways to distribute computations across devices, these parallelism methods are based
on a similar idea: the training problem can be divided into smaller tasks that can be executed si-
multaneously. The issue of parallelism in training LLMs has been extensively studied [Narayanan
et al., 2021; Fedus et al., 2022]. Here we sketch the basic concepts.
• Data Parallelism. This method is one of the most widely used parallelism methods for
training neural networks. To illustrate, consider the simplest case where the standard delta
rule is used in gradient descent
θt+1 = θt − lr ·
∂Lθt (Dmini )
∂θt
(2.32)2.2 Training at Scale
61
where the new parameters θt+1 is obtained by updating the latest parameters θt with a small
∂Lθt (Dmini )
is the gradient of the loss
step lr in the direction of the negative loss gradient.
∂θt
with respect to the parameters θt , and is computed on a minibatch of training sample Dmini .
In data parallelism, we divide Dmini into N smaller batches, denoted by {D 1 , ▷▷▷, D N }.
Then, we distribute these batches to N workers, each with a corresponding batch. Once
the data is distributed, these workers can work at the same time. The gradient of the entire
minibatch is obtained by aggregating the gradients computed by the workers, like this
∂Lθt (Dmini )
∂θt
=
∂Lθt (D 1 ) ∂Lθt (D 2 )
∂Lθt (D N )
+
+··· +
∂θt
∂θt
∂θt


worker 1



worker 2



worker N
(2.33)

In ideal cases where the workers coordinate well and the communication overhead is small,
data parallelism can achieve nearly an N -fold speed-up for training.
• Model Parallelism. Although data parallelism is simple and effective, it requires each
worker to run the entire LLM and perform the complete forward and backward process.
As LLMs grow larger, it sometimes becomes unfeasible to load and execute an LLM on a
single device. In this case, we can decouple the LLM into smaller components and run these
components on different devices. One simple way to do this is to group consecutive layers
in the layer stack and assign each group to a worker. The workers operate in the order of
the layers in the stack, that is, in the forward pass we process the input from lower-level to
upper-level layers, and in the backward pass we propagate the error gradients from upper-
level to lower-level layers. Consider, for example, a Transformer decoder with L stacked
blocks. To distribute the computation load, each block is assigned to a worker. See the
following illustration for a single run of the forward and backward passes of this model.
BL (↑) BL (↓)
Worker L
...
...
Worker 2
...
B2 (↑)
B2 (↓)
Worker 1 B1 (↑)
B1 (↓)
Here Bl denotes the computation of block l, and the symbols ↑ and ↓ denote the forward and
backward passes, respectively. Note that this parallelism method forces the workers to run
in sequence, so a worker has to wait for the previous worker to ﬁnish their job. This results
in the devices being idle for most of the time. In practical systems, model parallelism is
generally used together with other parallelism mechanisms to maximize the use of devices.
• Tensor Parallelism. Parallelism can also be performed in a single computation step. A
common example is splitting a large parameter matrix into chunks, multiplying an input
tensor with each of these chunks separately, and then concatenating the results of these
multiplications to form the output. For example, consider the multiplication of the repre-
sentation h ∈ Rd with the parameter matrix Wh ∈ Rd×dh in an FFN sub-layer (see Eq.
(2.25)). We can slice the matrix Wh ∈ Rd×dh vertically to a sequence of M sub-matrices
Wh =

Wh1 Wh2 ▷▷▷ WhM

(2.34)62
Generative Models
where each sub-matrix Whk has a shape of d × dMh . The multiplication of h with Wh can be
expressed as

hWh = h Wh1 Wh2 ▷▷▷ WhM
=


hWh1 hWh2 ▷▷▷ hWhM

(2.35)
We can perform matrix multiplications {hWh1 , hWh2 , ▷▷▷, hWhM } on M devices separately.
As a result, we distribute a large matrix multiplication across multiple devices, each of
which may have relatively small memory. From the perspective of the design of modern
GPUs, tensor parallelism over GPUs provides a two-level, tile-based approach to parallel
computing. First, at a higher level, we decompose a matrix multiplication into sub-matrix
multiplications that can directly ﬁt into the memory of GPUs. Then, at a lower level, we
execute these sub-matrix multiplications on GPUs using tile-based parallel algorithms that
are speciﬁcally optimized for GPUs.
• Pipeline Parallelism. Above, in model parallelism, we have described a simple approach
to spreading groups of model components across multiple devices. But this method is in-
efﬁcient because only one device is activated at a time during processing. Pipeline par-
allelism addresses this issue by introducing overlaps between computations on different
devices [Harlap et al., 2018; Huang et al., 2019]. To do this, a batch of samples is divided
into a number of micro-batches, and then these micro-batches are processed by each worker
as usual. Once a micro-batch is processed by a worker and passed to the next one, the
following micro-batch immediately occupies the same worker. In other words, we create
a pipeline in which different computation steps can overlap if multiple jobs are given to
the pipeline. The following shows an illustration of pipeline parallelism for processing 3
micro-batches.
Worker L
BL,1
...
...
Worker 2B2,1B2,2
Worker 1 B1,1B1,2B1,3
B2,3
BL,2
BL,3
BL,1
BL,2
BL,3
...
B2,1
B2,2B2,3
B1,1B1,2
B1,3
Here Bl,k represents the processing of the k-th micro-batch by the l-th worker. Ideally we
would like to maximize the number of micro-batches, and thus minimize the idle time of the
workers. However, in practice, using small micro-batches often reduces GPU utilization and
increases task-switching costs. This may, in turn, decrease the overall system throughput.
The ultimate goal of parallel processing is to achieve linear growth in efﬁciency, that is, the
number of samples that can be processed per unit of time increases linearly with the number of
devices. However, distributed training is complicated, and inﬂuenced by many factors in addition
to the parallelism method we choose. One problem, which is often associated with distributed
systems, is the cost of communication. We can think of a distributed system as a group of net-
worked nodes. Each of these nodes can perform local computation or pass data to other nodes. If2.2 Training at Scale
63
there are a large number of such nodes, it will be expensive to distribute and collect data across
them. Sometimes, the time savings brought about by parallelism are offset by the communica-
tion overhead of a large network. Another problem with large-scale distributed systems is that
the synchronization of nodes introduces additional costs. As is often the case, some nodes may
take longer to work, causing others to wait for the slowest ones. While we can use asynchronous
training to handle heterogeneity in computational resources, this may lead to stale gradients and
non-guaranteed convergence. Moreover, as more nodes are added to the network, there is more
chance to have crashed nodes during training. In this case, we need to ensure that the whole
system is fault tolerant. In many practical settings, to increase scalability, one needs to take into
account additional issues, including architecture design, data transfer and computation overlap,
load balancing, memory bandwidth and so on.
Training LLMs is so computationally expensive that, even though distributed training is al-
ready in use, researchers and engineers often still employ various model compression and speed-
up methods to improve training efﬁciency [Weng, 2021]. One example is mixed precision training,
in which low precision data (such as FP16 and FP8 data) is used for gradient computation on each
individual node, and single or double precision data (such as FP32/FP64 data) is used for updating
the model [Micikevicius et al., 2018]. A key operation in this approach is gradient accumulation
where gradients need to be accumulated and synchronized across nodes. However, due to the
non-associativity of ﬂoating-point addition, this can lead to slight numerical differences in accu-
mulated gradients on different nodes, which may affect model convergence and ﬁnal performance.
This problem is more obvious if there are a large number of nodes involved in distributed training,
especially given that low-precision numerical computations may encounter overﬂow and under-
ﬂow issues, as well as inconsistencies across different hardware devices. Therefore, the design of
distributed systems needs to consider these numerical computation issues to ensure satisfactory
results and convergence.
2.2.4
Scaling Laws
The success of LLMs reveals that training larger language models using more resources can lead
to improved model performance. Researchers have explained this as scaling laws of LLMs. More
speciﬁcally, scaling laws describe the relationships between the performance of LLMs and the
attributes of LLM training, such as the model size, the amount of computation used for training,
and the amount of training data. For example, Hestness et al. [2017] show that the performance of
deep neural networks is a power-law-like function of the training data size. In the beginning, when
the amount of training data is not large, the performance of the model improves slowly. Afterward,
when more training data is used, the model enters a phase of rapid performance improvement, and
the performance curve resembles a power-law curve. Ultimately, the improvement in performance
becomes slow again, and more data does not lead to signiﬁcant gains. Figure 2.3 shows an example
of such curves.
In NLP, a traditional view holds that the performance gains will disappear at a certain point
as the training is scaled up. However, recent results show that, if we consider the problem on
a larger scale, scaling up training is still a very effective method for obtaining stronger LLMs.
For example, both closed-source and open-source LLMs can beneﬁt from more data, even though
trillions of tokens have already been used for training.
With the increase in the scale of model training, LLMs exhibit new capabilities, known as theGenerative Models
Number of Test Errors (Log-scale)
64
Slow Reduction
Phase
Power-law Reduction
Phase
Convergence
Phase
(Irreducible Error)
Training Dataset Size (Log-scale)
Fig. 2.3: A scaling law of test error against a variable of interest (e.g., training dataset size) [Hestness et al., 2017]. The
curve of the scaling law can be divided into three phases. At the beginning, the number of test errors decreases slowly
when more training data is used, but this only lasts for a short period. In the second phase, the number of test errors
decreases drastically, and the curve becomes a power law curve. After that, the error reduction slows down again in the
third phase. Note that there are irreducible errors that cannot be eliminated, regardless of the amount of training data.
emergent abilities of LLMs. For example, Wei et al. [2022b] studied the scaling properties of
LLMs across different model sizes and amounts of computational resources. Their work shows
that some abilities emerge when we scale the model size to certain level. The appearance of
emergent abilities has demonstrated the role of scaled training in enhancing the performance of
LLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger
models. As larger and stronger LMs continue to appear, our understanding of the scaling laws
continues to mature. This helps researchers predict the performance of LLMs during training and
estimate the minimal computational resources required to achieve a given level of performance.
To understand how model performance scales with various factors considered during training,
it is common to express the model performance as a function of these factors. For example, in
the simplest case, we can express the loss or error of an LLM as a function of a single variable of
interest. However, there are no universal scaling laws that can describe this relationship. Instead,
different functions are proposed to ﬁt the learning curves of LLMs.
Let x be the variable of interest (such as the number of model parameters) and L(x) be the
loss of the model given x (such as the cross-entropy loss on test data). The simplest form of L(x)
is a power law
L(x) = axb
(2.36)
where a and b are parameters that are estimated empirically. Despite its simplicity, this function
has successfully interpreted the scaling ability of language models and machine translation sys-
tems in terms of model size (denoted by N ) and training dataset size (denoted by D) [Gordon
et al., 2021; Hestness et al., 2017]. For example, Kaplan et al. [2020] found that the performance
of their language model improves as a power law of either N or D after an initial transient period,
 N
−0▷076
 D −0▷095
and expressed these relationships using L(N ) = 8▷8×10
and L(D) = 5▷4×10
13
13
(see Figure 2.4).2.2 Training at Scale
65
4▷2
N
−0▷076
L(N ) = ( 8▷8·10
13 )
5▷6
3▷9
Test Loss
4▷8
Test Loss
D
−0▷095
L(D) = ( 5▷4·10
13 )
4▷0
3▷2
3▷6
3▷3
3
2▷4
2▷7
10
5
10
7
10
9
108
Number of Parameters
109
Dataset Size
Fig. 2.4: Test loss against model size (N ) and training dataset size (D) (data points are plotted for illustrative purposes).
 N −0▷076
We plot test loss as a function of N , which is deﬁned as L(N ) = 8▷8×10
, and a function of D, which is
13
deﬁned as L(D) =

D
5▷4×1013
−0▷095
[Kaplan et al., 2020].
An improvement to this scaling law is to add an irreducible error term to the power law. The
form of L(x) is then given by
L(x) = axb + ϵ∞
(2.37)
where ϵ∞ is the irreducible error that accounts for the error due to unknown variables, which is
present even as x → ∞. Eq. (2.37) is one of the most widely used forms for designing scaling
laws of LLMs. For example, Rosenfeld et al. [2020] developed a scaling law that involves both
model scaling and dataset scaling, like this
L(N, D) = aN b + cD d + ϵ∞
(2.38)
An example of such formulation is the Chinchilla scaling law. It states that the test loss per
token is the sum of the inverse proportion functions of N and D, with an additional irreducible
error term. Hoffmann et al. [2022] express this scaling law as
L(N, D) =
406▷4
0▷34
N
  
model scaling
+
410▷7
0▷28
D
  
dataset scaling
+
1▷69

(2.39)
irreducible error
All the scaling laws mentioned above are based on monotonic functions. So they cannot cover
functions with inﬂection points, such as double descent curves. In response, researchers have
explored more sophisticated functions to ﬁt the learning curves. Examples of such functions can
be found in Alabdulmohsin et al. [2022] and Caballero et al. [2023]’s work.
The signiﬁcance of scaling laws lies in providing directional guidance for LLM research: if
we are still in the region of the power law curve, using more resources to train larger models is a
very promising direction. While this result “forces” big research groups and companies to invest
more in computational resources to train larger models, which is very expensive, scaling laws
continuously push the boundaries of AI further away. On the other hand, understanding scaling
laws helps researchers make decisions in training LLMs. For example, given the computational66
Generative Models
resources at hand, the performance of LLMs may be predicted.
One last note on scaling laws in this section. For LLMs, a lower test loss does not always
imply better performance on all downstream tasks. To adapt LLMs, there are several steps such
as ﬁne-tuning and prompting that may inﬂuence the ﬁnal result. Therefore, the scaling laws for
different downstream tasks might be different in practice.
2.3
Long Sequence Modeling
We have already seen that, in large-scale training, larger language models can be developed by us-
ing more data and computational resources. However, scaling up can also occur in other directions.
For instance, in many applications, LLMs are adapted to process signiﬁcantly long sequences. An
interesting example is that we pre-train an LLM on extensive texts of normal length and then ap-
ply it to deal with very long token sequences, far beyond the length encountered in pre-training.
Here we use Pr(y|x) to denote the text generation probability where x is the context and y is the
generated text. There are broadly three types of long sequence modeling problems.
• Text generation based on long context (i.e., x is a long sequence). For example, we
generate a short summary for a very long text.
• Long text generation (i.e., y is a long sequence). For example, we generate a long story
based on a few keywords.
• Long text generation based on long context (i.e., both x and y are long sequences). For
example, we translate a long document from Chinese to English.
Recently, NLP researchers have been more interested in applying and evaluating LLMs on
tasks where extremely long input texts are involved. Imagine an LLM, which reads a C++ source
ﬁle containing tens of thousands of lines, and outlines the functionality of the program correspond-
ing to the source ﬁle. Such models, capable of handling extensive textual contexts, are sometimes
called long-context LLMs. In this section we will restrict ourselves to long-context LLMs, but
the methods discussed here can be applicable to other problems.
For Transformers, dealing with long sequences is computationally expensive, as the computa-
tional cost of self-attention grows quadratically with the sequence length. This makes it infeasible
to train and deploy such models for very long inputs. Two strands of research have tried to adapt
Transformers to long-context language modeling.
• The ﬁrst explores efﬁcient training methods and model architectures to learn self-attention
models from long-sequence data.
• The other adapts pre-trained LLMs to handle long sequences with modest or no ﬁne-tuning
efforts.
Here, we will discuss the former brieﬂy since it can be found in general discussions of efﬁcient
Transformer architectures [Tay et al., 2020; Xiao and Zhu, 2023]. We will focus on the latter,2.3 Long Sequence Modeling
67
highlighting popular methods in recent LLMs. We will also discuss the strengths and limitations
of these long-sequence models.
2.3.1
Optimization from HPC Perspectives
We begin our discussion by considering improvements to standard Transformer models from the
perspectives of high-performance computing. Most of these improvements, though not speciﬁ-
cally designed for LLMs, have been widely applied across various deep learning models [Kim
et al., 2023]. A commonly used approach is to adopt a low-precision implementation of Trans-
formers. For example, we can use 8-bit or 16-bit ﬁxed-point data types for arithmetic operations,
instead of 32-bit or 64-bit ﬂoating-point data types. Using these low-precision data types can
increase the efﬁciency and memory throughput, so that longer sequences can be processed more
easily. An alternative approach is to improve Transformers by using hardware-aware techniques.
For example, on modern GPUs, the efﬁciency of Transformers can be improved by using IO-aware
implementations of the self-attention function [Dao et al., 2022; Kwon et al., 2023].
Another way to handle long sequences is through sequence parallelism [Li et al., 2023b; Ko-
rthikanti et al., 2023]. Speciﬁcally, consider the general problem of attending the query qi at the
position i to the keys K and values V. We can divide K by rows and obtain a set of sub-matrices
{K[1] , ▷▷▷, K[nu ] }, each corresponding to a segment of the sequence. Similarly, we can obtain the
sub-matrices of V, denoted by {V[1] , ▷▷▷, V[nu ] }. Then, we assign each pair of K[u] and V[u] to a
computing node (e.g., a GPU of a GPU cluster). The assigned nodes can run in parallel, thereby
parallelizing the attention operation.
Recall that the output of the self-attention model can be written as
Attqkv (qi , K, V) =
m−1

αi,j vj
(2.40)
j=0
where αi,j is the attention weight between positions i and j. In Transformers, αi,j is obtained
by normalizing the rescaled version of the dot product between qi and kj . Let βi,j denote the
attention score between qi and kj . We have
βi,j
=
qi · kj
√
+ Mask(i, j)
d
(2.41)
where Mask(i, j) is the masking variable for (i, j). Then, we deﬁne the attention weight αi,j to
be
αi,j
= Softmax(βi,j )
exp(βi,j )
= 
j ′ exp(βi,j ′ )
(2.42)
On each computing node, we need to implement these equations. Given the keys and values
assigned to this node, computing the numerator of the right-hand side of Eq. (2.42) (i.e., exp(βi,j ))
is straightforward, as all the required information is stored on the node. However, computing the
denominator of the right-hand side of Eq. (2.42) involves a sum of exp(βi,j ′ ) over all j ′ s, which68
Generative Models
requires transferring data to and from other nodes. To illustrate, suppose that vj and kj are placed
on node u. We can rewrite Eq. (2.42) as
αi,j
=



exp(βi,j )
kj ′ ∈K[1]

node u

exp(βi,j ′ ) + · · · +

node 1


kj ′ ∈K[u]

exp(βi,j ′ ) + · · · +


node u

kj ′ ∈K[nu ]

exp(βi,j ′ )

(2.43)

node nu
where the notation kj ′ ∈ K[u] represents that kj ′ is a row vector of K[u] . In a straightforward

implementation, we ﬁrst perform the summations { kj ′ ∈K[u] exp(βi,j ′ )} separately on the corre-
sponding nodes. Then, we collect these summation results from different nodes to combine them
into a ﬁnal result. This corresponds to a collective operation in the context of parallel processing.
There are many efﬁcient implementations of such operations, such as the all-reduce algorithms.
Hence the sum of all exp(βi,j ) values can be computed using optimized routines in collective
communication toolkits.
Given the attention weights {αi,j }, we then compute the attention results using Eq. (2.40).
The problem can be re-expressed as
Attqkv (qi , K, V)
=

vj ′ ∈V[1]

αi,j ′ vj ′ + · · · +

node 1


vj ′ ∈V[u]

αi,j ′ vj ′ + · · · +

node u


vj ′ ∈V[nu ]

αi,j ′ vj ′

node nu
(2.44)

Like Eq. (2.43), Eq. (2.44) can be implemented as a summation program in parallel process-
ing. First, perform the weighted summations of values on different nodes simultaneously. Then,
we collect the results from these nodes via collective operations.
Note that, although this section primarily focuses on long sequence modeling, much of the mo-
tivation for sequence parallelism comes from the distributed training methods of deep networks,
as discussed in Section 2.2.3. As a result, the implementation of these methods can be based on
the same parallel processing library.
2.3.2
Efﬁcient Architectures
One difﬁculty of applying Transformers to long sequences is that self-attention has a quadratic
time complexity with respect to the sequence length. Moreover, a key-value cache (or KV cache
for short) is maintained during inference, and its size increases as more tokens are processed. Al-
though the KV cache grows linearly with the sequence length, for extremely long input sequences,
the memory footprint becomes signiﬁcant and it is even infeasible to deploy LLMs for such tasks.
As a result, the model architecture of long-context LLMs generally moves away from the standard
Transformer, turning instead to the development of more efﬁcient variants and alternatives.
One approach is to use sparse attention instead of standard self-attention. This family of
models is based on the idea that only a small number of tokens are considered important when
attending to a given token, and so most of the attention weights between tokens are close to zero.2.3 Long Sequence Modeling
69
As a consequence, we can prune most of the attention weights and represent the attention model
in a compressed form. To illustrate, consider the self-attention model
Attqkv (Q, K, V) = α(Q, K)V
(2.45)
where the attention weight matrix α(Q, K) ∈ Rm×m is obtained by
QKT
α(Q, K) = Softmax( √ + Mask)
d

α0,0
0
0
 α
α1,1
0
 1,0


α
α
α
2,0
2,1
2,2
= 
..
..
 ..
 .
.
.
αm−1,0 αm−1,1 αm−1,2

▷▷▷
▷▷▷
▷▷▷
..
.0
0
0
..
.
▷▷▷αm−1,m−1








(2.46)

Each row vector αi,0 ▷▷▷ αi,i 0 ▷▷▷ 0 corresponds to a distribution of attending the i-th
token to every token of the sequence. Since language models predict next tokens only based on
their left-context, we normally write the output of the attention model at position i as
Attqkv (qi , K≤i , V≤i ) =
=


αi,0
i


v
  0
.. 
▷▷▷ αi,i 
 . 
vi
αi,j vj
(2.47)
j=0




k0
v0
 . 
 . 
 . 
.
where K≤i = 
 .  and V≤i =  .  are the keys and values up to position i.
ki
vi


In the original version of self-attention αi,0 ▷▷▷ αi,i is assumed to be dense, that is, most of


the values are non-zero. In sparse attention, some of the entries of αi,0 ▷▷▷ αi,i are considered
non-zero, and the remaining entries are simply ignored in computation. Suppose G ⊆ {0, ▷▷▷, i} is
the set of indices of the non-zero entries. For language models, the output of the sparse attention
model at position i is given by
Attsparse (qi , K≤i , V≤i ) =

′
αi,j
vj
(2.48)
j∈G
′ } are normalized over G. Hence their values are different from the original attention
Here {αi,j
′ > α ). The sparsity of the model is determined by how large G is.
weights (in fact we have αi,j
i,j
Sparse attention models differ in the way we deﬁne G. One simple approach is to deﬁne G based
on heuristically designed patterns. For example, a widely-used pattern involves having G cover a
window of tokens located near position i [Parmar et al., 2018].
While sparse attention reduces the computation through the use of sparse operations, such
models still have signiﬁcant limitations as we must keep the entire KV cache (i.e., K≤i and V≤i )70
Generative Models
during inference. If the sequence is very long, storing this cache will become highly memory-
intensive. To address this, we can consider a different form of attention models where the KV
cache is not explicitly retained. Linear attention is one such approach [Katharopoulos et al.,
2020]. It uses a kernel function ϕ(·) to project each query and key onto points qi′ = ϕ(qi )
and ki′ = ϕ(ki ), respectively. By removing the Softmax function under such transformations10 ,
the form of the resulting attention model is given by
Attqkv (qi , K≤i , V≤i ) ≈ Attlinear (qi′ , K′≤i , V≤i )
=
qi′ µi
qi′ νi
(2.49)
where µi and νi are variables that are computed in the recurrent forms
T
µi = µi−1 + k′ i vi(2.50)
νi =(2.51)
T
νi−1 + k′ i
µi and νi can be seen as representations of the history up to position i. A beneﬁt of this model is
that we need not keep all past queries and values. Instead only the latest representations µi and
νi are used. So the computational cost of each step is a constant, and the model can be easily
extended to deal with long sequences.
In fact, this sequential approach to long sequence modeling arises naturally when we adopt a
viewpoint of recurrent models. Such models read one token (or a small number of tokens) at a
time, update the recurrent state using these inputs, and then discard them before the next token
arrives. The output at each step is generated based only on the recurrent state, rather than on all the
previous states. The memory footprint is determined by the recurrent state which has a ﬁxed size.
Recurrent models can be used in real-time learning scenarios where data arrives in a stream and
predictions can be made at any time step. In NLP, applying recurrent models to language mod-
eling is one of the earliest successful attempts to learn representations of sequences. Although
Transformer has been used as the foundational architecture in LLMs, recurrent models are still
powerful models, especially for developing efﬁcient LLMs. More recently, recurrent models have
started their resurgence in language modeling and have been reconsidered as a promising alterna-
tive to Transformers [Gu and Dao, 2023]. Figure 2.5 shows a comparison of the models discussed
in this subsection.
2.3.3
Cache and Memory
LLMs based on the standard Transformer architecture are global models. The inference for these
models involves storing the entire left-context in order to make predictions for future tokens. This
requires a KV cache where the representations (i.e., keys and values) of all previously-generated
10
In the new space after this transformation, the Softmax normalization can be transformed into the simple scaling
normalization.2.3 Long Sequence Modeling
71
Attqkv (qi , K≤i , V≤i )
k0k1···ki−2ki−1ki
v0v1···vi−2vi−1vi
qi
(a) Standard Self-attention
Attqkv (qi , {k1 , ki }, {v1 , vi })
k0k1···ki−2ki−1ki
v0v1···vi−2vi−1vi
qi
(b) Sparse Attention
T
µi = µi−1 + k′ i vi
T
νi = νi−1 + k′ i
⇒µi
⇒νi
q′ µ
Attlinear (qi , K≤i , V≤i ) = qi′ νii
i
k0k1···ki−2ki−1ki
v0v1···vi−2vi−1vi
qi
(c) Linear Attention
hi = f (hi−1 , inputi )
h0
h1
···
hi−3
hi−2
(d) Recurrent Models
hi−1
hi
inputi
Fig. 2.5: Illustrations of self-attention, sparse attention, linear attention and recurrent models. Blue boxes = cached
states for producing the output at position i. f (·) = a recurrent cell.
tokens are kept, and the cost of caching grows as the inference proceeds. Above, we have dis-
cussed methods for optimizing this cache via efﬁcient attention approaches, such as sparse atten-
tion and linear attention. Another idea, which may have overlap with the previous discussion, is
to explicitly encode the context via an additional memory model.72
Generative Models
2.3.3.1
Fixed-size KV Cache
A straightforward approach is to represent the keys and values using a ﬁxed-size memory model.
Suppose we have a memory Mem which retains the contextual information. We can write the
attention operation at position i in a general form
Att(qi , Mem) = Attqkv (qi , K≤i , V≤i )
(2.52)
In this model, Mem is simply the KV cache, i.e., Mem = (K≤i , V≤i ). Thus the size of
Mem is determined by i. If we deﬁne Mem as a ﬁxed-size variable, then the cost of performing
Att(qi , Mem) will be ﬁxed. There are several alternative ways to design Mem.
• One of the simplest methods is to consider a ﬁxed-size window of previous keys and values.
Mem is therefore given by
Mem = (K[i−nc +1,i] , V[i−nc +1,i] )
(2.53)
where nc denotes the size of the window. The notation K[i−nc +1,i] and V[i−nc +1,i] denote
the keys and values over positions from i − nc + 1 to i.11 This model can be seen as a type
of local attention model.
• It is also possible to deﬁne Mem as a pair of summary vectors, which leads to a more
compressed representation of the history. A simple way to summarize the previous keys
and values is to use the moving average of them. For example, Mem can be deﬁned as the
unweighted moving average of the previous nc keys and values
 i
j=i−nc +1 kj
Mem =
nc
,
i
j=i−nc +1 vj
nc

(2.54)
Alternatively, we can use a weighted version of moving average
Mem =
 i
j=i−nc +1 βj−i+nc kj
 nc
,
j=1 βj
i
j=i−nc +1 βj−i+nc vj
 nc
j=1 βj

(2.55)
Here {β1 , ▷▷▷, βnc } are the coefﬁcients, which can be either learned as model parameters
or determined via heuristics. For example, they can be set to increasing coefﬁcients (i.e.,
β1 < β2 < ▷▷▷ < βnc −1 < βnc ) in order to give larger weight to positions that are closer to
i. We can extend the moving average to include all the positions up to i. This leads to the
cumulative average of the keys and values, given in the form
Mem =

 i
j=0 kj
i+1

,
i
j=0 vj
i+1


(2.56)

vi−nc +1
ki−nc +1




.
..
11
..
More formally, we write K[i−nc +1,i] = 
 and V[i−nc +1,i] = 
. Sometimes we denote
.
ki
vi
K[i−nc +1,i] by {ki−nc +1 , ▷▷▷, ki } and V[i−nc +1,i] by {vi−nc +1 , ▷▷▷, vi } for notation simplicity.2.3 Long Sequence Modeling
73
In general, the cumulative average can be written using a recursive formula
Memi =
(ki , vi ) + i · Memi−1
i+1
(2.57)
where Memi and Memi−1 denote the cumulative averages of the current and previous po-
sitions, respectively. An advantage of this model is that we only need to store a single
key-value pair during inference, rather than storing all the key-value pairs. Note that the
above memory models are related to recurrent models, and more advanced techniques have
been used to develop alternatives to self-attention mechanisms in Transformers [Ma et al.,
2023].
• The memory Mem can also be a neural network. At each step, it takes both the previous
output of the memory and the current states of the model as input, and produces the new
output of the memory. This neural network can be formulated as the function
Mem = Update(Skv , Mempre )
(2.58)
Here Mem and Mempre represent the outputs of the memory at the current step and the
previous step, respectively. Skv is a set of key-value pairs, representing the recent states of
the model. This formulation is general and allows us to develop various memory models by
selecting different Update(·) and Skv conﬁgurations. For example, if Skv only contains the
latest key-value pair (ki , vi ) and Update(·) is deﬁned as a recurrent cell, then Eq. (2.58)
can be expressed as an RNN-like model
Mem = f ((ki , vi ), Mempre )
(2.59)
where f (·) is a recurrent cell. Recurrence can also be applied to segment-level modeling
for efﬁciency consideration. A simple approach is that we can divide the sequence into
segments, and treat Skv as a segment. Applying recurrent models to Update(·) will result in
memory models that operate on segments. A special example is that we deﬁne Update(·) as
an FIFO function that adds Skv into the memory and removes the oldest key-value segment
from the memory, given by
Mem = FIFO(Skv , Mempre )
(2.60)
Consider a memory which includes two segments, one for current segment, and one for the
previous segment. In the attention operation, each position can access the history key-value
pairs in two closest consecutive segments. This essentially deﬁnes a local memory, but it
and its variants have been widely used segment-level recurrent models [Dai et al., 2019;
Hutchins et al., 2022; Bulatov et al., 2022].
• The above memory models can be extended to involve multiple memories. An example
of this approach is compressive Transformer [Rae et al., 2019]. It employs two distinct
ﬁxed-size memories: one for modeling local context (denoted by Mem), and the other for
modeling and compressing long-term history (denoted by CMem). The KV cache in this
model is the combination of Mem and CMem. The attention function can be written as
Attcom (qi , Mem, CMem) = Attqkv (qi , [Mem, CMem])
(2.61)74
Generative Models
where [Mem, CMem] is a combined memory of Mem and CMem. As with other segment-
level models, the compressive Transformer model operates on segments of the sequence.
k as the key-value
Each segment is a sequence of ns consecutive tokens, and we denote Skv
pairs corresponding to the tokens of the k-th segment. When a new segment arrives, Mem
k to Mem, and then
is updated in an FIFO fashion: we append the nc key-value pairs in Skv
pop the ns oldest key-value pairs from Mem, which is given by
k
Mem = FIFO(Skv
, Mempre )
(2.62)
The popped key-value pairs are then used to update the compressive memory CMem. These
ns key-value pairs are compressed into ncs key-value pairs via a compression network.
CMem is an FIFO which appends the compressed ncs key-value pairs to the tail of the
queue, and drops the ﬁrst ncs key-value pairs of the queue. It is given by
k
CMem = FIFO(Ckv
, CMempre )
(2.63)
k represents the set of compressed key-value pairs. Implicit in the compressive
where Ckv
Transformer model is that local context should be represented explicitly with minimal in-
formation loss, while long-range context can be more compressed.
• We have already seen that both global and local contexts are useful and can be modeled
using attention models. This view motivates the extension to attention models for combining
both local and long-term memories [Ainslie et al., 2020; Zaheer et al., 2020; Gupta and
Berant, 2020]. A simple but widely-used approach is to involve the ﬁrst few tokens of
the sequence in attention, serving as global tokens. This approach is usually applied along
with other sparse attention models. An advantage of incorporating global tokens of the
sequence is that it helps smooth the output distribution of the Softmax function used in
attention weight computation, and thus stabilizes model performance when the context size
is very large [Xiao et al., 2024]. One drawback, however, is that using a ﬁxed-size global
memory may result in information loss. When dealing with long sequences, we need to
enlarge the KV cache for sufﬁcient representations of the context, but this in turn increases
the computational cost.
Figure 2.6 shows illustrations of the above approaches. Note that, while we focus on optimiza-
tion of the KV cache here, this issue is closely related to those discussed in the previous section.
All of the methods we have mentioned so far can broadly be categorized as efﬁcient attention
approaches, which are widely used in various Transformer variants.
2.3.3.2
Memory-based Models
The modeling of memories discussed above was based on updates to the KV cache, and the re-
sulting models are typically referred to as internal memories. We now consider another family
of models, called external memories, which operate as independent models to access large-scale
contexts for LLMs. Many such models are based on memory-based methods which have been
extensively discussed in machine learning [Bishop, 2006]. A common example is nearest neigh-
bor algorithms: we store context representations in a datastore, and try to ﬁnd the most similar2.3 Long Sequence Modeling
75
Memory
Size = 4 × 2
···Keys
···Values
i−7
i−6
i−5
i−4
i−3
i−2
i−1
i
(a) Window-based Cache
ki−3 +ki−2 +ki−1 +ki
4
vi−3 +vi−2 +vi−1 +vi
4
Memory
Size = 1 × 2
⇒
⇒
···Keys
···Values
i−7
i−6
i−5
i−4
i−3
i−2i−1
Mem = Update( Skv , Mempre )⇒
i
(b) Moving Average-based Cache
Memory
Size = 1 × 2
···Keys
···Values
i−7
i−6
i−5
i−4
i−3
i−2
i−1
i
(c) Recurrent Network as Cache
Compressed
Memory
Size = 2 × 2
Memory
Size = 4 × 2
···Keys
···Values
i−7
i−6
i−5
i−4
i−3
i−2
i−1
i
(d) Hybrid Cache (Compressed Memory + Local Memory)
Fig. 2.6: Illustrations of ﬁxed-size KV caches in LLMs. Blue boxes represent the keys and values generated during
LLM inference, green boxes represent the keys and values stored or encoded in the primary memory, and orange boxes
represent the keys and values stored or encoded in the compressed memory.
stored representations to match a given query. The retrieved context representations are then used
to improve attention for this query.76
Generative Models
Here, we consider the k-nearest neighbors (k-NN) method which is one of the most popular
memory-based methods. Since our focus is language modeling in this section, we deﬁne a sample
in the datastore as a key-value pair corresponding to some context state. Note that “context” is a
broad concept here, not just a sequence preﬁx in text generation. One might, for example, view
the entire dataset as the context for predicting tokens. This allows us to retrieve the closest context
situation in a set of sequences, rather than a given sequence preﬁx. Although we will restrict
ourselves to context modeling for a single sequence, in this subsection, we discuss a relatively
more general case.
Suppose we have a set of keys {kj } with corresponding values {vj }, and suppose we store
these key-value pairs in a vector database12 . For each query qi , we ﬁnd its k nearest neighbours by
growing the radius of the sphere centered as qi until it contains k data points in {kj }. This results
in a set of k keys along with their corresponding values, denoted by Memknn . As before, we
denote Mem as the local memory for the query, such as the KV cache of neighboring tokens. Our
goal is to attend query qi to both the local memory Mem and the long-term memory Memknn .
There are, of course, several ways to incorporate Mem and Memknn into the attention model.
For example, we might simply combine them to form a single KV cache [Mem, Memknn ], and
attend qi to [Mem, Memknn ] via standard QKV attention. Or we might use Mem and Memknn
in separate attention steps. An example of such approaches is the model developed by Wu et al.
[2021]. It linearly combines the two types of attention, given by
Att(qi , Mem, Memknn ) = g ⊙ Attlocal + (1 − g) ⊙ Attknn(2.64)
Attlocal = Att(qi , Mem)(2.65)
Attknn = Att(qi , Memknn )(2.66)
Here g ∈ Rd is the coefﬁcient vector, which can be the output of a learned gate.
Given the k-NN-based memory model described above, the remaining task is to determine
which key-value pairs are retained in the datastore. For standard language modeling tasks, we
consider the previously seen tokens in a sequence as the context, so we can add the keys and
values of all these tokens into the datastore. In this case, the resulting k-NN-based attention
model is essentially equivalent to a sparse attention model [Gupta et al., 2021].
Alternatively, we can extend the context from one sequence to a collection of sequences.
For example, we might collect all key-value pairs across the sequences in a training dataset and
add them to the datastore to model a larger context. Thus, LLMs can predict tokens based on a
generalized context. A problem with this approach is that the computational cost would be large
if many sequences are involved. Since these sequences are part of our training data, we can build
and optimize an index for the vectors in the datastore before running the LLMs. As a result, the
retrieval of similar vectors can be very efﬁcient, as in most vector databases.
In fact, all the above-mentioned methods can be viewed as instances of a retrieval-based ap-
proach. Instead of using retrieval results to improve attention, we can apply this approach in other
ways as well. One application of k-NN-based search is k-NN language modeling (or k-NN LM)
[Khandelwal et al., 2020]. The idea is that, although it is attempting to extend the context used
in self-attention by incorporating nearest neighbors in representation learning, in practice, similar
hidden states in Transformers are often highly predictive of similar tokens in subsequent positions.
12
A vector database, or vector store, is a database that provides highly optimized retrieval interfaces for ﬁnding stored
vectors that closely match a query vector.2.3 Long Sequence Modeling
77
In k-NN LM, each item in the datastore is a key-value tuple (z, w), where z represents a hidden
state of the LLM at a position, and w represents the corresponding prediction. A typical way to
create the datastore is to collect the output vector of the Transformer layer stack and the corre-
sponding next token for each position of each sequence in a training dataset. During inference,
we have a representation hi given a preﬁx. Given this representation, we ﬁrst search the datastore
for k closest matching data items {(z1 , w1 ), ▷▷▷, (zk , wk )}. Here {w1 , ▷▷▷, wk } are thought of as
reference tokens for prediction, and thus can be used to guide the token prediction based on hi .
One common way to make use of reference tokens is to deﬁne a distribution over the vocabulary
V,


Prknn (·|hi ) = Softmax( −d0 · · · −d|V | )
(2.67)
where dv equals the distance between hi and zj if wj equals the v-th entry of V , and equals 0
otherwise. We use a linear function with a coefﬁcient λ that interpolates between the retrieval-
based distribution Prknn (·|hi ) and the LLM output distribution Prlm (·|hi )
Pr(·|hi ) = λ · Prknn (·|hi ) + (1 − λ) · Prlm (·|hi )
(2.68)
Then, as usual, we can choose the next token y by maximizing the probability Pr(y|hi ).
As with information retrieval (IR) systems, the datastore can also manage texts and provide
access to relevant texts for a query. For example, we can store a collection of text documents
in a search engine with full-text indexing, and then search it for documents that match a given
text-based query. Applying IR techniques to LLMs leads to a general framework called retrieval-
augmented generation (RAG). The RAG framework works as follows. We use the context x as
the query and ﬁnd the k most relevant document pieces {c1 , ▷▷▷, ck } from the datastore via efﬁcient
IR techniques13 . These search results are combined with the original context via a prompting
template g(·)14 , resulting in an augmented input for the LLM
x′ = g(c1 , ▷▷▷, ck , x)
(2.69)
Then, we use x′ as the context and predict the following text using the model Pr(y|x′ ). One
advantage of RAG is that we need not modify the architecture of LLMs, but instead augment the
input to LLMs via an additional IR system. Figure 2.7 shows a comparison of the use of different
external memories in LLMs.
13
In piratical applications, queries are typically generated using a query generation system, which may expand it
with variations of tokens and query intent.
14
For example, the template could be:
message = {*c1 *} ... {*ck *}
input: {*x*}
output:78
Generative Models
g ⊙ Att(qi , Mem) + (1 − g) ⊙ Att(qi , Memknn )
Att(qi , Memknn )
Att(qi , Mem)
qi
···
···
KV Cache
k Nearest
Neighbors
Keys/values in LLM
Datastore
Search
Keys/values in Datastore
(a) k-NN Search Augmented Attention
Output Distribution
Distribution Pr(·)
Distribution Prknn (·)
···
Att(qi , Mem)
Att(qi , Mem)
qi
···
···
KV Cache
k Nearest
Neighbors
Keys/values in LLM
Datastore
Keys in Datastore
Search
Predicted Tokens
(b) k-NN Language Modeling
LLM
c1 = Deep network is ...Message: deep network ... machine learning ...
c2 = Machine learning is ...What is deep learning?
···
k Nearest
Neighbors
Datastore
Search
Input Context:
x = What is deep learning?
(c) Retrieval-augmented Generation
Fig. 2.7: Illustrations of external memories (or datastores) for language modeling.2.3 Long Sequence Modeling
2.3.3.3
79
Memory Capacity
A memory model in LLMs, in the form of a simple key-value cache or a datastore, can broadly
be seen as an encoder of contextual information. Ideally, before we say that a memory model
is representative of the entire context in token prediction, we need to make sure that the model
can accurately represent any part of the context. The standard KV cache is one such model that
completely stores all past history. In this case, the model is said to have adequate capacity for
memorizing the context. In many practical applications, however, complete memorization is not
required. Instead, the goal is to enable LLMs to access important contextual information. As a
result, efﬁcient and compressed memory models are developed, as described in this section. Note
that, the longer the sequence, the more difﬁcult it becomes for a low-capacity memory model to
capture important contextual information. It is therefore common practice to simply increase the
model capacity when processing long contexts.
While high-capacity models are generally favorable, they are difﬁcult to train and deploy. A
challenging scenario is that the tokens arrive in a stream and the context continuously grows.
Developing LLMs for such tasks is difﬁcult as we need to train Transformers on extremely long
sequences. A possible way to address this difﬁculty is to use non-parametric methods, such as
retrieval-based methods. For example, as discussed above, we can use a vector database to store
previously generated key-value pairs, and thus represent the context by this external memory
model. Although this approach side-steps the challenge of representing long context in Trans-
formers, building and updating external memory models are computationally expensive. These
models are more often used in problems where the context is given in advance and ﬁxed during
inference, and hence unsuitable for streaming context modeling.
In cases where the size of the context continuously grows, applying ﬁxed-size memory models
is a commonly used approach. For example, in recurrent models, a sequence of arbitrary length
can be summarized into a set of hidden states by which we have a ﬁxed computational cost per step.
While recurrent models were initially found to be not very good at handling long-distance depen-
dencies in sequence modeling in early applications of deep learning to NLP, recent advancements
have shown that their variants are now effective in modeling extremely long sequences. [Bulatov
et al., 2022; Hutchins et al., 2022; Munkhdalai et al., 2024; Ma et al., 2024].
There is no general deﬁnition of memory capacity in LLMs. A simple approach might consider
how much storage is used to retain contextual information. For example, memory capacity could
be deﬁned by the size of the KV cache in Transformers or the vector database used in retrieval-
based methods. A related concept is model complexity. In machine learning, there are several
ways to deﬁne the model complexity of a model. One of the simplest methods is by counting the
number of parameters. However, it should be emphasized that the memory models discussed here
primarily serve to store information, rather than add trainable parameters. Therefore, a model with
a large memory capacity is not necessarily more complex. Nevertheless, in practice determining
the capacity of a memory model is not straightforward. In general, we need to control the trade-off
between maximizing the performance and controlling the memory footprint.
2.3.4
Sharing across Heads and Layers
In Transformers, the KV cache is a data structure that can be dynamically adjusted along multiple
dimensions, such as heads, layers, and sequence length. For example, consider an LLM with L
layers. Each layer has τ attention heads, and each head produces a dh -dimensional output. During80
Generative Models
inference, we store the keys and values for up to m tokens. The space complexity of this caching
mechanism is O(L · τ · dh · m). As we have seen previously, this complexity can be reduced by
caching the keys and values for fewer tokens. For example, in sliding window attention, a ﬁxed-
size window is used to cache the keys and values in local context. And this model has a space
complexity of O(L · τ · dh · mw ), with mw being the size of the window.
In addition to reducing m, we can also decrease the size of the KV cache along other di-
mensions. A widely-used approach is to enable sharing across heads in multi-head self-attention.
Recall from Section 2.1.1 that multi-head self-attention uses multiple sets of queries, keys, and
values (each set is called a head), each performing the QKV attention mechanism as usual. This
can be expressed as
Output = Merge(head1 , ▷▷▷, headτ )Whead
(2.70)
where headj ∈ Rdh is computed using the standard QKV attention function
[j]
[j]
[j]
headj = Attqkv (qi , K≤i , V≤i )
[j]
[j]
(2.71)
[j]
Here, qi , K≤i , and V≤i are the query, keys, and values that are projected onto the j-th feature
sub-space. So this model can be interpreted as performing attention on a group of feature sub-
spaces in parallel (see Figure 2.8 (b)). The KV cache needs to retain the keys and values for all
[1]
[1]
[τ ]
[τ ]
these heads, that is, {(K≤i , V≤i ), ▷▷▷, (K≤i , V≤i )}.
One reﬁnement to the multi-head attention model, called multi-query attention (MQA), is to
share keys and values across heads, while allowing queries to be unique for each head [Shazeer,
2019]. In MQA, there is a single set of keys and values (K≤i , V≤i ). In addition, there are τ
[1]
[τ ]
queries {qi , ▷▷▷, qi }, each corresponding to a different head. For each head, we have
[j]
headj = Attqkv (qi , K≤i , V≤i )
(2.72)
Figure 2.8 (c) illustrates this model. By sharing keys and values, the size of the KV cache would
be O(L · dh · m).
Grouped query attention (GQA) is a natural extension to multi-head attention and MQA
[Ainslie et al., 2023]. In GQA, heads are divided into ng groups, each corresponding to a shared set
[n ]
[n ]
[1]
[1]
of keys and values. Hence we have ng sets of keys and values {(K≤i , V≤i ), ▷▷▷, (K≤ig , V≤ig )}.
See Figure 2.8 (d) for an illustration. Let g(j) be the group id for the j-th head. The GQA model
can be expressed as
[j]
[g(j)]
[g(j)]
headj = Attqkv (qi , K≤i , V≤i )
(2.73)
The size of the KV cache of GQA is O(L·ng ·dh ·m). One beneﬁt of GQA is that we can trade-off
between computational efﬁciency and model expressiveness by adjusting ng . When ng = τ , the
model becomes the standard multi-head attention model. By contrast, when ng = 1, it becomes
the GQA model.
Sharing can also be performed across layers. Such a method falls into the family of shared
weight and shared activation methods, which have been extensively used in Transformers [De-
hghani et al., 2018; Lan et al., 2020]. For example, one can share KV activations or attention2.3 Long Sequence Modeling
value
key
81
query
(a) Single-head Attention
value
key
(b) Multi-head Attention
query
value
(c) Multi-query Attention
value
query
key
value
query
key
(d) Grouped Query Attention
key
query
Layer l
Sharing
Layer l − 1
(e) Cross-layer Multi-head Attention
Fig. 2.8: Illustration of QKV attention based on different multi-head and sharing mechanisms. (a) = single-head
attention, and (b-e) = attention with multiple heads.
weights across layers to reduce both computation and memory footprints [Xiao et al., 2019; Bran-
don et al., 2024]. Figure 2.8 (e) shows an illustration of this method, where a query in a layer
directly accesses the KV cache of a lower-level layer.
2.3.5
Position Extrapolation and Interpolation
Since Transformer layers are order-insensitive to input, we need some way to encode positional
information in the input tokens. To do this, it is common to add positional embeddings to token
embeddings, and then feed these combined embeddings into the Transformer layer stack as input.
In this case, the embedding at position i can be expressed as
ei = xi + PE(i)
(2.74)82
Generative Models
where xi ∈ Rd denotes the token embedding, and PE(i) ∈ Rd denotes the positional embedding.
In general, the token embedding xi is a position-independent vector, and so the positional embed-
ding PE(i) is used to encode the positional context. A straightforward approach is to treat PE(i)
as a learnable variable and train it alongside other model parameters. In this way, we can learn
a unique representation for each position, and thus distinguish the tokens appearing at different
positions of a sequence.
Representations of positions using learned vectors can work well in tasks where the sequences
at training and test times are of similar lengths. In practice, however, we often impose length
restrictions on sequences during training to prevent excessive computational costs, but wish to
apply the trained models to much longer sequences during inference. In this case, using learned
positional embeddings has obvious drawbacks, as there are no trained embeddings for positions
that are not observed in the training phase.
An alternative approach to modeling positional information is to develop positional embed-
dings that can generalize: once trained, the embedding model can be used to handle longer se-
quences. Suppose that we train a positional embedding model on sequences with a maximum
length of ml , and we wish to apply the trained model to a sequence of length m (m >> ml ). If
the embedding model is limited in the range of positions that we can observe from training data,
then this model will simply fail to deal with new data outside that range. See Figure 2.9 (a) for
an illustration where the learned embedding model cannot model data points outside the training
domain if it lacks the ability to extrapolate.
There are several approaches to making positional embedding models generalize. They can
be grouped into two classes.
• Extrapolation. The model learned on observed data points (i.e., positions) can be directly
employed to assign meaningful values to data points beyond the original range. For ex-
ample, suppose we have a series of numbers 1, 2, ▷▷▷, 10, and we want to understand the
meaning of a new number, 15. Knowing that these numbers are natural numbers used for
ordering, we can easily infer that 15 is a number that follows 10, even though 15 has not
been observed before. Figure 2.9 (b) shows an example of this approach, where a function
is learned to ﬁt the data points within a speciﬁc range and then applied to estimate the values
of data points outside that range.
• Interpolation. This approach maps a larger range of data points into the original obser-
vation range. For example, suppose we have a model designed for numbers in the range
[1, 10]. When given a new range of [1, 20], we can scale this down by dividing every num-
ber by 2, thereby ﬁtting all numbers into [1, 10]. This scaling allows us to use the model
trained on the range [1, 10] to describe data points in the expanded range of [1, 20]. See
Figure 2.9 (c) for an illustration of this approach.
In fact, positional embeddings in many systems have achieved some level of generalization.
For example, sinusoidal encoding, the most common positional embedding method, employs sine
and cosine functions that can naturally extend to sequences of any length. Although this approach
might seem direct and simple, it does not perform well when we signiﬁcantly extend the sequences
for processing. In this subsection, we will discuss several alternative methods based on either
extrapolation or interpolation.2.3 Long Sequence Modeling
83
Value
1
0
−1
01,024
Sequence Length
(a) Encoding with No Generalization2,048
01,024
Sequence Length
(b) Extrapolation2,048
01,024
Sequence Length
(c) Interpolation2,048
Value
1
0
−1
Value
1
0
−1
Fig. 2.9: Illustrations of different positional embedding methods for a range of positions. Blue points represent the
positions that have been observed during training, and red points represent the positions that are newly observed at test
time. In sub-ﬁgure (a), the encoding model only memorizes the points seen during training, and cannot generalize. In
sub-ﬁgures (b) and (c), the model can generalize through extrapolation and interpolation.
2.3.5.1
Attention with Learnable Biases
One problem with Eq. (2.74) is that the embedding model treats each token independently and
therefore ignores the distance between different tokens. A common improvement to this model,
called relative positional embedding, is to consider the pairwise relationship between tokens
[Shaw et al., 2018]. The general idea behind this is to obtain the offset between any pair of posi-
tions and incorporate it into the self-attention model. One of the simplest forms of self-attention
with relative positional embedding is given by
Attqkv (qi , K≤i , V≤i ) =
i

α(i, j)vj
(2.75)
j=0
α(i, j) = Softmax(
qi kjT + PE(i, j)
√
+ Mask(i, j))
d
(2.76)
The only difference between this model and the original self-attention model is that a bias term
PE(i, j) is added to the query-key product in this new model. Intuitively, PE(i, j) can be inter-
preted as a distance penalty for the pair of positions i and j. As i moves away from j, the value of84
Generative Models
PE(i, j) decreases.
PE(i, j) can be deﬁned in several different ways. Here, we consider the T5 version of relative
positional embedding, called the T5 bias [Raffel et al., 2020]. For each pair of query qi and key
kj , the offset between them is deﬁned to be15
d(i, j) = i − j
(2.77)
A simple design for the bias PE(i, j) is to share the same learnable variable for all query-key
pairs with the same offset, i.e., PE(i, j) = ui−j , where ui−j is the variable corresponding to
the offset i − j. However, simply assigning a unique value to each offset will restrict this model
to observed offsets. When i − j is larger than the maximum trained offset, the model cannot
generalize.
The T5 bias instead adopts a generalization of this model. Rather than assigning each query-
key offset a unique bias term, it groups difference offsets into “buckets”, each corresponding to
one learnable parameter. More speciﬁcally, the bias terms for nb + 1 buckets are given as follows.
• For buckets 0 to nb2+1 − 1, each bucket corresponds to one offset, that is, bucket 0 ↔ offset
0, bucket 1 ↔ offset 1, bucket 2 ↔ offset 2, and so on. We express this as b(i − j) = i − j.
• For buckets nb2+1 to nb , the size of each bucket increases logarithmically. For example, the
bucket number for a given offset i − j ≥ nb2+1 can be deﬁned as
b(i − j) =
log(i − j) − log( nb2+1 ) nb + 1
nb + 1
·
+⌊
⌋
2
2
log(distmax ) − log( nb2+1 )
(2.78)
where the parameter distmax is typically set to a relatively large number to indicate the
maximum offset we may encounter.
• When i − j > distmax , we place i − j in the last bucket. In other words, bucket nb contains
all the offsets that are not assigned to the previous buckets.
Together, these can be expressed as the function
b(i − j)
=


i − j
n +1
log(i−j)−log( b2 )
n +1
nb +1

min(nb , b2 + ⌊
nb +1 ·
2 ⌋)
log(distmax )−log(
2
)
0 ≤ i − j < nb2+1
i − j ≥ nb2+1
(2.79)
Figure 2.10 shows an illustration of these buckets. We see that in the ﬁrst half of the buckets,
each bucket is associated with only one value of i − j, while in the second half, the bucket size
increases as i − j grows. The last bucket is designed to handle sequences of arbitrarily long
lengths.
For language modeling, a query is only allowed to attend to its left-context, and so we have i − j ≥ 0. In the more
general case of self-attention, where a token can attend to all tokens in the sequence, we may have negative offsets
when i < j.
152.3 Long Sequence Modeling
85
logarithmically increased bucket size
ﬁxed bucket size
Bucket0123
Offset0123
(i − j)
···
14
15
16
17
14 15 16 ∼ 20 21 ∼ 26
18
27 ∼ 33
···
32
802 ∼ ∞
Fig. 2.10: Illustration of distributing query-key offsets into buckets in the T5 model (nb = 32 and distmax = 1024).
Boxes represent buckets. In the ﬁrst half of the buckets, we use a ﬁxed bucket size. In the second half of the buckets,
we increase the bucket size logarithmically. The last bucket contains all the query-key offsets that are not covered by
previous buckets.
All PE(i, j)s in a bucket share the same bias term ub(i−j) . Substituting PE(i, j) = ub(i−j)
into Eq. (2.76), the attention weight for qi and kj becomes16
α(i, j) = Softmax(
qi kjT + ub(i−j)
√
+ Mask(i, j))
d
(2.81)
The parameters {u0 , ▷▷▷, unb } are learned as common parameters during training. It should
be emphasized that this model can generalize to long sequences. This is because PE(i, j)s with
similar query-key offsets share the same parameter, and this sharing strategy is particularly im-
portant for achieving good generalization, given that large query-key offsets are rare in training.
In practice, we often set nb to a moderate number, and thus it can help control the overﬁtting of
positional embedding models.
2.3.5.2
Attention with Non-learned Biases
Relative positional embedding models are based on a set of learned biases for the query-key prod-
uct in self-attention. An alternative approach is to give these biases ﬁxed values via heuristics,
rather than training them on a particular dataset. One beneﬁt of this heuristics-based approach is
that it does not rely on a training process and thus can be directly applied to any sequences once
the biases are set.
One example of such an approach is Press et al. [2022]’s approach, called attention with
linear biases or ALiBi for short. In the ALiBi approach, the bias term is deﬁned as the negative
scaled query-key offset
PE(i, j) = −β · (i − j)
= β · (j − i)
(2.82)
where β is the scaling factor. Adding this term to the query-key product, we obtain a new form of
16
Note that, in Raffel et al. [2020]’s T5 model, the rescaling operation for the query-key product is removed. The
attention weight α(i, j) is then given by
α(i, j)
=
Softmax(qi kjT + ub(i−j) + Mask(i, j))
(2.80)86
Generative Models
qi kjT
Bias (ub(i−j) )
q0 kT
0u0T
q1 kT
0 q1 k1u1u0T
T
q2 kT
0 q2 k1 q2 k2u2u1u0u2u2u1u0T
T
T
T
q4 kT
0 q4 k1 q4 k2 q4 k3 q4 k4u3u2u2u1u0T
T
T
T
T
q5 kT
0 q5 k1 q5 k2 q5 k3 q5 k4 q5 k5u3u3u2u2u1u0
T
T
T
T
T
T
q6 kT
0 q6 k1 q6 k2 q6 k3 q6 k4 q6 k5 q6 k6u3u3u3u2u2u1
+
T
T
T
q3 kT
0 q3 k1 q3 k2 q3 k3
u0
(a) The T5 bias (nb = 3 and distmax = 5)
qi kjT
Bias (−β(i − j))
0
q0 kT
0
0
T
q1 kT
0 q1 k1−1β
T
T
q2 kT
0 q2 k1 q2 k2−2β −1β
+
T
T
T
q3 kT
0 q3 k1 q3 k2 q3 k3
0
−3β −2β −1β
0
T
T
T
T
q4 kT
0 q4 k1 q4 k2 q4 k3 q4 k4−4β −3β −2β −1β0T
T
T
T
T
q5 kT
0 q5 k1 q5 k2 q5 k3 q5 k4 q5 k5−5β −4β −3β −2β−β0
T
T
T
T
T
T
q6 kT
0 q6 k1 q6 k2 q6 k3 q6 k4 q6 k5 q6 k6−6β −5β −4β −3β −2β−β
0
(b) The ALiBi bias
Fig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the
biases ranges from light blue denoting small absolute values to deep blue denoting large absolute values.
attention weights
α(i, j) = Softmax(
qi kjT + β · (j − i)
√
+ Mask(i, j))
d
(2.83)
This model can be interpreted as adding a ﬁxed penalty to qi kjT whenever j moves one step
away from i. So we do not need to adapt it to a range of sequence lengths, and can employ it to
model arbitrarily long sequences. See Figure 2.11 for a comparison of the T5 bias and the ALiBi
bias.
In general, the scalar β should be tuned on a validation dataset. However, Press et al. [2022]
found that setting β to values decreasing geometrically by a factor of 21a for multi-head attention
performs well on a variety of tasks. Speciﬁcally, for a self-attention sub-layer involving nhead2.3 Long Sequence Modeling
87
EntryQuery-Key Bias (PE(i, j))
T5 [Raffel et al., 2020]
ALiBi [Press et al., 2022]ub(i−j)
−β · ( i − j )
−β1 ( i − j )β2
Kerple [Chi et al., 2022]
(power)
−β1 log(1 + β2 ( i − j ))
¯
d◁2
k=1 cos
Sandwich [Chi et al., 2023]


(logarithmic)
f ψ( i − j )◁ψ(max(mlen , i))
FIRE [Li et al., 2024b]

¯
( i − j )◁100002k◁d

¯ and mlen are hyper-parameters. In the T5
Table 2.4: Query-key biases as relative positional embeddings. β, β1 , β2 , d,
model, b(i − j) denotes the bucket assigned to i − j. In the FIRE model, ψ(·) is a monotonically increasing function
such as ψ(x) = log(cx + 1), and f (·) is an FFN.
heads, the scalar for the k-th head is given by
βk =
1
8
2k
(2.84)
The ALiBi approach provides a simple form of relative positional embeddings. There are
other similar methods for designing query-key biases using the offset i − j. Table 2.4 shows a
comparison of such biases. As an aside it is worth noting that the form of the right-hand side
of Eq. (2.82) is very similar to length features used in conventional feature-based systems. For
example, in statistical machine translation systems, such features are widely used to model word
reordering problems, resulting in models that can generalize well across different translation tasks
[Koehn, 2010].
2.3.5.3
Rotary Positional Embedding
As with sinusoidal embeddings, rotary positional embeddings are based on hard-coded values for
all dimensions of an embedding [Su et al., 2024]. Recall that in the sinusoidal embedding model,
positions are represented as combinations of sine and cosine functions with different frequencies.
These embeddings are then added to token embeddings to form the inputs to the Transformer
layer stack. Rotary positional embeddings instead model positional context as rotations to token
embeddings in a complex space. This leads to a model expressed in the form of multiplicative
embeddings
ei = xi R(i)
(2.85)
where R(i) ∈ Rd×d is the rotation matrix representing the rotations performed on the token
embedding xi ∈ Rd .
For simplicity, we will ﬁrst consider embeddings with only two dimensions and return to a
discussion of the more
 general formulation later. Suppose we have a 2-dimensional token embed-
ding x = x1 x2 . We can represent it as a vector in a plane, originating at the origin (0, 0)
and terminating at (x1 , x2 ). A counterclockwise rotation of this vector refers to an operation of
moving the vector around the origin while maintaining its magnitude, as shown in Figure 2.12 (a).
The degree of rotation is usually deﬁned by a speciﬁc angle, denoted by θ. The rotation can be88
Generative Models
x2
x2
vector x
rotated vector
xRθ
x
xRθ
θ
θ
θ
x1
xR2θ
x1
θ
xR3θ
(a) Single-step Rotation
(b) Multi-step Rotation
The1 cat2 is3 sleeping4 peacefully5
in6 the7 warm8 sunlight9 .10
x2
sleeping4
7θ cat2
7θ
x1
sleeping11
Every1 afternoon2 ,3 you4 ’ll5 ﬁnd6 that7
the8 cat9 is10 sleeping11 on12 my13 bed14 .15
cat9
(c) Angles between embeddings of two tokens at different positions
Fig. 2.12: Illustrations of vector rotations in a plane. Sub-ﬁgures (a) and (b) show rotations of a vector in a single
step and multiple steps, respectively. Sub-ﬁgure (c) shows the embeddings of tokens cat and sleeping in two different
sentences. We show these sentences with a subscript afﬁxed to each token to indicate its position. If we represent
tokens as vectors, we can add positional information by rotating these vectors. This rotation preserves the “distances”
between the vectors. For example, given that the distance between cat and sleeping is the same in both sentences, the
angle between their embeddings also remains the same during rotation.
expressed mathematically in the form
Ro(x, θ) = xRθ
=
=




x1 x2


cos θ sin θ
− sin θ cos θ

cos θ · x1 − sin θ · x2 sin θ · x1 + cos θ · x2

(2.86)
cos θ sin θ
where Rθ =
is the rotation matrix. If two or more rotations are performed on the
− sin θ cos θ
same vector, we can rotate the vector further. This follows from the fact that the composition of
successive rotations is itself a rotation. More formally, rotating a vector by an angle θ for t times2.3 Long Sequence Modeling
89
can be expressed as
Ro(x, tθ) = xRtθ
=

cos tθ · x1 − sin tθ · x2 sin tθ · x1 + cos tθ · x2

(2.87)
If we interpret t as the position of a token represented by x in a sequence, then we will ﬁnd
that the above equation deﬁnes a simple positional embedding model. As shown in Figure 2.12
(b), we start moving the token from position 0. Each time we move one step forward, the vector
is rotated by the angle θ. Upon arriving at the position t, the representation of the token with
positional context is given by Ro(x, iθ). As the rotations do not change the magnitude of the
embedding, the original “meaning” of the token is retained. The positional information is injected
into the embedding, when it gets rotated.
A popular way to understand
 vector rotation is to deﬁne it in complex spaces. It is easy
to transform each vector x = x1 x2 in the 2D Euclidean space R2 to a complex number
x′ = x1 + ix2 in the complex space C via a bijective linear map. Then, the rotation of x with the
angle tθ corresponds to the multiplication by eitθ . Given that eitθ = cos tθ + i sin tθ, the rotation
operation can be re-expressed in the form
xRtθ → x′ eitθ
=(x1 + ix2 )(cos tθ + i sin tθ)
=cos tθ · x1 − sin tθ · x2 + i(sin tθ · x1 + cos tθ · x2 )
(2.88)
Here we denote the token representation x′ eitθ by C(x, tθ). The inner product of the representa-
tions of the tokens at positions t and s can be written as
⟨C(x, tθ), C(y, sθ)⟩ = (x′ y′ )ei(t−s)θ
(2.89)
where y′ is the complex conjugate of y′ . As can be seen, the result of this inner product involves
a term t − s, and so it can model the offset between the two tokens.
Now we go back to representations in the 2D Euclidean space. The dot-product of Ro(x, tθ)
and Ro(y, sθ) is can be written as a function of (t − s)θ
Ro(x, tθ)[Ro(y, sθ)]T = xRtθ [yRsθ ]T
= xRtθ [Rsθ ]T yT
= xR(t−s)θ yT
(2.90)
Given this result, if we consider Ro(x, tθ) and Ro(y, sθ) as the query and the key, then the self-
attention operation will implicitly involve the modeling of relative positional context.
This rotary positional embedding can be extended to multi-dimensional embeddings. For
a d-dimensional token embedding x = x1 x2 ▷▷▷ xd , we can treat it as a d2 -dimensional




complex vector x′ = x′1 x′2 ▷▷▷ x′d◁2 = x1 + ix2 x3 + ix4 ▷▷▷ xd−1 + ixd , where
each consecutive pair of items forms a complex number. Then, the rotary positional embedding in90
Generative Models
the complex space is given by
C(x, tθ) =
d◁2

x′k eitθk ⃗ek
(2.91)
k=1
where ⃗ek is the standard basis vector with a single non-zero value in the k-th coordinate and 0’s
elsewhere [Biderman et al., 2021].
Although this formula involves a complicated expression, its equivalent form in the d-dimensional
Euclidean space is relatively easy to understand. We can write it as
Ro(x, tθ) =


x1 x2 ▷▷▷



xd 



Rtθ1
Rtθ2
..
.
Rtθd◁2






(2.92)


cos tθk sin tθk
where Rtθk =
. θ = θ1 , ▷▷▷, θd◁2 are the parameters for controlling the an-
− sin tθk cos tθk
gles of rotations in different dimensions. Typically, θk is set to 10000−
to the setting in sinusoidal embeddings.
2(k−1)
d
, which is analogous
In a practical implementation, Eq. (2.92) can be rewritten into a form that relies solely on the
element-wise product and addition of vectors.

T

T

T

T
x1
cos tθ1
−x2
sin tθ1
 x 
 cos tθ 
 x 
 sin tθ 
1 
1 
 2 

 1 

 . 






..
..
 ⊙
 +  ..  ⊙ 

.
Ro(x, tθ) = 
.
.
 . 


 . 










xd−1 
cos tθd◁2 
 −xd 
sin tθd◁2 
xd
cos tθd◁2
xd−1
sin tθd◁2
(2.93)
Finally, we rewrite Eq. (2.85) to obtain the form of the embedding at position i
ei = Ro(xi , iθ)
2.3.5.4
(2.94)
Position Interpolation
In position interpolation, our goal is to map the positions in the new sequence to match the ob-
served range in training. Suppose the sequence length for training ranges from 0 to ml . When
m > ml at test time, we represent the positions in [0, m] such that our representations ﬁt [0, ml ].
To illustrate, consider the rotary positional embedding model described
above.
The embedding


of each token is described by a model Ro(xi , iθ) in which θ = θ1 , ▷▷▷, θd◁2 are the parameters.
Ro(xi , iθ) can be cast in the form of a linear combination of two periodic functions (see Eq.2.3 Long Sequence Modeling
91
(2.93))
cos iθ =
sin iθ =


cos iθ1 ▷▷▷ cos iθd◁2
sin iθ1 ▷▷▷ sin iθd◁2


(2.95)
(2.96)
θk is a exponential function of k and takes the form
θk = b−
2(k−1)
d
(2.97)
where b is the base. The period of cos iθk and sin iθk is
Tk = 2π · b
2(k−1)
d
(2.98)
The key idea behind position interpolation is to adjust this period so that the new positions can
m
be encoded within the range [0, ml ]. One way to achieve this is to scale up Tk by m
, given by
l
Tk′ =
2(k−1)
m
· 2π · b d
ml
(2.99)
Hence all points in [0, m] are compressed into [0, ml ]. This linear scaling can be easily realized
by modifying the input to the embedding model [Chen et al., 2023c]. The new model with linear
positional interpolation is given by
Ro′ (xi , iθ) = Ro(xi ,
ml
iθ)
m
(2.100)
Another method of positional interpolation is to scale the base17 . Suppose that the base b is
scaled by λ. We wish the period of this new model in the last dimension of θ (i.e., dimension d2 )
to be equal to that of the linear positional interpolation model. This can be expressed as
2π · (λb)
2( d
2 −1)
d
=
2( d
m
2 −1)
· 2π · b d
ml
(2.101)
Solving this equation, we obtain
λ =
 m  2( dd−1)
2
ml
m d
d−2
=
ml(2.102)
Ro′ (xi , iθ) = Ro(xi , iθ ′ )(2.103)
This gives an embedding model
17
This method was ﬁrst proposed in https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/
ntkaware_scaled_rope_allows_llama_models_to_have/92
Generative Models
where

0
2
θ′ = (λb)− d , (λb)− d , ▷▷▷, (λb)− d
d−2

(2.104)
Note that scaling the base provides a non-uniform method for scaling the periods across dif-
ferent dimensions of θ. This method has been found to be helpful for extending LLMs to longer
sequences, and several improvements have been developed [Peng et al., 2024; Ding et al., 2024].
2.3.6
Remarks
In this section, we have presented a variety of methods for long-context language modeling. We
close this section by discussing some interesting issues related to these methods.
2.3.6.1
Need for Long Context
One of the ultimate goals of long-context LLMs is that these models can precisely encode inﬁnite
context. The so-called inﬁnite context refers more to the fact that an LLM can continuously read
words. This motivates LLMs that can handle extremely long context or stream data. As discussed
in Section 2.3.3, it is common to use ﬁxed-size memory models to process continuously expanding
context. Many such systems are based on recurrent architectures or their variants, because they
are inherently suited to model time series problems where the effects of past inputs continue
indeﬁnitely. Another way to achieve inﬁnite memory is to develop alternatives to self-attention
models, for example, one can use continuous-space attention models to encode context, which
removes the dependency on context length [Martins et al., 2022].
When studying long-context LLMs, it is natural to wonder what mechanisms may explain the
use of long context in language modeling. Can we compress the representation of inﬁnite context
into a relatively small-sized model? Are all context tokens useful for predicting next tokens? How
do LLMs prepare for token prediction when they see the context? Can we know in advance which
contextual information will be critical for prediction? General answers to all these questions
are not obvious, but they inspire follow-on research of explainable models, and some interesting
results have been found. For example, Deletang et al. [2024] conducted extensive experiments
to show that LLMs are powerful in-context compressors. Although viewing predictive models
as compression models has long been studied in machine learning, it also provides insights into
our understanding of the LLM scaling laws. Pal et al. [2023] and Wu et al. [2024] investigated
whether the features learned up to the current step, though not intentionally, are already sufﬁcient
for predicting tokens at the following steps. Note that the need for long-context in language
modeling is highly dependent on the problem that we address. A related issue is where to apply
LLMs and how to evaluate them. For example, in summarization tasks we may only need to distill
and focus on a few key aspects of the text, while in retrieval-like tasks we need to “memorize”
the entire context so that the relevant information can be accessed. We will discuss the evaluation
issue later in this subsection.
2.3.6.2
Pre-training or Adapting LLMs?
Training LLMs requires signiﬁcant computational costs. Although it is straightforward to train
LLMs on long sequence data, the training becomes computationally unwieldy for large data sets. It2.3 Long Sequence Modeling
93
is common practice to pre-train LLMs on general datasets, and then adapt them with modest ﬁne-
tuning effort. For example, LLMs with relative or rotary positional embeddings can be directly
trained on large-scale data in the pre-training phase. While the resulting models may exhibit some
abilities to extrapolate lengths in the inference phase, it may be more effective to ﬁne-tune them
on longer sequences.
Ideally, we would like to pre-train LLMs with standard Transformer architectures and adapt
them to new tasks. This allows us to use many off-the-shelf LLMs and efﬁciently adapt them to
handle long sequences. However, when new architectures are adopted, it seems inevitable that
we need to train these models from scratch. This poses practical difﬁculties for developing long-
context LLMs, as we cannot leverage well-developed, pre-trained models and must instead train
them ourselves. On the other hand, ﬁne-tuning is still an effective way to adapt LLMs with certain
architectures that are different from those in pre-training. An example is models augmented with
external memories. In these models, the pre-trained LLMs are ﬁxed, and the focus is on how
to make these LLMs collaborate with the memory models. In RAG, for instance, it is common
to ﬁne-tune LLMs to improve their use of retrieval-augmented inputs. Another example of ﬁne-
tuning LLMs for long-context modeling is that we train an LLM with full attention models, and
then replace them with sparse attention models in the ﬁne-tuning phase. The pre-trained LLM
provides initial values of model parameters used in a different model, and this model is then ﬁne-
tuned as usual.
2.3.6.3
Evaluating Long-context LLMs
Evaluating long-context LLMs is important, but it is a new issue in NLP. The general idea is that,
if we input a long context to an LLM, then we can check from the output of the LLM whether it
understands the entire context and makes use of it in predicting following tokens. In conventional
research of NLP, such evaluations are often aimed at examining the ability of NLP models in
handling long-range dependencies. However, the size of contexts used in recent LLMs is much
larger than that used in NLP systems a few years ago. This motivates researchers to develop new
evaluation benchmarks and metrics for long-context LLMs.
One approach is to use the perplexity metric. However, in spite of its apparent simplicity, this
method tends to reﬂect more on the LLMs’ ability to make use of local context rather than global
context. It is therefore tempting to develop evaluation methods that are speciﬁc to long-context
LLMs. Popular methods include various synthetic tasks where artiﬁcially generated or modiﬁed
data is used to evaluate speciﬁc capabilities of long-context LLMs. In needle-in-a-haystack18 and
passkey retrieval tasks [Mohtashami and Jaggi, 2024; Chen et al., 2023c], for instance, LLMs are
required to identify and extract a small, relevant piece of information from a large volume of given
text. The assumption here is that an LLM with sufﬁcient memory should remember earlier parts
of the text as it processes new information. This LLM can thus pick out the relevant details, which
might be sparse and hidden among much irrelevant information, from the text. Alternatively, in
copy memory tasks (or copy tasks for short), LLMs are used to repeat the input text or a speciﬁc
segment multiple times. These tasks were initially proposed to test the extent to which recurrent
models can retain and recall previously seen tokens [Hochreiter and Schmidhuber, 1997; Arjovsky
et al., 2016], and have been adopted in evaluating recent LLMs [Bulatov et al., 2022; Gu and Dao,
2023].
18
https://github.com/gkamradt/LLMTest_NeedleInAHaystack94
Generative Models
Another approach to evaluating long-context LLMs is to test them on NLP tasks that involve
very long input sequences. Examples include long-document or multi-document summarization,
long-document question answering, code completion, and so on. A beneﬁt of this approach is that
it can align evaluations with user expectations.
Although many methods have been developed, there is still no general way to evaluate long-
context LLMs [Liu et al., 2024c]. One problem is that most of these methods focus on speciﬁc
aspects of LLMs, rather than their fundamental ability to model very long contexts. Even though
an LLM can pick out the appropriate piece of text from the input, we cannot say that it truly un-
derstands the entire context. Instead, it might just remember some important parts of the context,
or even simply recall the answer via the model learned in pre-training. Moreover, the data used
in many tasks is small-scale and relatively preliminary, leading to discrepancies between evalu-
ation results and actual application performance. A more interesting issue is that the results of
LLMs are inﬂuenced by many other factors and experimental setups, for example, using different
prompts can lead to very different outcomes. This makes evaluation even more challenging be-
cause improvements may not solely result from better modeling of long contexts, and there is a
risk of overclaiming our results. Nevertheless, many open questions remain in the development
and evaluation of long-context LLMs. For example, these models still suffer from limitations
such as restricted context length and high latency. Studying these issues is likely to prove valuable
future directions.
2.4
Summary
In this chapter, we have discussed the concept of LLMs and related techniques. This can be consid-
ered a general, though not comprehensive, introduction to LLMs, laying the foundation for further
discussions on more advanced topics in subsequent chapters. Furthermore, we have explored two
ways to scale up LLMs. The ﬁrst focuses on the large-scale pre-training of LLMs, which is cru-
cial for developing state-of-the-art models. The second focuses on methods for adapting LLMs to
long inputs, including optimizing attention models, designing more efﬁcient and compressed KV
caches, incorporating memory models, and exploring better positional embeddings.
The strength of LLMs lies in their ability to break the constraints of training NLP models for
a limited number of speciﬁc tasks. Instead, LLMs learn from large amounts of text through the
simple task of token prediction — we predict the next token in a sentence given its prior tokens.
A general view is that, by repeating this token prediction task a large number of times, LLMs can
acquire some knowledge of the world and language, which can then be applied to new tasks. As a
result, LLMs can be prompted to perform any task by framing it as a task of predicting subsequent
tokens given prompts. This emergent ability in language models comes from several dimensions,
such as scaling up training, model size, and context size. It is undeniable that scaling laws are
currently the fundamental principle adopted in developing large language models, although sim-
ply increasing model size has yet to prove sufﬁcient for achieving AGI. These continuously scaled
LLMs have been found to show capabilities in general-purpose language understanding, genera-
tion, and reasoning. More recently, it has been found that scaling up the compute at inference time
can also lead to signiﬁcant improvements in complex reasoning tasks [OpenAI, 2024].
Given their amazing power, LLMs have attracted considerable interest, both in terms of tech-
niques and applications. As a result, the explosion of research interest in LLMs has also led to a2.4 Summary
95
vast number of new techniques and models. However, we do not attempt to provide a comprehen-
sive literature review on all aspects of LLMs, given the rapid evolution of the ﬁeld. Nevertheless,
one can still gain knowledge about LLMs from general reviews [Zhao et al., 2023; Minaee et al.,
2024] or more focused discussions on speciﬁc topics [Ruan et al., 2024].https://github.com/NiuTrans/NLPBook
https://niutrans.github.io/NLPBook
C HAPTER 3
Prompting
In the context of LLMs, prompting refers to the method of providing an LLM with a speciﬁc input
or cue to generate a desired output or perform a task. For example, if we want the LLM to translate
a sentence from English to Chinese, we can prompt it like this
Translate the text from English to Chinese.
Text: The early bird catches the worm.
Translation:
Prompting is crucial for LLMs because it directly inﬂuences how effectively these models under-
stand and respond to user queries. A well-crafted prompt can guide an LLM to generate more
accurate, relevant, and contextually appropriate responses. Furthermore, this process can be iter-
atively reﬁned. By analyzing the responses of the LLM, users can adjust their prompts to align
more closely with their speciﬁc needs. Given the importance of prompting in applying LLMs,
prompt design has become an essential skill for users and developers working with LLMs. This
leads to an active research area, called prompt engineering, in which we design effective prompts
to make better use of LLMs and enhance their practical utility in real-world applications.
An important concept related to prompting is in-context learning. When prompting an LLM,
we can add new information to the context, such as demonstrations of problem-solving. This
allows the LLM to learn from this context how to solve the problem. Here is an example of
prompting LLMs with a few demonstrations of how to classify text based on sentiment polarity.
Here are some examples of text classiﬁcation.
Example 1: We had a delightful dinner together. → Label: Positive
Example 2: I’m frustrated with the delays. → Label: Negative
What is the label for “That comment was quite hurtful.”?
Label:
In-context learning is often seen as an emergent ability of LLMs that arises after pre-training.
Though LLMs can be trained or tuned to perform new tasks, in-context learning provides a very
efﬁcient way to adapt these models without any training or tuning effort. Perhaps this is one of
the most notable features of LLMs: they indeed learn general knowledge about the world and
language during pre-training, which we can easily apply to new challenges. Moreover, in-context
learning reﬂects the broader trend of making AI systems more generalizable and user-friendly.
Instead of requiring specialized engineers to ﬁne-tune models for every unique task, users can
interact with LLMs in a more intuitive way, simply providing examples or adjusting the context
as needed.
In this chapter, we focus on prompting techniques in LLMs. We begin by considering several
interesting prompt designs commonly used in prompt engineering. Then, we discuss a series of3.1 General Prompt Design
97
reﬁnements to these methods. Finally, we explore approaches for automating prompt design.
3.1
General Prompt Design
This section presents basic concepts in prompt design, along with examples of how to prompt
LLMs for various NLP tasks. Since the effectiveness of prompting is highly dependent on the
LLMs being used, prompts often vary across different LLMs, making it difﬁcult to provide a
comprehensive list of prompts for all LLMs and downstream tasks. Therefore, this discussion is
not focused on any speciﬁc LLM. Instead, the goal is to provide guiding principles for prompt
design.
3.1.1
Basics
The term prompt is used in many different ways. In this chapter we deﬁne a prompt as the input
text to an LLM, denoted by x. The LLM generates a text y by maximizing the probability Pr(y|x).
In this generation process, the prompt acts as the condition on which we make predictions, and it
can contain any information that helps describe and solve the problem.
A prompt can be obtained using a prompt template (or template for short) [Liu et al., 2023a].
A template is a piece of text containing placeholders or variables, where each placeholder can
be ﬁlled with speciﬁc information. Here are two templates for asking the LLM for weekend
suggestions.
Please give me some suggestions for a fun weekend.
If {∗premise∗}, what are your suggestions for a fun weekend.
In the ﬁrst template, we simply instruct the LLM to return some suggestions. So the template
is just a piece of text with no variables. In the second template, the variable {∗premise∗} needs to
be speciﬁed by the users to provide a premise for making suggestions. For example, if we input
premise = the weather is nice this weekend
then we can generate a prompt
If the weather is nice this weekend,
what are your suggestions for a fun weekend.
We can also design a template with multiple variables. Here is an example in which we
compare the two sentences in terms of their semantic similarity.98
Prompting
Here is a sentence
{∗sentence1∗}
Here is another sentence
{∗sentence2∗}
Compute the semantic similarity between the two sentences
A popular way to format prompts is to write each input or output in a “name:content” style.
For example, we can describe a conversation between two people, named John and David, and use
the LLM to continue the conversation. A template of such prompts is given by
John: {∗utterance1∗}
David: {∗utterance2∗}
John: {∗utterance3∗}
David: {∗utterance4∗}
John: {∗utterance5∗}
David: {∗utterance6∗}
John: {∗utterance7∗}
David:
The “name:content” format can be used to deﬁne the task that we want the LLM to perform.
For example, given that “Q” and “A” are commonly used abbreviations for “Question” and “An-
swer”, respectively, we can use the following template to do question-answering.
Q: {∗question∗}
A:
This format can be used to describe more complex tasks. For example, the following is an
example of providing a speciﬁcation for a translation task
Task: Translation
Source language: English
Target language: Chinese
Style: Formal text
Template: Translate the following sentence: {∗sentence∗}
In practical systems, it is common to represent and store such data in key-value pairs, such as the
JSON format1 .
When the problem is difﬁcult to describe in an attribute-based manner, it is more common
to instruct LLMs with a clear and detailed description. There are many ways to do this. One
1
The JSON representation is3.1 General Prompt Design
99
example is to assign a role to LLMs and provide sufﬁcient context. The following is a template
that instructs an LLM to act as an expert and answer questions from children.
You are a computer scientist with extensive knowledge in the ﬁeld of deep learn-
ing.
Please explain the following computer-related concept to a child around 10 years
old, using simple examples whenever possible.
{∗concept∗}
Here the text “You are a computer scientist ... deep learning. ” is sometimes called system
information, and is provided to help the LLM understand the context or constraints of the task it
is being asked to perform.
3.1.2
In-context Learning
Learning can occur during inference. In-context learning is one such method, where prompts
involve demonstrations of problem-solving, and LLMs can learn from these demonstrations how
to solve new problems. Since we do not update model parameters in this process, in-context
learning can be viewed as a way to efﬁciently activate and reorganize the knowledge learned in
pre-training without additional training or ﬁne-tuning. This enables quick adaptation of LLMs to
new problems, pushing the boundaries of what pre-trained LLMs can achieve without task-speciﬁc
adjustments.
In-context learning can be illustrated by comparing three methods: zero-shot learning, one-
shot learning and few-shot learning. Zero-shot learning, as its name implies, does not involve a
traditional “learning” process. It instead directly applies LLMs to address new problems that were
not observed during training. In practice, we can repetitively adjust prompts to guide the LLMs in
generating better responses, without demonstrating problem-solving steps or providing examples.
Consider the following example. Suppose we want to use an LLM as an assistant that can help
correct English sentences. A zero-shot learning prompt is given by
{
}
"Task": "Translation"
"Source language": "English"
"Target language": "Chinese"
"Style": "Formal text"
"Template": "Translate the following sentence: {∗sentence∗}"100
Prompting
SYSTEM
USER
You are a helpful assistant, and are great at grammar correction.
You will be provided with a sentence in English. The task is
to output the correct sentence.
Input: She don’t like going to the park.
Output:
Here the gray words are used to indicate different ﬁelds of the prompt.
In one-shot learning, we extend this prompt by adding a demonstration of how to correct
sentences, thereby allowing the LLM to learn from this newly-added experience.
SYSTEM
DEMO
You are a helpful assistant, and are great at grammar correction.
You will be provided with a sentence in English. The task is
to output the correct sentence.
Input: There is many reasons to celebrate.
Output: There are many reasons to celebrate.
USER
You will be provided with a sentence in English. The task is
to output the correct sentence.
Input: She don’t like going to the park.
Output:
Furthermore, we can add more demonstrations to enable few-shot learning.
SYSTEMYou are a helpful assistant, and are great at grammar correction.
DEMO1You will be provided with a sentence in English. The task is
to output the correct sentence.
Input: There is many reasons to celebrate.
Output: There are many reasons to celebrate.
DEMO2
You will be provided with a sentence in English. The task is
to output the correct sentence.
Input: Me and my friend goes to the gym every day.
Output: My friend and I go to the gym every day.
USER
You will be provided with a sentence in English. The task is
to output the correct sentence.
Input: She don’t like going to the park.
Output:
In few-shot learning, we essentially provide a pattern that maps some inputs to the corre-
sponding outputs. The LLM attempts to follow this pattern in making predictions, provided that
the prompt includes a sufﬁcient number of demonstrations, although generally small. It is also3.1 General Prompt Design
101
possible to use simpler patterns to achieve this. For example, one can use the following few-shot
learning prompt for translating words from Chinese to English.
DEMO
USER
现在
来
去
男孩
女孩
→
→
→
→
→
now
come
go
boy
If the LLM is powerful enough, few-shot learning can enable it to address complex prob-
lems, such as mathematical reasoning. For example, consider the following task of summing two
numbers and then dividing the sum by their product.
DEMO
USER
12 5
3 1
−9 4
15 15
19 73
→
→
→
→
→
(12 + 5)◁(12 × 5) = 0▷283
(3 + 1)◁(3 × 1) = 1▷33
(−9 + 4)◁(−9 × 4) = 0▷138
(15 + 15)◁(15 × 15) = 0▷133
In many practical applications, the effectiveness of in-context learning relies heavily on the
quality of prompts and the fundamental abilities of pre-trained LLMs. On one hand, we need a
signiﬁcant prompt engineering effort to develop appropriate prompts that help LLMs learn more
effectively from demonstrations. On the other hand, stronger LLMs can make better use of in-
context learning for performing new tasks. For example, suppose we wish to use an LLM to
translate words from Inuktitut to English. If the LLM lacks pre-training on Inuktitut data, its
understanding of Inuktitut will be weak, and it will be difﬁcult for the model to perform well in
translation regardless of how we prompt it. In this case, we need to continue training the LLM
with more Inuktitut data, rather than trying to ﬁnd better prompts.
It might be interesting to explore how in-context learning emerges during pre-training and why
it works during inference. One simple understanding is that LLMs have gained some knowledge
of problem-solving, but there are many possible predictions, which are hard to distinguish when
the models confront new problems. Providing demonstrations can guide the LLMs to follow the
“correct” paths. Furthermore, some researchers have tried to interpret in-context learning from
several different perspectives, including Bayesian inference [Xie et al., 2022], gradient descent
[Dai et al., 2023; Von Oswald et al., 2023], linear regression [Akyürek et al., 2023], meta learning
[Garg et al., 2022], and so on.
3.1.3
Prompt Engineering Strategies
Designing prompts is highly empirical. In general, there are many ways to prompt an LLM for
performing the same task, and we need to perform a number of trial-and-error runs to ﬁnd a
satisfactory prompt. To write good prompts more efﬁciently, one can follow certain strategies.
Examples of common prompting principles include102
Prompting
• Describing the task as clearly as possible. When we apply an LLM to solve a problem,
we need to provide a precise, speciﬁc, and clear description of the problem and instruct the
LLM to perform as we expect. This is particularly important when we want the output of
the LLM to meet certain expectations. For example, suppose we are curious about climate
change. A simple prompt for asking the LLM to provide some information is
Tell me about climate change.
Since this instruction is too general, the LLM may generate a response that addresses any
aspect of climate change, which may not align with our speciﬁc interests. In this case, we
can instead use prompts that are speciﬁc and detailed. One such example is
Provide a detailed explanation of the causes and effects of climate change,
including the impact on global temperatures, weather patterns, and sea
levels. Also, discuss possible solutions and actions being taken to mitigate
these effects.
Now suppose we intend to explain climate change to a 10-year-old child. We can adjust the
above prompt further.
Explain the causes and effects of climate change to a 10-year-old child.
Talk about how it affects the weather, sea levels, and temperatures. Also,
mention some things people are doing to help. Try to explain in simple
terms and do not exceed 500 words.
• Guiding LLMs to think. LLMs have exhibited surprisingly good capabilities to “think”.
A common example is that well-developed LLMs have achieved impressive performance
in mathematical reasoning tasks, which are considered challenging. In prompt engineering,
the “thinking” ability of LLMs needs to be activated through appropriate prompting, espe-
cially for problems that require signiﬁcant reasoning efforts. In many cases, an LLM that
is instructed to “think” can produce completely different results compared with the same
LLM that is instructed to perform the task straightforwardly. For example, Kojima et al.
[2022] found that simply appending “Let’s think step by step” to the end of each prompt
can improve the performance of LLMs on several reasoning tasks. LLMs can be prompted
to “think” in a number of ways. One method is to instruct LLMs to generate steps for rea-
soning about the problem before reaching the ﬁnal answer. For example, consider a task of
solving mathematical problems. See below for a simple prompt for this task.3.1 General Prompt Design
103
You are a mathematician. You will be provided with a math problem.
Please solve the problem.
Since solving math problems requires a detailed reasoning process, LLMs would probably
make mistakes if they attempted to work out the answer directly. So we can explicitly ask
LLMs to follow a given reasoning process before coming to a conclusion.
You are a mathematician. You will follow these detailed reasoning steps
when solving math problems.
Step 1: Problem Interpretation.
The mathematician carefully listens to your query and understands the in-
tricate details of the mathematical challenge you have presented.
Step 2: Strategy Formulation.
Drawing upon their extensive knowledge, the mathematician chooses the
most effective strategy tailored to the type of math problem, whether it is
algebra, calculus, or geometry.
Step 3: Detailed Calculation.
With precision and expertise, the mathematician performs the necessary
calculations step by step, adhering to all mathematical principles.
Step 4: Solution Review.
Before providing the ﬁnal answer, the mathematician meticulously checks
the calculations for accuracy and offers a concise explanation or rationale
for the solution.
You will be provided with a math problem. Please solve the problem.
{∗problem∗}
Another method to guide LLMs to “think” is through multiple rounds of interaction with
LLMs. For example, as a ﬁrst step, we can instruct LLMs to solve the problem directly
You will be provided with a math problem. Please solve the problem.
{∗problem∗}
Now we have an initial answer to the problem. As a second step, we prompt LLMs to
evaluate the correctness of the answer and, if necessary, rework it to ﬁnd a better solution.104
Prompting
You will be provided with a math problem, along with a solution. Evaluate
the correctness of this solution, and identify any errors if present. Then,
work out your own solution.
Problem: {∗problem∗}
Solution: {∗solution∗}
The prompts presented here are closely related to a long line of research on reasoning prob-
lems in LLMs. It is impossible to provide a complete discussion of all related issues because
this topic covers a large family of methods. But we will see a relatively more detailed dis-
cussion on how to improve prompting through more reasoning in Section 3.2.
• Providing reference information. As discussed in the previous section, we can include
demonstrations in prompts and allow LLMs to in-context learn from these demonstrations
how to perform the task. In fact, given the remarkable ability of language understanding of
LLMs, we can add any type of text into the prompts and so these models can predict based
on enriched contexts. In many applications, we have various information that is relevant
to user queries. Instead of using LLMs to make unconstrained predictions, we often want
LLMs to produce outputs that are conﬁned to the relevant text. One such example is RAG,
where the relevant text for the user query is provided by calling an IR system, and we
prompt LLMs to generate responses based on this provided relevant text. The following
prompt shows an example.
You are an expert that can generate answers to input queries. You have now
been provided with a query and the corresponding context information.
Please generate an answer based on this context information. Note that
you need to provide the answer in your own words, not just copy from the
context provided.
Context information: {∗IR-result∗}
Query: {∗query∗}
If the context information is highly reliable, we can even restrict LLMs to answering using
only the provided text. An example prompt is shown as follows3.1 General Prompt Design
105
You are an expert tasked with generating answers from input queries. You
have been provided with a query and corresponding context information,
organized in a table where each row represents a useful record. Please
generate an answer using only this context information. Ensure that you
provide the answer in your own words.
Context information: {∗table∗}
Query: {∗query∗}
When dealing with real-world problems, we often have prior knowledge and additional
information about the problems that help produce better answers. Considering such infor-
mation in prompting is generally helpful in improving the result.
• Paying attention to prompt formats. In general, the performance of LLMs is highly
sensitive to the prompts we input. Sometimes a small modiﬁcation to a prompt can lead to a
big change in model output. An interesting example is that changing the order of sentences
in a prompt may cause LLMs to generate different results. To make prompts easy to read
and reduce ambiguity, it is common to format them in a way that ensures clarity. One
example is that we deﬁne several ﬁelds for prompts and ﬁll different information in each
ﬁeld. Another example is we can use code-style prompts for LLMs which can understand
and generate both natural language and code. See the following for a code-style prompt that
performs translation where one demonstration is presented.
[English] = [I have an apple.]
[German] = [Ich habe einen Apfel.]
[English] = [I have an orange.]
[German] =
LLMs can receive text in various formats. This allows us to use control characters, XML
tags, and speciﬁc formatting to represent complex data. And it is useful to specify how the
input and output should be formatted or structured. For example, we can delimit sections of
text using quotes and prompt LLMs accordingly (e.g., adding a sentence like “the input text
is delimited by double quotes” to the prompt).
Above, we have discussed only a few strategies for writing good prompts. There are, of course,
many such methods, and one needs to develop their own through practice. Interested readers can
refer to various online documents for more information, such as OpenAI’s manual on the GPT
series models2 .
2
See
https://platform.openai.com/docs/guides/prompt-engineering/
six-strategies-for-getting-better-results.106
3.1.4
Prompting
More Examples
In this subsection, we consider more examples of prompting LLMs to perform various NLP tasks.
The motivation here is not to give standard prompts for these tasks, but rather to use simple
examples to illustrate how LLMs can be prompted to deal with NLP problems.
3.1.4.1
Text Classiﬁcation
Text classiﬁcation is perhaps one of the most common problems in NLP. Many tasks can be
broadly categorized as assigning pre-deﬁned labels to a given text. Here we consider the polarity
classiﬁcation problem in sentiment analysis. We choose polarity classiﬁcation for illustration be-
cause it is one of the most popular and well-deﬁned text classiﬁcation tasks. In a general setup of
polarity classiﬁcation, we are required to categorize a given text into one of three categories: neg-
ative, positive, or neutral. Below is a simple prompt for doing this (for easy reading, we highlight
the task description in the prompt).
Analyze the polarity of the following text and classify it as positive, negative, or
neutral.
Text:
The service at the restaurant was slower than expected, which was a bit frustrat-
ing.
The polarity of the text can be classiﬁed as negative.
To make the example complete, we show the response generated by the LLM (underlined text).
Although the answer is correct, the LLM gives this answer not in labels but in text describing
the result. The problem is that LLMs are designed to generate text but not to assign labels to text
and treat classiﬁcation problems as text generation problems. As a result, we need another system
to map the LLM’s output to the label space (call it label mapping), that is, we extract “negative”
from “The polarity of the text can be classiﬁed as negative”. This is trivial in most cases because
we can identify label words via simple heuristics. But occasionally, LLMs may not express the
classiﬁcation results using these label words. In this case, the problem becomes more complicated,
as we need some way to map the generated text or words to predeﬁned label words.
One method to induce output labels from LLMs is to reframe the problem as a cloze task. For
example, the following shows a cloze-like prompt for polarity classiﬁcation.
Analyze the polarity of the following text and classify it as positive, negative, or
neutral.
Text:
The service at the restaurant was slower than expected, which was a bit frustrat-
ing.
The polarity of the text is negative3.1 General Prompt Design
107
We can use LLMs to complete the text and ﬁll the blank with the most appropriate word. Ide-
ally, we wish the ﬁlled word would be positive, negative, or neutral. However, LLMs are not
guaranteed to generate these label words. One method to address this problem is to constrain the
prediction to the set of label words and select the one with the highest probability. Then, the output
label is given by
label = arg max Pr(y|x)
(3.1)
y∈Y
where y denotes the word ﬁlled in the blank, and Y denotes the set of label words
{positive, negative, neutral}.
Another method of using LLMs to generate labels is to constrain the output with prompts. For
example, we can prompt LLMs to predict within a controlled set of words. Here is an example.
Analyze the polarity of the following text and classify it as positive, negative, or
neutral.
Text:
The service at the restaurant was slower than expected, which was a bit frustrat-
ing.
What is the polarity of the text?
Just answer: positive, negative, or neutral.
Negative
Sentiment analysis is a common NLP problem that has probably been well understood by
LLMs through pre-training or ﬁne-tuning. Thus we can prompt LLMs using simple instructions
to perform the task. However, for new classiﬁcation problems, it may be necessary to provide
additional details about the task, such as the classiﬁcation standards, so that the LLMs can perform
correctly. To do this, we can add a more detailed description of the task and/or demonstrate
classiﬁcation examples in the prompts. To illustrate, consider the following example.108
Prompting
Analyze the polarity of the following text and classify it as positive, negative, or
neutral. Here’s what each category represents:
Positive: This indicates that the text conveys a positive emotion or attitude. For
example, texts expressing happiness, satisfaction, excitement, or admiration are
considered positive.
Negative: This refers to a text that expresses a negative emotion or attitude. It
encompasses feelings of sadness, anger, frustration, or criticism.
Neutral: Neutral sentiment is used to describe texts that do not exhibit clear posi-
tive or negative emotions but instead convey informational, factual, or indifferent
tones.
Text:
The service at the restaurant was slower than expected, which was a bit frustrat-
ing.
What is the polarity of the text?
Negative
While it seems straightforward to use LLMs for classiﬁcation problems, there are still issues
that have not been well addressed. For example, when dealing with a large number of categories,
it remains challenging to effectively prompt LLMs. Note that if we face a very difﬁcult classiﬁca-
tion problem and have a certain amount of labeled data, ﬁne-tuning LLMs or adopting “BERT +
classiﬁer”-like architectures is also desirable.
3.1.4.2
Information Extraction
Many NLP problems can be regarded as information extraction problems, involving the identiﬁ-
cation or extraction of speciﬁc pieces of information from unstructured text. This information can
include named entities, relationships, events, and other relevant data points. The goal of infor-
mation extraction is to transform raw data into a format that can be easily analyzed and used in
various downstream applications.
As information extraction covers a wide range of problems, we cannot discuss them all here.
Instead, we start with the task of named entity recognition — a task that has long been a concern
in NLP. Named entity recognition is a process that detects and classiﬁes key information in text
into speciﬁc groups. These key pieces of information, known as named entities, typically include
proper names and are categorized into distinct classes such as people, locations, organizations,
dates, monetary values, and percentages. Consider ﬁrst a simple example of extracting person
names from a text3 .
3
The text is from https://edition.cnn.com/travel3.1 General Prompt Design
109
Identify all person names in the provided text.
Text:
Is the UK really doing that badly or have travel trends shifted, post-pandemic?
For Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I
think the UK is doing perfectly well but we’ll see more people going to Europe,”
he says of 2024, adding that there’s “allegedly a slight plateau of American de-
mand for the UK.”
The person name identiﬁed in the provided text is: Tom Jenkins
Then, we can extract all named entities.
Identify and classify all named entities in the provided text into categories such
as person names, locations, dates, and organizations. List each entity with its type
on one line.
Text:
Is the UK really doing that badly or have travel trends shifted, post-pandemic?
For Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I
think the UK is doing perfectly well but we’ll see more people going to Europe,”
he says of 2024, adding that there’s “allegedly a slight plateau of American de-
mand for the UK.”
1. Tom Jenkins - Person Name
2. European Tourism Organisation - Organization
3. UK - Location
4. Europe - Location
5. 2024 - Date
Given these identiﬁed named entities, we can further process the text using other information
extraction tasks. For example, we can identify the relationships between these named entities (call
it relation extraction). The corresponding prompt is shown as follows.110
Prompting
Given a text and a list of named entities identiﬁed within it, analyze and describe
the relationships between these entities. Explain how each entity is contextually
related to others.
Text:
Is the UK really doing that badly or have travel trends shifted, post-pandemic?
For Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I
think the UK is doing perfectly well but we’ll see more people going to Europe,”
he says of 2024, adding that there’s “allegedly a slight plateau of American de-
mand for the UK.”
Named Entity:
Tom Jenkins, European Tourism Organisation, UK, Europe, 2024
Tom Jenkins - European Tourism Organisation: Tom Jenkins is the CEO of the
European Tourism Organisation. This indicates a professional association where
Tom Jenkins likely has a signiﬁcant role in directing the organization’s activities,
especially those related to tourism in Europe.
...
If LLMs have been ﬁne-tuned with instruction following for information extraction tasks, it
is relatively easy to perform various information extraction tasks. For example, the following is a
prompt template for information extraction.
You will be provided with a text. Your task is to {∗task-description∗}
Text: {∗text∗}
Here task-description is the instruction for the task. Examples of the instruction include
extract the keywords from the text.
extract the key events from the text.
detect the coreference and anaphoric links between entities from the text.
and so on. Of course, we can provide a more detailed description of the task to prompt LLMs
more accurately.
3.1.4.3
Text Generation
In NLP, the term text generation is used broadly to cover a wide range of different problems, many
of which focus on generating text based on given text. These problems can be roughly categorized
into two classes: 1) continual writing based on the input text (call it text completion), and 2)
transformation of the input text into another text (call it text transformation). Here, we illustrate
the prompts for the former and consider the prompts for the latter in the following discussion.3.1 General Prompt Design
111
A simple example of text completion is continuing to write a story starting with a few given
sentences.
You are a writer who is great at writing stories. You are provided with a few
sentences to start with. Please continue the story from the provided sentences.
Start of the story:
One sunny afternoon, Lily discovered a tiny, golden key hidden under a rock in
her backyard. Curious, she picked it up and wondered what it might unlock. Just
then, her cat, Whiskers, began to meow excitedly, leading her towards the old
oak tree at the edge of the garden.
Lily followed Whiskers, her heart pounding with excitement and a touch of
nervousness. The old oak tree had always seemed mysterious to her, its gnarled
branches casting long shadows even on the brightest days. As they reached the
tree, Whiskers darted ahead and started pawing at something buried beneath a
pile of autumn leaves ...
Likewise, we can create a conversation given the conversational context.
You are provided with a conversation between a tourist and a taxi driver. Please
complete the conversation.
Tourist: Could you take me to the downtown museum, please?
Taxi Driver: Of course! First time in the city?
Tourist: Yes, any must-see places around?
Taxi Driver: Deﬁnitely check out the botanical gardens after the museum.
They’re nearby and quite beautiful.
Tourist: Great, thanks for the tip!
Taxi Driver: No problem. Enjoy your visit!
Another application of text completion is generating text based on user requirements. The
requirements can include style, tone, length, and any speciﬁc content that the text should contain.
For example, users may need a formal report, a creative story, or a piece of programming code.
Here is a prompt for generating a ﬁve-character regulated Chinese poem.112
Prompting
You are a poet. Please write a traditional Chinese poem. Here is the requirement.
Theme: Spring’s rejuvenation
Structure: Five-character regulated poem
Emotion: Convey a sense of happiness and renewal through vivid spring imagery.
春意盎然
翠柳风中舞
红花雨后新。
山明水又绿
天宽鸟自频。
If the LLM is trained to generate language and code, we can prompt it to perform code com-
pletion tasks. Here is an example.
Please write a Python function to calculate the average of a list of numbers.
def calculate_average(numbers):
if numbers: # Check if the list is not empty
return sum(numbers) / len(numbers)
else:
return 0 # Return 0 if the list is empty
3.1.4.4
Text Transformation
Typical text transformation tasks include machine translation, summarization, text style transfer,
and so on. The prompts used to perform these tasks are not signiﬁcantly different from the ones
presented previously. Here, we present a few examples to showcase these prompts.
Translate the following text from English to Spanish.
Text:
The quick brown fox jumps over the lazy dog.
El rápido zorro marrón salta sobre el perro perezoso.3.1 General Prompt Design
113
Summarize the following article in no more than 50 words:
Article:
In recent years, urban areas have been facing signiﬁcant challenges related to sus-
tainability and environmental management. Cities are experiencing higher rates
of pollution, increased trafﬁc congestion, and greater demands on infrastructure.
This growth has led to numerous environmental issues, including elevated lev-
els of air and water pollution, increased waste production, and strained public
services ...
Urban areas are grappling with sustainability challenges, such as rising pollution,
trafﬁc congestion, and infrastructure demands ...
Rewrite this text in a formal tone.Contato
(19) 99635-1395 (Work)
www.linkedin.com/in/rodolfo-
rodrigues-980319345 (LinkedIn)
github.com/devrodts (Personal)
www.hackerrank.com/profile/
dev_rodts (Personal)
www.atous.com.br (Other)
Principais competências
Rust (Linguagem de programação)
Internet das Coisas
C#
Languages
Francês (Limited Working)
Português (Native or Bilingual)
Espanhol (Limited Working)
English (Full Professional)
Certifications
React Developer Certified
Next.Js
C# Basics Certified
JQuery
Typescript
Rodolfo Rodrigues
Software Developer | Web3 | Security | Java | Typescript | Javascript
| Python | Um dos Guadiões da Era Digital
Campinas, São Paulo, Brasil
Resumo
Full Stack Sofware Developer with 6 years of hands on experience,
and 12 years at the technology industry.
Java 8/11/17/21/24 - Spring Boot
Blockchain - P2P
Spark
Typescript - Javascript(ES6+) - Node.Js-Nest.Js-Express.Js -
Handlebars.Js
MySQL - NoSQL- PostgreSQL -
Docker
React - Next - React Native
Bootstrap - TailwindCSS - Styled-Components
Jest - Junit5 - Mockito
Python - Flask
AWS Cloud | S3 | IAM | Lambda | EC2 | RDS | DynamoDB
Azure Functions
Google Cloud Platform
Firebase
I create a new github its year to show my skills:
https://github.com/devrodts
https://github.com/developedbyrod
https://www.hackerrank.com/profile/dev_rodts
Experiência
Orch-Mind
Co-founder
junho de 2025 - Present (1 mês)
The New AI paradigm
Atous Technology Systems
Fundador
abril de 2025 - Present (3 meses)
Page 1 of 3Atous Security Protocol will be the future of our communication systems.
Blackcorporate
Tech lead | Senior Full Stack Developer | Scrum Master
setembro de 2024 - fevereiro de 2025 (6 meses)
Mitra
Senior Analyst Developer
junho de 2024 - outubro de 2024 (5 meses)
Lux Performance
Software Developer
abril de 2023 - setembro de 2024 (1 ano 6 meses)
CredAluga
Front End Developer
novembro de 2023 - março de 2024 (5 meses)
PagBank
Front End Developer | Full Stack Developer
março de 2022 - setembro de 2023 (1 ano 7 meses)
Banco Carrefour
Software Developer
outubro de 2021 - março de 2022 (6 meses)
Vitabe Group
Full Stack Developer
dezembro de 2020 - agosto de 2021 (9 meses)
Dev Codar
Full Stack Developer
agosto de 2019 - junho de 2021 (1 ano 11 meses)
Formação acadêmica
Senac São Paulo
Sistemas para internet, Tecnologia da Informação · (janeiro de 2021)
Page 2 of 3Instituto Federal de Educação, Ciência e Tecnologia de São Paulo -
IFSP
Ensino Médio, ENEM · (novembro de 2016)
Anhanguera Educacional
Análise e Desenvolvimento de Sistemas, Tecnologia da Informação · (janeiro
de 2017 - dezembro de 2017)
Page 3 of 3
Text:
Hey, what’s up? Long time no see!
Hello, how have you been? It has been quite some time since we last met!
3.1.4.5
Question Answering
The question-answering format is inherently simple. For a given question, there is an answer that
corresponds to it. For example, in open-domain question answering, we expect the system to
return an answer in response to a user-submitted question. Prompt templates for general-purpose
question answering can be:
{∗question∗}
Question: {∗question∗}
Answer:
Question answering is important in NLP because many problems can be framed as question-
answering tasks. In particular, many recent reasoning tasks are deﬁned in the form of question
answering. For example, in the MMLU benchmark [Hendrycks et al., 2021], each example con-
sists of a multiple-choice question, and LLMs are required to select the correct answer. See the
following for an example prompt for answering a question in this dataset.114
Prompting
(Z, ∗) is a group with a ∗ b = a + b + 1 for all a, b in Z. The inverse of a is
(A) 0
(B) −2
(C) a − 2
(D) (2 + a) ∗ −1
D
Another widely-used benchmark is the GSM8K dataset [Cobbe et al., 2021]. It consists of
thousands of grade school math word problems. For each problem, we use LLMs to return a
solution in natural language. For example, the following is a prompt used for solving a problem
in GSM8K.
DEMO
Q: Jerry’s two daughters play softball on different teams. They each have
8 games this season. Each team practices 4 hours for every game they
play. If each game lasts for 2 hours, how many hours will Jerry spend at
the ﬁeld watching his daughters play and practice altogether?
A: Jerry will spend 8 games * 2 hours per game = ≪ 8 ∗ 2 = 16 ≫ 16
hours watching one daughter play her games. He will spend 16 ∗ 2 =≪
16 ∗ 2 = 32 ≫ 32 hours watching both daughters play their games. He
will spend 8 games * 4 hours of practice = ≪ 8 ∗ 4 = 32 ≫ 32 hours
watching one daughter practice. He will spend 32∗2 =≪ 32∗2 = 64 ≫
64 hours watching both daughters practice. He will spend a total of 32
hours watching games + 64 hours watching practice =≪ 32 + 64 =
96 ≫ 96 hours. #### 96
Q: Mary bought six apples from the store. From the apples she bought,
for each that Mary ate, she planted two trees from the remaining ones.
How many apples did Mary eat?
A: She planted eight trees. This means she used half of that amount of
apples, which is 8 trees ◁2 trees/apple =≪ 8◁2 = 4 ≫ 4 apples. That
means that she planted four of the six apples she bought, leaving only 6
apples − 4 apples =≪ 6 − 4 = 2 ≫ 2 apples to be eaten. #### 2
USER
Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. If Boris
gives Beck 10 apples, how many fewer apples does Beck have than Boris
now?
A: Boris starts with 100 apples and gives Beck 10 apples, leaving
him with 100 apples −10 apples =≪ 100 − 10 = 90 ≫ 90 apples.
Initially, Beck has 23 fewer apples than Boris, giving him a
starting count of 100 − 23 =≪ 100 − 23 = 77 ≫ 77 apples. After
receiving 10 apples from Boris, Beck’s total comes to 77 apples +10
apples =≪ 77 + 10 = 87 ≫ 87 apples. Consequently, Beck now has
90 − 87 =≪ 90 − 87 = 3 ≫ 3 fewer apples than Boris. #### 33.2 Advanced Prompting Methods
115
Here a few-shot prompt is adopted. The LLM learns from these demonstrations of problem-
solution pairs not only the way of problem-solving but also the way of formatting the output. For
example, the ﬁnal result of calculation follows the #### token, and ≪ ▷▷▷ ≫ annotates the detailed
calculation steps (called calculation annotation)4 .
3.2
Advanced Prompting Methods
So far in this chapter, we have introduced the basic concepts related to LLM prompting and pre-
sented a number of prompts for NLP tasks. We now consider several techniques for enhancing the
effectiveness of prompting.
3.2.1
Chain of Thought
We have encountered the concept of chain of thought (CoT) several times in this chapter and
previous ones [Wei et al., 2022c; Chowdhery et al., 2022]. CoT methods provide a simple way
to prompt LLMs to generate step-by-step reasoning for complex problems, thereby approaching
tasks in a more human-like manner. Rather than coming to a conclusion directly, the CoT methods
instruct LLMs to generate reasoning steps or to learn from demonstrations of detailed reasoning
processes provided in the prompts. To illustrate CoT, we consider the problem of algebraic calcu-
lation, as commonly described in the literature. Suppose we are given an algebraic problem
Calculate the average of the numbers 2, 4, and 6.
We can consider it as the question and prompt an LLM to answer it.
Q: Please calculate the average of the numbers 2, 4, and 9.
A: The answer is 6.
It seems difﬁcult for the LLM to directly give a correct answer. A simple improvement is to
add demonstrations of similar problems in the prompt, and thus the LLM can learn from these
demonstrations.
Q: Please calculate the average of the numbers 1, 3, 5, and 7.
A: The answer is 4.
Q: Please calculate the average of the numbers 2, 4, and 9.
A: The answer is 7.
The problem here is that, although we have shown a similar question-answer pair, it remains
difﬁcult for the LLM to reason out the correct answer. In CoT, not only can LLMs learn from the
4
During prediction, a calculator is used when we see ≪ ▷▷▷ ≫. More speciﬁcally, once the LLM encounters “=”
in a ≪ ▷▷▷ ≫, then the calculator calculates the expression on the left-hand side of “=”. This method helps reduce the
calculation errors made by LLMs.116
Prompting
correspondence between questions and answers but they may gain more from detailed problem-
solving steps that are used to derive the answers. To do this, we can incorporate some reasoning
steps into the prompt to obtain a CoT prompt.
Q: Please calculate the mean square of the numbers 1, 3, 5, and 7.
A: Calculate the square of each number: 12 = 1, 32 = 9, 52 = 25, and 72 = 49.
Sum the squares, 1 + 9 + 25 + 49 = 84. There are 4 numbers in total. Divide
the sum by the number of items, 84◁4 = 21. The answer is 21.
Q: Please calculate the average of the numbers 2, 4, and 9.
A: Calculate 2 + 4 + 9, which equals 15. There are three numbers. Divide the
total sum by the count, resulting in 15◁3 = 5. The answer is 5.
Here we highlight the reasoning steps in green. By providing a detailed reasoning process for
a similar problem, the LLM learns to reason and thereby generates a problem-solving path that
leads to the correct answer.
There are several beneﬁts of using CoT prompting. First, CoT allows LLMs to decom-
pose complex problems into smaller, sequential reasoning steps. This somewhat mirrors human
problem-solving behaviors, making it particularly effective for tasks requiring detailed, multi-step
reasoning. Second, CoT makes the reasoning process more transparent and interpretable. Since all
reasoning steps are visible, we can understand and interpret how a conclusion was reached. Third,
if users can follow the logic behind the reasoning process, they will be more likely to trust the pre-
dictions of an LLM. This is particularly important when applying LLMs in ﬁelds like medicine,
education, and ﬁnance. Fourth, CoT is an in-context learning approach, and thus, it is applicable
to most well-trained, off-the-shelf LLMs. Moreover, CoT provides efﬁcient ways to adapt LLMs
to different types of problems. It can even inspire more creative solutions by exploring various
alternative reasoning paths, which might not be obvious when arriving at a conclusion directly.
The method described above requires providing one or more examples of CoT reasoning, typ-
ically called the few-shot CoT method. By contrast, the zero-shot CoT method does not require
such examples. It instead prompts LLMs to reason step-by-step by incorporating speciﬁc instruc-
tions in prompts. For example, below is a zero-shot CoT prompt.
Q: Please calculate the average of the numbers 2, 4, and 9.
A: Let’s think step-by-step.
We have three numbers: 2, 4, and 9.
Add these numbers together,
2 + 4 + 9 = 15. Determine how many numbers there are, which in this case
is three. The average is calculated by dividing the total sum by the number of
elements. Completing the division gives 15◁3 = 5. So the answer is 5.
Following the instruction “Let’s think step by step”, the LLM is prompted to generate detailed
reasoning steps. As discussed in Kojima et al. [2022]’s work, prompting with such instructions
may result in LLMs generating only the reasoning steps without a clear conclusion. In this case,
a second round of prompting can be used to extract the answer from these reasoning steps. For
example, Kojima et al. [2022] create a second prompt which combines both the input and output3.2 Advanced Prompting Methods
117
in the ﬁrst round of prompting. Using this combined input, the LLM can continue its reasoning
process and then generate the correct answer. Furthermore, it is possible to prompt LLMs to
reason using instructions other than “Let’s think step by step”, such as “Let’s think logically” and
“Please show me your thinking steps ﬁrst”.
While we have illustrated CoT methods using an algebraic reasoning problem, these methods
can be applied to a variety of different problems. Typical problem-solving scenarios for CoT
include mathematical reasoning, logical reasoning, commonsense reasoning, symbolic reasoning,
code generation, and so on. See Figure 3.1 for more examples of applying CoT in various tasks.
CoT today is one of the most active ﬁelds of prompt engineering. This has not only led to
improved performance for LLM prompting but has opened the door to a wide range of methods
for studying and verifying reasoning capabilities of LLMs. Although we have focused on the
basic idea of CoT in this section, it can be improved in several ways. For example, we can
consider the reasoning process as a problem of searching through many possible paths, each of
which may consist of multiple intermediate states (i.e., reasoning steps). In general, we wish the
search space to be well-deﬁned and sufﬁciently large, so that we are more likely to ﬁnd the optimal
result. For this reason, an area of current LLM research is aimed at designing better structures for
representing reasoning processes, allowing LLMs to tackle more complex reasoning challenges.
These structures include tree-based structures [Yao et al., 2024], graph-based structures [Besta
et al., 2024], and so on. By using these compact representations of reasoning paths, LLMs can
explore a wider range of decision-making paths, analogous to System 2 thinking5 . Another line of
research focuses on prompting LLMs with multi-round interactions. This involves decomposing
complex problems into sub-problems, verifying and reﬁning model outputs, employing model
ensembling, and so on. Note that these methods and the issues involved are not limited to CoT. In
fact, they are often used as more general approaches to improving LLMs, while CoT can be seen
as a way to test the capabilities of LLMs. We will see discussions of some of these issues in the
following subsections.
Before leaving our discussion of CoT, we should consider its practical limitations. One of
them is the need for detailed, multi-step reasoning demonstrations in few-shot CoT scenarios,
which may be difﬁcult to obtain, either automatically or manually. Also, there is no standard
method for breaking down complex problems into simpler problem-solving steps. This often
heavily depends on the user’s experience. In addition, errors in intermediate steps can also affect
the accuracy of the ﬁnal conclusion. For further discussion on the pros and cons of CoT, the
interested reader can refer to recent surveys on this topic [Chu et al., 2023; Yu et al., 2023; Zhang
et al., 2023a].
3.2.2
Problem Decomposition
We have seen that LLMs can beneﬁt from solving a complex problem by breaking it down into
simpler problem-solving tasks. Such an approach can be seen as an example of a broader paradigm
known as problem decomposition, which has been extensively explored and discussed in psy-
chology and computer science. From the psychological perspective, complex problem-solving
refers to a process of addressing a problem using knowledge that helps overcome the barriers of
5
System 1 and System 2 thinking, as described by Kahneman [2011], represent two different modes of cognitive
processing. System 1 is fast, automatic, intuitive, and emotional. This mode of thinking operates effortlessly and
quickly, and is often what guides our daily decisions, judgments, and impressions. System 2 is slow, deliberate, and
analytical. It is activated when we need to perform complex computations.118
Prompting
Q: What home entertainment equipment re-
quires cable?
Answer Choices: (a) radio shack (b) substa-
tion (c) television (d) cabinet
A: The answer must require cable. Of the
above choices, only television requires cable.
So the answer is (c).
Q: What type of person typically contracts
illness?
Answer Choices: (a) hospital (b) head (c)
sick person (d) elderly person (e) doctor’s of-
ﬁce
A: The answer must be a person suscepti-
ble to illness. Of the choices, "elderly per
son" is correct because they typically have
weaker immune systems.
So the answer is (d).
Q: Do hamsters provide food for any ani-
mals?
A: Hamsters are prey animals. Prey are food
for predators. Thus, hamsters provide food
for some animals. So the answer is yes.
Q: Can you use Microsoft Ofﬁce without in-
ternet?
A: Yes, you can use Microsoft Ofﬁce appli-
cations , although features like cloud stor-
age and online collaboration require inter-
net. So the answer is yes.
(a) CSQA(b) StrategyQA
Q: Complete the rest of the sequence, mak-
ing sure that the parentheses are closed prop-
erly. Input: [ {
A: Let’s think step by step.
0: empty stack
1: [ ; stack: [
2: { ; stack: [ {
So the answer is } ].Q: Take the last letters of the words in "Elon
Musk" and concatenate them.
A: The last letter of "Elon" is "n". The last
letter of "Musk" is "k". Concatenating them
is "nk". The answer is nk.
Q: Complete the rest of the sequence, mak-
ing sure that the parentheses are closed prop-
erly. Input: < [ [
A: Let’s think step by step.
0: empty stack
1: < ; stack: <
2: [ ; stack: < [
3: [ ; stack: < [ [
So the answer is ] ] >.
(c) Dyck languages
Q: Take the last letters of each word in "Re-
nata Mariela Mona Kristin" and concatenate
them.
A: The last letter of "Renata" is "a". The
last letter of "Mariela" is "a". The last let-
ter of "Mona" is "a". The last letter of
"Kristin" is "n". Concatenating them is
"aaan". The answer is aaan.
(d) Last Letter Concatenation
Fig. 3.1: CoT in four different reasoning tasks, including CSQA, StrategyQA, Dyck languages, and Last Letter Con-
catenation. The CoT parts are highlighted in green.
the problem6 . There are generally no standard or clear paths to a solution for a complex prob-
lem. However, it is often advantageous to employ strategies that decompose the problem, thereby
making it easier to tackle the corresponding sub-problems with less effort. For example, consider
writing a blog about the risks of AI. If we simply prompt an LLM with the instruction “Please
write a blog about the risks of AI”, the LLM may generate a blog with arbitrary structures and
6
A relatively formal deﬁnition can be found in Frensch and Funke [2014]’s book: complex problem-solving occurs
to overcome barriers between a given state and a desired goal state by means of behavioral and/or cognitive, multi-step
activities.3.2 Advanced Prompting Methods
119
writing styles. A better method, instead, could be to outline the blog and provide more detailed
information about each section. Consider the following prompt
