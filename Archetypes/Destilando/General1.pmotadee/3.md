You are a blog writer. Please follow the provided outline below to write a blog
about the risks of AI.
• Introduction
Introduce AI, its relevance, and the importance of understanding its risks for youth.
• Privacy Concerns
Discuss how AI might compromise personal privacy through interactions online.
• Misinformation
Explore AI’s role in spreading misinformation and inﬂuencing young people’s deci-
sions.
• Cyberbullying
Highlight how AI tools can be utilized in cyberbullying and the impact on mental
health.
• Tips for Safe AI Use
Offer guidelines for responsible AI usage and promote critical thinking.
• Conclusion
Recap main points and encourage proactive engagement with AI ethics.
Here we give the title and major points for each section. Then, the LLM can use this structure to
break down the writing task by ﬁlling in content for these sections. Note that the way to structure
the blog can be provided by humans or even generated automatically. For example, we can use
the LLM to ﬁrst generate the outline, and then ask it to follow this outline to complete the writing.
In computer science, decomposing complex problems is a commonly used strategy in software
and hardware system design. A well-known example is the divide-and-conquer paradigm, which
is often used to design algorithms for computation problems that can be reduced to simpler, more
manageable problems. For example, consider a problem of determining whether a document
discusses the risks of AI. We can instruct the LLM with the following prompt.
You are provided with a text. Please determine whether it discusses the risks of
AI.
{∗document∗}
If the document is long, the computation will be expensive. Alternatively, we can divide
the document into relatively short segments and perform the same task on each segment. These
segments can be processed in parallel to further reduce the computational cost. Next, we determine120
Prompting
the relevancy of each segment to the topic of AI risks. The ﬁnal output is then generated using
another prompt.
Your task is to determine whether a text discusses the risks of AI. This text has
been divided into segments, and you have obtained the relevancy of each segment
to the topic of AI risks. Based on this, please provide your ﬁnal result.
Segment 1: {∗relevancy-to-the-topic1∗}
Segment 2: {∗relevancy-to-the-topic2∗}
Segment 3: {∗relevancy-to-the-topic3∗}
...
Now let us return to a more general discussion of problem decomposition in prompting. While
problem decomposition can be applied to various NLP problems, it has been more extensively
discussed and tested in reasoning tasks recently. For complex reasoning tasks, we often need
a multi-step reasoning path to reach a correct conclusion. We can use LLMs to achieve this in
three different ways. First, LLMs can directly reach the conclusion. In other words, they can
predict without explicit reasoning processes, and there is a hidden and uninterpretable reasoning
mechanism. Second, LLMs are prompted to generate a multi-step reasoning path that leads to the
conclusion, like CoT. However, we run LLMs just once, and all intermediate steps in reasoning
are generated in a single prediction. Third, we break down the original problem into a number of
sub-problems, which are either addressed in separate runs of LLMs or tackled using other systems.
Here we focus our attention on the third approach, which is closely related to problem decompo-
sition. Note, however, that a more comprehensive discussion could cover all these approaches,
while the ﬁrst two have been discussed to some extent in this chapter.
A general framework for problem decomposition involves two elements.
• Sub-problem Generation. This involves decomposing the input problem into a number of
sub-problems.
• Sub-problem Solving. This involves solving each sub-problem and deriving intermediate
and ﬁnal conclusions through reasoning.
These two issues can be modeled in different ways, leading to various problem decomposition
methods. One approach is to treat them as separate steps in a two-step process. For example,
consider the blog writing task described at the beginning of this subsection. In the ﬁrst step, we
decompose the entire problem into sub-problems all at once (i.e., outline the blog). In the second
step, we solve the sub-problems either sequentially or in another order (i.e., ﬁll in content for
each section as needed). The ﬁnal output of this process combines the results from solving each
sub-problem. While this method is simple and straightforward, it assumes that the problem is
compositional, making it more suitable for tasks like writing and code generation.
However, many real-world problems require complex reasoning. One key characteristic of
these problems is that the reasoning steps may not be ﬁxed. The reasoning path can vary for
different problems, and each step of reasoning may depend on the outcomes of prior steps. In3.2 Advanced Prompting Methods
121
such cases, it is undesirable to use ﬁxed sub-problem generation in advance. Instead, sub-problems
should be generated dynamically based on the input problem, and, if possible, generated on the
ﬂy during the reasoning process. This makes problem decomposition more challenging compared
with designing divide-and-conquer algorithms. Ideally, we would like to jointly design both the
systems for sub-problem generation and sub-problem solving. But a more practical and widely
used approach is to adopt separate models for these tasks. A straightforward way to achieve this
is to adapt an LLM for these tasks by either prompting or tuning the model.
Here we consider a method based on the above idea, called least-to-most prompting [Zhou
et al., 2023b]. The motivation for this method arises from the challenges of solving difﬁcult rea-
soning problems — those that cannot be addressed by simply generalizing from a few examples.
For these problems, a more effective problem-solving strategy is to follow a progressive sequence
of sub-problems that systematically lead to the conclusion. More speciﬁcally, in the least-to-most
prompting method, sub-problem generation is performed by prompting an LLM with instructions
and/or demonstrations. For example, below is a 2-shot prompt for sub-problem generation in
least-to-most prompting.
TASKYour task is to decompose a problem into several sub-problems. You will
be given a few examples to illustrate how to achieve this.
DEMOQ: In a community, 5% of the population are infants, 15% are children,
40% are adults, and 40% are seniors. Which group makes up the largest
portion of the population?
A: To answer the question “Which group makes up the largest portion of the
population?”, we need to know: “How many percent are infants?”, “How
many percent are children?”, “How many percent are adults?”, “How many
percent are seniors?”.
Q: Alice, Bob, and Charlie brought beads for their group project in their
craft class. Alice has twice as many beads as Bob, and Bob has ﬁve times
as many beads as Charlie. If Charlie has 6 beads, how many beads can they
use for their craft project?
A: To answer the question “How many beads can they use for their craft
project?”, we need to know: “How many beads does Bob have?”, “How
many beads does Alice have?”.
USER
Q: The environmental study conducted from 2015 to 2020 revealed that the
average temperature in the region increased by 2.3 degrees Celsius. What
was the duration of the environmental study?
A: To answer the question “What was the duration of the environmental
study?”, we need to know: “When did the environmental study start?”,
“When did the environmental study end?”.
By learning from the examples, the LLM can generate two sub-problems for answering the
new problem “What was the duration of the environmental study?” (highlighted in blue and
orange). Given these sub-problems, we solve them sequentially. For each sub-problem, we take
all previously-generated QA pairs as context, and then produce the answer. For the example above,122
Prompting
we need to answer the ﬁrst sub-problem by prompting the LLM, like this
The environmental study conducted from 2015 to 2020 revealed that
the average temperature in the region increased by 2.3 degrees Celsius.
SUB-PROB1
Q: When did the environmental study start?
A: The environmental study started in 2015.
Once we have the answer to the ﬁrst sub-problem, we proceed to the second one. This time,
we include both the ﬁrst sub-problem and its corresponding answer in the input.
The environmental study conducted from 2015 to 2020 revealed that
the average temperature in the region increased by 2.3 degrees Celsius.
SUB-PROB1
Q: When did the environmental study start?
A: The environmental study started in 2015.
SUB-PROB2
Q: When did the environmental study end?
A: The environmental study ended in 2020.
Finally, we use the LLM to solve the original problem given the answers to all the sub-
problems.
The environmental study conducted from 2015 to 2020 revealed that
the average temperature in the region increased by 2.3 degrees Celsius.
SUB-PROB1
Q: When did the environmental study start?
A: The environmental study started in 2015.
SUB-PROB2
Q: When did the environmental study end?
A: The environmental study ended in 2020.
FINAL
Q: What was the duration of the environmental study?
A: The duration of the environmental study was 5 years.
The least-to-most method offers a basic approach to prompting LLMs to generate and solve
sub-problems separately. We can improve it in several ways. One simple improvement is to apply
various advanced prompting techniques, which do not require changes to the problem decom-
position framework. For example, we can incorporate CoT into the prompting to enhance the
reasoning performance of sub-problem generation and solving.
Another improvement is to explore methods for better decomposing problems and organizing
problem-solving paths. To describe these approaches, we will use the symbol p0 to denote the3.2 Advanced Prompting Methods
123
input problem, and use the symbols {p1 , ▷▷▷, pn } to denote the sub-problems corresponding to p0 .
For least-to-most prompting, we decompose p0 into {p1 , ▷▷▷, pn }, given by
{p1 , ▷▷▷, pn } = G(p0 )
(3.2)
where G(·) denotes the function of sub-problem generation. Then, we solve the sub-problems
{p1 , ▷▷▷, pn } sequentially, resulting in a sequence of answers {a1 , ▷▷▷, an }. For answering the i-th
sub-problem pi , we include both the original problem p0 and all previously-seen problem-answer
pairs in the context for prediction. The answer ai is given by
ai = Si (pi , {p0 , p<i , a<i })
(3.3)
where p<i = {p1 , ▷▷▷, pi−1 } and a<i = {a1 , ▷▷▷, ai−1 }. Si (·) denotes the function that solves the
sub-problem pi given the context {p0 , p<i , a<i }. The last step is to generate the answer to the
original problem p0 , which can be expressed in a similar manner to Eq. (3.3).
a0 = S0 (p0 , {p≤n , a≤n })
(3.4)
One way to reﬁne this model is to modify the G(·) function so that the model can dynamically
generate answers. Instead of generating all sub-problems at one time, we can generate each of
them during problem-solving [Dua et al., 2022]. To do this, we can replace Eq. (3.2) with
pi = Gi (p0 , {p<i , a<i })
(3.5)
Hence we obtain a sub-problem generation model that operates in a step-by-step manner. At each
step i, we ﬁrst generate the sub-problem pi by prompting an LLM with the original problem p0
and the problem-solving history {p<i , a<i }. We then generate the answer ai for this sub-problem
using the same or a different LLM, based on the same contextual information (see Eq. (3.3)). This
method effectively expands the reasoning capacity of LLMs by allowing them to dynamically
generate and solve sub-problems in intermediate reasoning steps. As a result, the reasoning paths
are not ﬁxed in advance, and the models can choose and adapt their reasoning strategies during
problem-solving.
Another way to improve the above model is to focus on developing better sub-problem solvers.
In our previous discussion, we restricted Si (·) to LLMs that are prompted to solve the sub-problem
pi . In fact, we can expand this function to any system that is capable of addressing the sub-
problem. For example, Si (·) could make calls to IR systems, thereby allowing us to access a
broader range of data for problem-solving. Another example is using Si (·) as a calculator to
accurately compute results in mathematical problem-solving. If the sub-problem pi is complex
and requires multiple intermediate problem-solving steps, it is also possible to further decompose
pi into smaller sub-problems. For example, Si (·) can be deﬁned as a recursive program that
generates and solves sub-problems. This incorporates recursion into problem-solving and allows
us to address problems by iteratively decomposing them. As a result, we can deﬁne a hierarchical
structure for problem-solving [Khot et al., 2023].
If we generalize the above formulation a bit further, we can consider it as a reinforcement
learning problem. A typical method is to model a problem-solving process as a decision making
process. In each step of this process, an action is taken based on the current state. These actions124
Prompting
can include all functions for sub-problem generation and solving (i.e., Gi (·) and Si (·)). Thus,
the action sequence corresponds to a problem-solving path. Since the discussion of reinforcement
learning problems is beyond the scope of this chapter, we skip the precise description of this
learning task. Nevertheless, developing an agent or controller to determine when and how to
generate and solve a sub-problem is also a natural choice.
In NLP, problem decomposition is related to a long line of research on multi-hop question
answering [Mavi et al., 2024]. This task requires the system to gather and combine information
from multiple pieces of text to provide an accurate answer to a complex question. For example,
to answer the question “What is the capital of the country where Albert Einstein was born?”, we
need to know “Where Albert Einstein was born?” and “What’s the capital of Germany?”. Earlier
work in this area and related ones has investigated the issue of problem decomposition, though the
methods might not be based on LLMs. For example, a popular method is to develop an additional
neural model to generate simpler questions that address different aspects of the original question
[Andreas et al., 2016; Talmor and Berant, 2018; Min et al., 2019]. This question generator can
create questions in a batch or sequential manner.
Broadly speaking, problem decomposition is also related to the compositionality issue in NLP
[Drozdov et al., 2022; Press et al., 2023]. For example, in semantic parsing, we map natural lan-
guage sentences into structured meaning representations by breaking them down into constituent
parts and understanding the sentences based on the meanings of these parts and the rules used to
combine them. In early studies of this ﬁeld, highly compositional sentences were considered easier
for testing systems, as it is relatively straightforward to decompose such sentences and compose
the meanings of their parts. However, the task becomes much more difﬁcult when more gener-
alization is required for modeling compositionality in new data. In this case, we want systems
to have improved abilities of compositional generalization. In more recent research on LLMs,
this issue has been frequently discussed in compositional reasoning tasks, such as SCAN7 , as it
is considered an important aspect of testing the language understanding and reasoning abilities
of LLMs. This also presents new tasks for developing and examining problem decomposition
methods.
In LLMs, one interesting application of problem decomposition is tool use. In some cases,
it is necessary to integrate external tools into LLMs to access accurate data not available during
training or ﬁne-tuning. For example, LLMs can integrate with APIs to fetch real-time data such
as weather updates, stock market prices, or news feeds, enabling them to provide up-to-date re-
sponses to user queries. When using tools, LLM predictions might include markers that indicate
where and how to call external APIs. This requires decomposing the problem into sub-problems,
with some handled by the LLMs and others by external tools. More detailed discussions on this
topic will be presented in Section 3.2.5.
3.2.3
Self-reﬁnement
In many cases, predictions of LLMs can be inaccurate or incorrect. Given that current LLMs can
perform tasks like reﬁnement and correction, it makes sense to explore methods for these models
to self-reﬁne their outputs. Self-reﬁnement is a common phenomenon in human psychological
7
The SCAN tasks (Simpliﬁed versions of the CommAI Navigation tasks) are designed to evaluate the ability of
LLMs to perform compositional generalization [Lake and Baroni, 2018]. They involve translating natural language
commands into a sequence of actions. For example, a command “jump opposite left and walk thrice” can be translated
into the action sequence “LTURN LTURN JUMP WALK WALK WALK”.3.2 Advanced Prompting Methods
125
activities and daily behavior. For example, when designing a product, a designer might ﬁrst create
a basic prototype, then reﬁne the design after evaluation and testing to enhance user experience
and functionality. The reﬁnement can be iterated several times until the design is satisfactory. The
idea of predict-then-reﬁne can also be found in NLP. One early example is Brill’s tagger [Brill,
1992], where an initial POS tagging result of a sentence can be iteratively reﬁned using a rule-
based system. In the era of deep learning, a good deal of work on sequence-to-sequence problems,
such as grammar correction and text rewriting, can also be seen as examples on this theme.
We can prompt LLMs to do self-reﬁnement. Consider a simple example where we ﬁrst use
an LLM to translate a text from Chinese to English and then use the same model to reﬁne the
translation. Here is how we can prompt the model to translate:
Translate the following text from Chinese to English:
Chinese:
一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公
布的研究成果认为大约从距今5800年开始中华大地上各个区域相继出
现较为明显的社会分化进入文明起源的加速阶段。
English:
A series of discoveries have cemented the signiﬁcant role of the Hongshan
culture in studies on the origins of Chinese civilization. The latest research
ﬁndings suggest that, starting from around 5800 years ago, many regions
across China began to experience noticeable social differentiations, entering an
accelerated phase in the origin of civilization.
Then, we can prompt it to reﬁne the translation:
NiuTrans126
Prompting
You are provided with a Chinese text along with its English translation. Please
reﬁne the translation to improve its accuracy and ﬂuency if necessary.
Chinese:
一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公
布的研究成果认为大约从距今5800年开始中华大地上各个区域相继出
现较为明显的社会分化进入文明起源的加速阶段。
English:
A series of discoveries have cemented the signiﬁcant role of the Hongshan cul-
ture in studies on the origins of Chinese civilization. The latest research ﬁndings
suggests that, starting from around 5800 years ago, many regions across China
began to experience noticeable social differentiations, entering an accelerated
phase in the origin of civilization.
Reﬁnement:
A series of archaeological discoveries have cemented the signiﬁcant role
of the Hongshan culture in studies on the origins of Chinese civilization.
The latest research ﬁndings suggest that, starting from around 5800 years
ago, various regions across China began to experience noticeable social
differentiations, entering an accelerated phase in the origin of civilization.
Here modiﬁcations to the input translation are highlighted in green. This process can be iterative:
at each step, we use the translation from the previous step as input, and prompt the LLM to produce
a potentially better translation.
In the above example, the reﬁnement of the initial model output was achieved by prompting the
LLM with a simple instruction like “Please reﬁne it!”. However, the reﬁnement is based solely on
the instruction-following ability of the LLM, and there is no guidance or supervision on how and
where to improve the model output. A more effective approach would be to consider feedback on
speciﬁc aspects that need reﬁnement. For example, we can prompt the LLM with “Please correct
all the grammatical errors in the translation”, so that the model can focus more on grammatical
error correction during reﬁnement.
A general framework of self-reﬁnement with LLMs involves three steps [Madaan et al., 2024].
• Prediction. We use an LLM to produce the initial model output.
• Feedback Collection. We obtain feedback on the model output.
• Reﬁnement. We use the LLM to reﬁne the model output based on the feedback.
The last two steps can be repeated multiple times, which leads to an iterative self-reﬁnement
process. In this framework, receiving accurate and detailed feedback is critical as it guides the
subsequent reﬁnement step. In general, feedback can be obtained in various ways [Fernandes
et al., 2023; Pan et al., 2024]. We can manually examine the model’s predictions to identify
issues. Also, we can use a feedback model to automatically generate feedback. For instance, a
reward model trained on some labeled data can help evaluate the model output and return a score
indicating its quality.3.2 Advanced Prompting Methods
127
Since the focus of this chapter is on LLM prompting, we consider using LLMs to provide
feedback. To illustrate, let us consider an example of generating good responses to user questions:
• First, we prompt an LLM to answer the input question, as usual.
Generate a response to the question: “What are some major environmental
concerns today?”
Response:
Major environmental concerns today include climate change, air and water
pollution, deforestation, loss of biodiversity, and ocean degradation.
• Then, using the following prompt, we prompt the same LLM again to get feedback on the
generated response.
You will be provided with a question and its corresponding response. Eval-
uate the accuracy and depth of this response, and identify any factual in-
accuracies or areas where additional details could enhance understanding.
Question:
What are some major environmental concerns today?
Response:
Major environmental concerns today include climate change, air and water
pollution, deforestation, loss of biodiversity, and ocean degradation.
Feedback:
The response provided is accurate and highlights key environmental
concerns. However, it would beneﬁt from more speciﬁc and detailed
descriptions of these concerns. Here are suggestions for improvement:
1. Include speciﬁc examples of environmental issues.
2. Add information about major pollutants.
3. Specify the regions most affected.
• Given the feedback the LLM produces, we prompt it to reﬁne the response.128
Prompting
You will be provided with a question, its corresponding response, and
some feedback on this response.
Question:
What are some major environmental concerns today?
Response:
Major environmental concerns today include climate change, air and water
pollution, deforestation, loss of biodiversity, and ocean degradation.
Feedback:
The response provided is accurate and highlights key environmental con-
cerns. However, it would beneﬁt from more speciﬁc and detailed descrip-
tions of these concerns. Here are suggestions for improvement:
1. Include speciﬁc examples of environmental issues.
2. Add information about major pollutants.
3. Specify the regions most affected.
Now reﬁne the response based on the provided feedback.
Reﬁned Response:
Key environmental concerns include climate change, evidenced by rising
global temperatures and severe weather patterns; signiﬁcant air and
water pollution in urban and industrial areas; rampant deforestation in
regions like the Amazon, causing biodiversity loss; and ocean degradation,
highlighted by coral reef bleaching and widespread overﬁshing.
Ideally, if a strong LLM is adopted, we would like to have it perform all three steps without
extra training. On the other hand, if we have enough labeled data for the task of interest, we can
enhance the performance of the LLM using supervised learning. For example, we can ﬁne-tune
the LLM to better adapt it to reﬁnement tasks, or alternatively, use task-speciﬁc models, which
may not necessarily be based on LLMs [Welleck et al., 2023; Schick et al., 2023]. In a broader
sense, improving LLMs for self-reﬁnement tasks can be seen as an alignment issue. For example,
it has been found that some self-correction abilities can be activated through RLHF [Ganguli et al.,
2023]. However, discussing these issues is beyond the scope of this chapter. Further discussion
can be found in Chapter 4.
In LLMs, self-reﬁnement is related to several concepts that reveal the psychological aspects
of these models, such as the ability to self-reﬂect. A view is that if LLMs are capable of self-
reﬂection, their predictions can become more accurate and even possess self-correcting capabili-
ties. This self-reﬂection can be activated in various ways, for example, by prompting these LLMs
to engage in more in-depth and careful thinking, or by providing examples from which the models
can learn and reﬂect. To illustrate, we consider here the deliberate-then-generate (DTG) method
presented in Li et al. [2023a]’s work, where LLMs are prompted to deliberate. In DTG, we are
given an initial model output which may contain errors. LLMs are then prompted to identify the
error types of this model output and provide an improved output. Below is a template of DTG
prompting for Chinese-to-English translation tasks.3.2 Advanced Prompting Methods
129
Given the Chinese sentence: {∗source∗}
The English translation is: {∗target∗}
Please ﬁrst detect the type of error, and then reﬁne the translation.
Error Type:
We aim to ﬁrst predict the error type (red), and then produce a reﬁned translation (blue). This
process of deliberation is guided by the instruction “Please ﬁrst detect the type of error, and then
reﬁne the translation”. It encourages LLMs to initially engage in thoughtful analysis and then give
better results. Since error type prediction and reﬁnement are performed in a single run of LLMs,
this method incorporates both steps of feedback and reﬁnement into one process.
In the above prompts, we assume that the LLM we use is able to review the input translation
and correctly identify its error types. However, this raises new difﬁculties as the model may not
be good at ﬁnding errors in translations. This will in turn result in extra ﬁne-tuning or prompt-
ing engineering efforts. So a simpler method is to reduce the burden of error identiﬁcation and
use LLMs for deliberation only. To do this, we can replace the input translation with a random
translation and assign a default error type. An example of such a prompt is shown below.
Given the Chinese sentence:
一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。
The English translation is:
A variety of innovative techniques have redeﬁned the importance of modern art
in contemporary cultural studies.
Please ﬁrst detect the type of error, and then reﬁne the translation.
Error Type: Incorrect Translation
In this example, the input translation is not generated by LLMs but is instead randomly sam-
pled from the dataset. So it is simply an incorrect translation for the source sentence, and we can
set the error type accordingly. The LLMs then generate a new translation by taking both the source
sentence and the incorrect translation as input. The design of this prompt can also be considered as
activating the learning capabilities of LLMs through “negative evidence” [Marcus, 1993], thereby
enabling them to reﬂect and produce better outcomes through contrastive analysis. Nevertheless,
this method does not rely on any feedback and can enhance the performance of a single LLM
prediction via simple prompting.
Note that while DTG is non-iterative, iterative learning and reﬁnement are commonly used in
NLP. An advantage of these iterative approaches is that they mimic human learning and problem-
solving, where continuous feedback and adjustments lead to progressively improved outcomes.
Iterative methods can be applied to a range of LLM prompting problems. For example, in problem
decomposition, one can incorporate new sub-problems and their solutions into the context at each
step, and thus LLMs can progressively approach the solution of the original problem. On the other
hand, iterative methods raise several issues that are absent in non-iterative methods, for example,130
Prompting
errors in earlier steps may negatively impact subsequent problem-solving, and determining when
to stop iterating often requires additional engineering effort.
3.2.4
Ensembling
Model ensembling for text generation has been extensively discussed in the NLP literature. The
idea is to combine the predictions of two or more models to generate a better prediction. This
technique can be directly applicable to LLMs. For example, we can collect a set of LLMs and run
each of them on the same input. The ﬁnal output is a combined prediction from these models.
For LLM prompting, it is also possible to improve performance by combining predictions
based on different prompts. Suppose we have an LLM and a collection of prompts that address
the same task. We can run this LLM with each of the prompts and then combine the predictions.
For example, below are three different prompt templates for text simpliﬁcation.
Make this text simpler.
{∗text∗}
Condense and simplify this text.
{∗text∗}
Rewrite for easy reading.
{∗text∗}
Each of these prompts will lead to a different prediction, and we can consider all three predictions
to generate the ﬁnal one.
Formally, let {x1 , ▷▷▷, xK } be K prompts for performing the same task. Given an LLM Pr(·|·),
we can ﬁnd the best prediction for each xi using ŷi = arg maxyi Pr(yi |xi ). These predictions
can be combined to form a “new” prediction:
ŷ = Combine(ŷ1 , ▷▷▷, ŷK )
(3.6)
Here Combine(·) is the combination model, which can be designed in several different ways. For
example, we can select the best prediction by voting or by identifying the one that overlaps the
most with others. Another method for model combination is to perform model averaging during
token prediction. Let ŷj be the predicted token at the j-th step for model combination. The
probability of predicting ŷj is given by
ŷj
= arg max
yj
K

k=1
log Pr(yj |xk , ŷ1 , ▷▷▷, ŷj−1 )
(3.7)3.2 Advanced Prompting Methods
131
In ensembling for LLM prompting, it is generally advantageous to use diverse prompts so that
the combination can capture a broader range of potential responses. This practice is common in
ensemble learning, as diversity helps average out biases and errors that may be speciﬁc to any
single model or conﬁguration. From the Bayesian viewpoint, we can treat the prompt x as a latent
variable, given the problem of interest, p. This allows the predictive distribution of y given p to
be written as the distribution Pr(y|x) marginalized over all possible prompts

Pr(y|p) =
Pr(y|x) Pr(x|p)dx
(3.8)
The integral computes the total probability of y by considering all possible values of x, weighted
by their likelihoods given p. Here Pr(y|x) is given by the LLM, and Pr(x|p) is the prior distri-
bution of prompts for the problem. This is a good model because the integral effectively accounts
for the uncertainty in the choice of x, ensuring that the ﬁnal predictive distribution Pr(y|p) is
robust and encompasses all potential variations and biases in the prompts. However, computing
this integral directly can be computationally infeasible due to the potentially inﬁnite space of x.
One approach to addressing this issue is to employ methods like Monte Carlo sampling, which
approximate the integral using a manageable, ﬁnite number of prompts.
While the Bayesian treatment is mathematically well-deﬁned, it is common practice in NLP
to assume a non-informative or uniform prior and focus instead on constructing a set of diverse
prompts. Consequently, the output can be computed using a straightforward combination model,
as described in Eq. (3.6). The issue of creating high-quality, diverse prompts has been studied in
CoT and other in-context learning areas. Most of the research focuses on incorporating a variety
of demonstration examples across different prompts. Here, we list some of these methods.
• Given a problem, we manually create a number of demonstrations and use different ones
for different prompts.
• Given a problem, we use LLMs to automatically generate demonstrations and prompts.
• Given a prompt, we create different prompts by changing the order of demonstrations in the
prompt.
• Given a prompt, we use LLMs to generate a number of similar prompts.
• Given a prompt, we transform it into other forms, e.g., translating it into other languages.
Of course, in practice, we can combine these methods to achieve greater diversity. An underly-
ing assumption here is that diverse prompts can lead to diverse model outputs. This is particularly
the case when the problem we deal with is relatively new and difﬁcult. For stronger and more ro-
bust LLMs, the variance in output for similar prompts might not be large. In this case, the beneﬁt
of involving multiple prompts can be modest.
In addition to providing diverse prompts for LLMs, another approach is to make use of the
inherent variance in the outputs of LLMs. One simple way to generate multiple outputs is to
sample outputs from the hypothesis space. This is straightforward for LLMs because their search
algorithms are naturally suited to producing multiple results. For example, if beam search is
used, we can easily collect all the complete hypotheses in the beam to generate multiple output132
Prompting
candidates. Also, we can modify the search algorithms to implement more powerful sampling
methods and explore a wider range of high-quality hypotheses.
The next question is how to combine diverse model predictions. This leads us back to the
model combination issue as described in Eq. (3.6). Here we consider the self-consistency method,
which outputs not the prediction with the highest probability, but rather the one that best aligns
with other predictions [Wang et al., 2022a; 2023b]. First, an LLM is prompted with CoT as
usual and generates multiple reasoning paths by sampling. Then, the frequency of each answer is
counted across these reasoning paths. The ﬁnal output is the answer that has the highest count.
Below shows three different predictions by prompting an LLM with CoT.
Prediction 1 (Correct):
Q: Three friends play a game where they ﬂip a fair coin. Each friend ﬂips the
coin once. What is the probability that exactly one of them ﬂips heads?
A: Let’s think step by step.
Each coin ﬂip is an independent event with a 50% chance of landing heads
(H) and 50% tails (T). Consider the possible outcomes for three ﬂips: HHH,
HHT, HTH, HTT, THH, THT, TTH, TTT. Out of these, only HHH, HHT, HTH,
and THH are relevant. We are only interested in scenarios with exactly one
head: HTT, THT, and TTH. There are 3 such outcomes out of 8 total, giving a
probability of 3◁8.
Prediction 2 (Correct):
Q: Three friends play a game where they ﬂip a fair coin. Each friend ﬂips the
coin once. What is the probability that exactly one of them ﬂips heads?
A: Let’s think step by step.
Using a binomial probability formula, where p (success or heads) is 0▷5, and
each of the 3 trials (ﬂips) is independent, we look for exactly one success
(head). The formula for exactly k successes in n trials is given by the
 
binomial coefﬁcient formula nk pk (1 − p)n−k . Plugging in the values, we get
3
1
3−1 = 3 · 0▷5 · 0▷25 = 0▷375.
1 (0▷5) (0▷5)
Prediction 3 (Wrong):3.2 Advanced Prompting Methods
133
Q: Three friends play a game where they ﬂip a fair coin. Each friend ﬂips the
coin once. What is the probability that exactly one of them ﬂips heads?
A: Let’s think step by step.
Since each coin ﬂip has a 50% chance of landing heads and a 50% chance of
landing tails, and we are looking for the case where only one of the three ﬂips
results in heads, we might initially think this is simply a matter of picking which
ﬂip is the heads. There are three ﬂips, so one might incorrectly assume that the
chance of picking one speciﬁc outcome like this would be 1 out of 3. Thus, they
might conclude that the probability of exactly one head is 1◁3 = 33▷3%.
Predictions 1 and 2 correctly identify the three cases where exactly one head is ﬂipped, both
obtaining a probability of 37.5%. The reasoning in Prediction 3 fails to account for the total num-
ber of outcomes possible with three coin ﬂips, thus giving a wrong answer of 33.3%. Therefore,
we select 37.5% as the ﬁnal answer because it is the consensus.
Self-consistency provides a criterion for determining the best prediction in a pool of can-
didates. Since the prompt and the model are ﬁxed in this method, it is not strictly a prompt
ensembling method. Instead, it can be seen as an instance of output ensembling methods, also
known as hypothesis selection methods, which have long been explored in NLP, particularly for
text generation problems [Xiao et al., 2013]. In these methods, multiple outputs are generated by
varying model architectures or parameters. Each output is then assigned a score by some criterion,
and the outputs are re-ranked based on these scores. There are various ways to deﬁne the scoring
function, such as measuring the agreement between an output and others, and using a stronger
model to rescore each output8 . Figure 3.2 shows a comparison of different ensembling methods
for LLMs.
Now, let us brieﬂy review the methods we have discussed so far in this section, such as problem
decomposition and self-reﬁnement. It is apparent that these methods enhance decision-making by
introducing more “choices” into the reasoning process. To some extent, they all involve evaluating
and providing feedback on the results of LLMs. For example, in self-reﬁnement, we need to
offer suggestions for improving the prediction of LLMs, and in output ensembling, we select the
optimal output from a pool of candidates. In this sense, these methods fall under the broader
category of predict-then-verify approaches, where predictions are initially made, then veriﬁed and
reﬁned. The fundamental problem here involves verifying and evaluating the reasoning results
or intermediate steps. This issue is somewhat related to the problem of training reward models
in RLHF, although RLHF addresses a different aspect. In fact, the development of veriﬁers has
been explored and implemented in reasoning with LLMs. Most work, rather than developing
heuristic-based inference-time algorithms, focuses on learning veriﬁers in a supervised manner.
A straightforward method is to train veriﬁers as binary classiﬁers, such as classifying an answer
8
An interpretation of self-consistency is to view it as a minimum Bayes risk search process. It searches for the best
output by minimizing the Bayes risk. More speciﬁcally, a risk function R(y, yr ) is deﬁned on each pair of outputs
(denoted by (y, yr )), representing the cost of replacing y with yr . Given a set of outputs Ω, the risk of an output
y ∈ Ω is given by
Risk(y)
=
=
Eyr ∼Pr(yr |x) R(y, yr )

yr ∈Ω
R(y, yr ) · Pr(yr |x)
(3.9)134
Prompting
Combine/Select
Prompt
LLM2Prediction3
LLM2Prediction2
LLM1Prediction1
Final
Prediction
(a) Model Ensembling
Combine/Select
Prompt3
Prompt2
Prediction3
LLM
Prediction2
Prompt1
Final
Prediction
Prediction1
(b) Prompt Ensembling
Combine/Select
m
Sa
Prompt
LLM
e
pl
Prediction3
Prediction2
Final
Prediction
Prediction1
(c) Output Ensembling
Fig. 3.2: Ensembling methods for LLMs. In standard model ensembling (a), multiple LLMs varying in architectures or
parameters are used. Each LLM receives the same prompt and produces a prediction. These predictions are combined
to generate the ﬁnal prediction. In prompt ensembling (b), we have one LLM and multiple prompts. The LLM produces
a prediction for each prompt, and these predictions are combined as usual. In output ensembling (c), the LLM samples
multiple predictions over the prediction space given a prompt. It can be seen as a method to boost the performance
of the LLM itself. Note that these ensembling methods can be combined to increase the diversity of predictions. For
example, we can use both prompt ensembling and output ensembling to obtain more diverse predictions.
as correct or incorrect, although these veriﬁers are typically used as scoring models. Given a
reasoning path for a problem, the veriﬁers can be used to score either the entire path (called
outcome-based approaches) [Cobbe et al., 2021], or each individual reasoning step (called process-
based approaches) [Uesato et al., 2022; Lightman et al., 2024].
3.2.5
RAG and Tool Use
RAG is generally employed when standard LLMs, which rely solely on pre-trained knowledge,
lack accuracy and depth in the generated text. By drawing from external databases and documents,3.2 Advanced Prompting Methods
135
RAG can signiﬁcantly improve the quality of responses, ensuring they are both contextually rel-
evant and factually correct. Such an approach is particularly useful in scenarios that require high
factual accuracy and up-to-date information, such as complex question answering.
The concept of RAG has been mentioned several times in the previous sections and chapters.
For completeness, we outline the key steps involved in RAG here.
• We prepare a collection of texts which are treated as an additional source of knowledge we
can access.
• We retrieve relevant texts for a given query.
• We input both the retrieved texts and the query into an LLM, which is then prompted to
produce the ﬁnal prediction.
Steps 1 and 2 can be implemented by using an external information retrieval system. For
example, we can store the collection of texts in a vector database and then retrieve the most similar
texts through vector-based search techniques. Since information retrieval is not the focus of this
chapter, we will assume that such systems are available off-the-shelf and use them directly.
Here we present how to prompt LLMs to make use of retrieved texts. To illustrate, consider
an example of using LLMs to answer the following question.
Where will the 2028 Olympics be held?
We can simply input this question into an online search engine. It will then return the relevant
pieces of text found on the internet, for example,
(Wikipedia)
The 2028 Summer Olympics, ofcially the Games of the XXXIV Olympiad and com-
monly known as Los Angeles 2028 or LA28, is an upcoming international multi-sport
event scheduled to take place from July 14-30, 2028, in the United States. ...
(The Sporting News)
In 2028, Los Angeles will become the third city, following London and Paris respectively,
to host three Olympics after hosting the Summer Games in 1932 and 1984. It will also
be the rst time the United States has hosted an Olympic Games since the 2002 Winter
Games in Salt Lake City. ...
...
We can use these retrieved texts as additional context, and prompt an LLM to generate a
response based on these texts. Below is an example RAG prompt.136
Prompting
Your task is to answer the following question. To help you with this, relevant
texts are provided. Please base your answer on these texts.
Question:
Where will the 2028 Olympics be held?
Relevant Text 1:
The 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Olympiad and
commonly known as Los Angeles 2028 or LA28 ...
Relevant Text 2:
In 2028, Los Angeles will become the third city, following London and Paris
respectively, to host three Olympics after ...
...
The 2028 Olympics will be held in Los Angeles.
This prompt assumes that the provided texts are relevant to the question and expects the LLM
to generate a faithful response using these texts. However, the information retrieval system may
sometimes provide irrelevant or incorrect texts, which may lead the LLM to produce an incorrect
answer. One straightforward way to address this issue is to improve the accuracy of the informa-
tion retrieval system. Nevertheless, as with most AI systems, errors may still occur. Therefore, it
is also necessary to enhance the robustness of the LLM, so that it can make reasonable predictions
even when the input is inaccurate. Below is a new prompt that enables the LLM to be more faith-
ful to the facts, and allows it to choose not to answer questions when the information provided is
inaccurate.
Your task is to answer the following question. To help you with this, relevant
texts are provided. Please base your answer on these texts.
Please note that your answers need to be as accurate as possible and faithful to
the facts. If the information provided is insufﬁcient for an accurate response, you
may simply output "No answer!".
Question:
Where will the 2028 Olympics be held?
Relevant Text 1:
The 2024 Summer Olympics, ofﬁcially the Games of the XXXIII Olympiad and
branded as Paris 2024, were an international multi-sport event ...
...
No answer!
In this example, the LLM refuses to answer because the provided information is insufﬁcient and
irrelevant to the question.
Both RAG and ﬁne-tuning are common methods for adapting LLMs using task-speciﬁc data.
Standard RAG is training-free and can be directly applied to LLMs. To further improve RAG, it3.2 Advanced Prompting Methods
137
is also possible to ﬁne-tune LLMs, though this will require some training effort. For example, we
can ﬁne-tune LLMs using human-labelled data to supervise them in learning to refuse to answer.
Note that, while the examples shown above seem simple, RAG is not trivial. From the prompt
engineering perspective, different use cases may require different prompts, though our somewhat
“greedy” goal is to develop a universal prompting strategy that can adapt to different tasks. In
many cases, we need to control how much we depend on the retrieved context to make predictions.
Sometimes, LLMs must derive responses strictly from the provided texts, while at other times,
they may need to generate responses using their pre-trained knowledge if the provided texts are
insufﬁcient. There are many aspects of RAG, such as improvements to the retrieval systems, that
cannot be covered in this chapter. Interested readers can refer to surveys of RAG techniques for
more information [Li et al., 2022; Gao et al., 2023c].
One reason we discuss RAG here is that it can be broadly regarded as an instance of the
general problem decomposition framework (see Section 3.2.2). RAG divides problem-solving
into two steps. In the ﬁrst step, we collect relevant and supporting information for a given query
from various knowledge sources. In the second step, we use LLMs to generate responses based
on the collected information. If we extend the concept of problem decomposition further, we
will ﬁnd that many tasks requiring the use of external systems or tools can be treated as similar
problems. One such example is tool use in LLMs. In many applications, LLMs need to employ
external databases, APIs, and even simulation tools to generate accurate responses. For example,
LLMs can access real-time data from ﬁnancial markets to provide up-to-date investment advice or
integrate with healthcare databases to offer personalized medical insights. This integration extends
the capabilities of LLMs by allowing them to interact with, and in some contexts, inﬂuence or
control external systems. Consequently, LLMs function more as autonomous agents rather than
mere text generators [Franklin and Graesser, 1996].
The issue of tool use is broad and vast. Here we narrow our discussion to tasks that can be
facilitated by calling external APIs to solve some of the sub-problems [Parisi et al., 2022; Gao
et al., 2023b]. Consider again the example of asking an LLM to answer “Where will the 2028
Olympics be held?”. Suppose the LLM can access a web search tool. We can then prompt the
LLM to answer the question with web search, like this
Your task is to answer the following question. You may use external tools, such
as web search, to assist you.
Question:
Where will the 2028 Olympics be held?
The information regarding this question is given as follows:
{tool: web-search, query: "2028 Olympics"}
So the answer is: Los Angeles
Here {tool: web-search, query: "2028 Olympics"} indicates a request to the web search
system using the query “2028 Olympics”. When the LLM sees this string, it executes a web search
and uses the result to replace the string. Then, in subsequent steps of prediction, the LLM uses
this web search result as context to produce the correct answer.
Consider another example where we ask the LLM to solve a mathematical problem.138
Prompting
Problem:
A swimming pool needs to be ﬁlled with water. The pool measures 10 meters
in length, 4 meters in width, and 2 meters in depth. Calculate the volume of the
pool in cubic meters and then determine how many liters of water are needed to
ﬁll it (considering 1 cubic meter equals 1000 liters).
Solution:
To solve this problem, the LLM needs to ﬁrst calculate the volume of the
pool by using the formula for the volume of a rectangular prism: Length×
Width × Depth▷ Therefore, The volume is 10 m × 4 m × 2 m = {tool:
calculator, expression: 10 * 4 * 2} m3 . Next, to ﬁnd out how
many liters of water are needed, the LLM multiplies the volume in cubic
meters by 1000 (since 1 cubic meter equals 1000 liters). Thus, 80 ×1000
= {tool: calculator, expression: 80*1000} liters▷
Here the string {tool: calculator, expression: 10 * 4 * 2} triggers the invocation
of a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,
80) will replace {tool: calculator, expression: 10 * 4 * 2} and can be referred to
in the following token predictions. For example, in the last step of problem-solving, 80 is used
instead of {tool: calculator, expression: 10 * 4 * 2}.
A key difference between the tool use examples here and the previously discussed RAG ex-
amples is that in tool use, external functions can be called during inference. In contrast, in RAG,
the retrieved texts are provided before the prediction process begins. However, from the language
modeling perspective, they are actually doing the same thing: before generating the ﬁnal result,
we use external tools, either manually or automatically, to obtain sufﬁcient and relevant context. A
high-level interpretation of these approaches is that they both rely on an “agent” that can determine
where and how to call external functions to generate the context necessary for prediction.
An issue with tool use is that the original LLMs are not trained to generate the necessary
markers for tool use. Therefore, we need to ﬁne-tune the LLMs to adapt them for these tasks
[Schick et al., 2024]. As this chapter focuses on prompting, we will not present the details of this
ﬁne-tuning process. To put it simply, we ﬁrst need to annotate data. For each ﬁne-tuning example,
we replace parts of the output that require the use of external tools with predeﬁned commands
or markers. Then, we use this labeled data to ﬁne-tune the parameters of the LLM as usual. As
a result, the LLM can gain the ability to generate commands for calling external tools. During
inference, we can execute these tool use commands in the model outputs to get assistance from
external tools.
3.3
Learning to Prompt
So far in this chapter, we have considered several basic prompting strategies and various reﬁne-
ments to them. However, all the prompts we have discussed were designed manually. This leads
to a number of problems: First, designing high-quality prompts is inherently difﬁcult and requires
substantial manual effort. For example, extensive experimentation with different prompts is often
needed to identify the most effective ones. Since different LLMs may respond better to certain3.3 Learning to Prompt
139
types of prompts, developing universally effective prompts can be even more resource-intensive.
Second, manual prompt design relies heavily on human expertise, which can limit the diversity
of approaches and overlook potentially effective prompts that are not immediately obvious to hu-
mans. Third, prompts created by humans can be complex and redundant, leading to longer inputs
for LLMs and higher computational costs.
In this section, we discuss techniques for automated prompting. These methods aim to auto-
matically create, optimize, and represent prompts so that the downstream tasks can be addressed
more effectively and efﬁciently. In particular, we consider three issues here.
• How can we automate the process of designing and optimizing prompts for LLMs?
• Are there other forms of representing prompts beyond strings, and how can we learn such
representations?
• How can we make prompts more concise and compact, thereby reducing their complexity
and length?
Note that there are many settings in which we can investigate these issues. For example, we
might specify that prompts are developed speciﬁcally for a particular LLM, or that the develop-
ment is independent of the LLM used. These settings can lead to different methods and application
scenarios, but these methods may overlap in some ways. In the following discussion, we will cover
several different scenarios and discuss the connections between various methods.
3.3.1
Prompt Optimization
Given that prompt design is difﬁcult and labor-intensive, it is desirable to use machine learning
models to discover the optimal prompt for a speciﬁc task (call it automatic prompt design or
prompt optimization). This approach can broadly be regarded as an instance of automated ma-
chine learning (AutoML), which aims to reduce or eliminate the need for expert-driven manual
design of machine learning models. Although our focus here is on the design of prompts, prompts
themselves are discrete structures. Therefore, designing prompts is very similar to designing ma-
chine learning models, such as discrete model architectures. Perhaps one of the most related ﬁelds
is neural architecture search (NAS), where the most optimal neural networks are identiﬁed by
exploring a space of possible neural networks [Zoph and Le, 2016; Elsken et al., 2019]. If we con-
sider prompt optimization as a search process, then we can describe a general prompt optimization
framework involving the following components:
• Prompt Search Space. This deﬁnes all possible prompts that the algorithms can explore.
For example, one can edit some seed prompts to generate a set of diverse candidate prompts.
• Performance Estimation. Once a prompt is chosen, it needs to be evaluated. For example,
a straightforward way is to input it to an LLM and measure its performance on a validation
set.
• Search Strategy. The search process is generally the same as that used in many AI sys-
tems. At each step, the system explores a set of promising prompts in the search space and140
Prompting
evaluates them. This process continues as more prompts are explored. The outcome of the
search is the best-performing prompt observed until the search stops.
This is a very general framework, and different prompt optimization systems can vary in their
design of each component. A widely-used approach is to use LLMs as the basis to develop these
components. Initially, a few prompts are provided. Then, the following process is iterated until
a stopping criterion is met: 1) the prompts are evaluated on a validation set; 2) a candidate pool
is maintained by keeping only the most promising prompts; and 3) new prompts are created by
employing LLMs to infer similar prompts from this candidate pool. One beneﬁt of this approach is
that it allows us to use off-the-shelf LLMs to perform the tasks mentioned above without the need
for substantial system development. To achieve this, we can prompt or ﬁne-tune LLMs to adapt
them to these tasks. Here we consider Zhou et al. [2023c]’s method for illustrating LLM-based
prompt optimization. It involves the following steps.
• Initialization. Let C represent the pool of the candidate prompts we intend to explore. The
ﬁrst step is to add initial prompts into C. We can do this in several ways. A simple method is
to create such prompts by hand for a given task. However, in many cases where humans have
limited knowledge about how to write effective prompts for the task, developing prompts
becomes challenging. In these cases, it is desirable to use LLMs to generate prompts.
For example, we can directly instruct LLMs to produce prompts, providing them with a
description of the task.
You are given a task to complete using LLMs. Please write a prompt to
guide the LLMs.
{∗task-description∗}
This method is straightforward, but it still requires a human-provided description of the task.
An alternative method is to use LLMs to generate prompts given examples of the input and
output of the task. Here is a prompt template.
You are provided with several input-output pairs for a task. Please write
an instruction for performing this task.
Input: {∗input1∗} Output: {∗output1∗}
Input: {∗input2∗} Output: {∗output2∗}
...
As such, LLMs can infer the corresponding instruction for the task from the provided inputs
and outputs.
• Evaluation. Once we obtain the candidate pool C, we need to evaluate the prompts in C.
One method is to feed each prompt into an LLM and assess the results on the downstream3.3 Learning to Prompt
141
task. For example, we can evaluate the output of the LLM given an input using a pre-deﬁned
metric, or alternatively, use the log-likelihood of the output as a measure of the quality of
the prompt.
• Pruning. If C contains a large number of prompts, it is reasonable to prune the unpromising
prompts within it, thus reducing the computational burden in subsequent steps. This is a
standard pruning problem. Given the evaluation score for each prompt, a simple method is
to keep only a certain percentage of the prompts and discard the rest.
• Expansion. Expansion is a key operation in search algorithms used to explore different
states in the search space. The expansion operation here can be deﬁned as a function
C ′ = Expand(C, f )
(3.10)
where C ′ is the set of new prompts generated from C using the model f . If we consider f
as an LLM, we can perform the expansion operation by instructing f to generate new and
relevant prompts based on C. Below is an example.
Below is a prompt for an LLM. Please provide some new prompts to per-
form the same task.
Input: {∗prompt∗}
Then, we replace C with C ′ . The steps of evaluation, pruning and expansion can be re-
peated, and so we can gradually explore a wider range of prompts.
In prompt optimization, the expansion step plays a key role, as it deﬁnes how we explore
the search space, and our goal is to ﬁnd optimal results with minimal effort. One improvement
to this step is to treat the problem as a paraphrasing task. A simple method is to apply off-the-
shelf paraphrasing systems, either based on LLMs or other models, to transform input prompts
into semantically equivalent forms [Jiang et al., 2020]. Alternatively, we can deﬁne speciﬁc edit
operations, such as insertions and modiﬁcations, for each token. A given prompt can be edited
into new prompts by applying these operations [Prasad et al., 2023]. Also, further evaluation and
pruning can be applied to ﬁlter out low-quality prompts. In addition to framing prompt generation
as a paraphrasing problem, we can improve the quality of prompts during expansion by learning
from feedback [Pryzant et al., 2023]. This approach is somewhat related to the self-reﬁnement
issue discussed in Section 3.2.3. An LLM can be used to generate feedback on an input prompt,
which is then revised based on this feedback. This feedback-and-revision cycle can be repeated
multiple times until the result converges or the desired outcome is achieved.
Another approach to prompt optimization is to apply classic optimization techniques. For
example, the problem can be framed as an evolutionary computation problem, where prompts are
treated as candidates that evolve generation by generation as the optimization progresses [Guo
et al., 2024]. Since many powerful optimization algorithms have been developed in related ﬁelds,
they can be directly applied to this problem.142
Prompting
In practice, we might be tempted to use existing LLM APIs to implement the steps described
above. Such an approach, however, would be strongly dependent on the inference and in-context
learning abilities of the LLMs. If these LLMs are not strong and lack adaptation to the tasks, they
may introduce errors into search, for example, generating incorrect prompts during expansion. In
such cases, it is preferable to train models that are better suited to the tasks. One approach in
this research direction appeals to reinforcement learning, which has been widely used in solving
discrete decision making and optimization problems. For example, Deng et al. [2022] developed
a prompt generator by integrating an FFN-based adaptor into an LLM. The prompt generator is
trained as a typical policy network, but only the parameters of the adaptor are updated while the
remaining parameters of the model are kept unchanged. During training, the reward is obtained by
testing the generated prompts using another LLM, similar to the evaluation method as discussed
above. Once the training is complete, the prompt generator is then employed to generate new
prompts.
Note that, in our discussion here, prompts are simply seen as sequences of tokens, and the out-
put of prompt optimization is such a sequence. However, in a strict sense, prompts have complex
structures and include different ﬁelds such as user input, instruction, and demonstration. While
our discussed approaches are mostly general, much work in prompt optimization has focused on
learning better instructions for prompting. Speciﬁcally, the goal is to generate instructions that
effectively guide LLMs based on a given task. Of course, the concept of prompt optimization
can also be extended to learning other parts of prompts. For example, there has been substantial
research interest in learning to select or generate demonstrations in CoT [Liu et al., 2022; Rubin
et al., 2022; Zhang et al., 2023b]. One of the differences between learning instructions and learn-
ing demonstrations is that generating high-quality demonstrations using LLMs is relatively easy
and the focus of learning demonstrations is typically on how to sample appropriate demonstra-
tions from a pool of candidates. In contrast, the difﬁculty in learning instructions is partly because
pre-trained LLMs are not suited to predict the quality of instructions, and testing these instructions
on downstream tasks is computationally expensive. This makes the optimization methods costly
to apply, and exploring a wide variety of instructions poses signiﬁcant challenges.
3.3.2
Soft Prompts
Although developing natural language prompts, either manually or automatically, is a straight-
forward and widely applied approach, it presents some problems. One problem is that natural
language prompts can be complex and lengthy, resulting in signiﬁcant computational burdens
when processed via LLMs. In many applications, users may need to perform a task repeatedly,
and inputting the same long prompt into the LLMs a large number of times is clearly inefﬁcient.
Another problem is that while prompts are typically represented as discrete token sequences (call
them hard prompts) in regular LLM input, the LLMs encode them as low-dimensional real-
valued vectors. This raises the question of whether there are more compact and efﬁcient ways to
represent prompts.
In this subsection, we introduce the concept of soft prompts, which can be viewed as hidden,
distributed representations of prompts. When prompting LLMs, we are concerned with commu-
nicating tasks or questions to elicit the desired responses. We can deﬁne hard prompts as explicit,
predeﬁned text sequences that users input directly into LLMs to guide the responses. In contrast,
we can think of soft prompts as implicit, adaptable prompting patterns embedded within LLMs.
Unlike hard prompts, which are expressed in natural language and should be understandable for3.3 Learning to Prompt
Soft Prompt
143
.........
...
...
...
...
...
...
...
......
...hjhj+1 hj+2 hj+3 hj+4 hj+5 hj+6 hj+7 hj+8 hj+9...
Transformer
... Translate this
into Chinese
.
I
have
a
cat
.
...
Hard Prompt (Instruction)
Fig. 3.3: Illustration of hard and soft prompts. Here the hard prompt is the instruction we input to the LLM for
performing the task. The LLM encodes this instruction as usual, and the intermediate representations corresponding to
the instruction can be viewed as some sort of soft prompt.
humans, soft prompts are encoded in a format that is more comprehensible to the model rather
than to humans. To illustrate, consider a simple prompt
Translate the sentence into Chinese.
Consider it done!
Here, the instruction “Translate the sentence into Chinese” can be seen as a hard prompt, denoted
by the token sequence c1 ▷▷▷c5 . By feeding these tokens into an LLM, they are transformed into a
sequence of real-valued vectors h1 ▷▷▷h5 , each corresponding to a token. We can roughly think of
h1 ▷▷▷h5 as a soft prompt, as illustrated in Figure 3.3.
While the above example shows that soft prompts can be generated by transforming hard
prompts, there is not necessarily a direct correspondence between them. In fact, we do not even
need to interpret soft prompts using meaningful text. They are instead simply hidden states in
LLMs and can be learned as standard parameters of the models through continuous optimiza-
tion. Such a treatment allows us to explore prompting methods beyond text. As another beneﬁt,
soft prompts provide dense, low-dimensional, and learnable representations for encoding how
we guide LLMs to generate speciﬁc outputs. The training and application of these representa-
tions require signiﬁcantly lower computational costs than those required for processing long hard
prompts. This approach would be of great practical value in LLM inference applications where
the same prompt is repeatedly used.
3.3.2.1
Adapting LLMs with Less Prompting
One obvious way to adapt an LLM for a particular task is to simply ﬁne-tune the model using
labeled data. This leads to a variety of LLM alignment methods, such as supervised ﬁne-tuning,
which update the model parameters by aligning the responses to given prompts with supervision
signals. Fine-tuned LLMs embed task-related information in model parameters, and thus these144
Prompting
models can respond correctly when dealing with similar prompts with those in ﬁne-tuning.
If we take this idea further, we can expect LLMs to absorb the knowledge about prompting
of a task as much as possible during ﬁne-tuning. Consequently, the prompting information is
partially captured in the model parameters, and the ﬁne-tuned LLMs can perform the task with
less prompting. Here we consider a simple form of prompt, where only an instruction (denoted by
c) and a user input (denoted by z) are included. A prompt can be expressed using the following
tuple
x = (c, z)
(3.11)
Given a set of prompt-response pairs D = {(x, y)}, the objective of ﬁne-tuning is to minimize
the total loss incurred over this set. A popular method is to minimize the negative log-likelihood
(i.e., maximize the log-likelihood) with respect to the model parameters θ:
θ̂ = arg max
θ
= arg max
θ

log Prθ (y|x)
(x,y)∈D

log Prθ (y|c, z)
(3.12)
(x,y)∈D
where Prθ (·|·) is the probability predicted by an LLM with the parameters θ9 .
In general, the instruction in each ﬁne-tuning example should follow the guideline of prompt
design, for example, a good instruction should be as clear as possible and provide a detailed
description of the task. However, the method described in the above equation does not restrict the
instruction to any particular form. This ﬂexibility allows us to instruct LLMs in any way that we
want. Consider an example where we intend to instruct LLMs to translate an English sentence
into Chinese. Of course, as mentioned earlier in this chapter, we can prompt LLMs using the
instruction
Translate the following sentence from English to Chinese.
If we want the instruction to be simpler, we may rephrase it into a simpler form
Translate this into Chinese.
Even, we can deﬁne the instruction as a single phrase
Translate!
With certain ﬁne-tuning effort, we can adapt LLMs to follow any of these instructions. From
an efﬁcient prompting perspective, there are computational advantages in simplifying instructions
in prompting. For example, we can use simple instructions like “Translate!” to perform tasks
that would typically require more complex and detailed instructions. This can make subsequent
9
In practice, we initialize θ with the parameters obtained from pre-training, and then adjust θ moderately to ensure
that the results after ﬁne-tuning do not deviate too much from the pre-trained results.3.3 Learning to Prompt
145
Teacher Model:
+
Full Context
c
User InputPrt (y|c, z)
zy
Loss
Student Model:
+
Simpliﬁed Context
c′
User InputPrs (y|c′ , z)
zy
Fig. 3.4: Illustration of context distillation [Snell et al., 2022]. The teacher model is a standard LLM, which takes both
the context and the user input as model input and produces a prediction as model output. Then, we simplify the context
(e.g., simplifying the instruction in prompting) and use the student model to make predictions based on the simpliﬁed
context and the user input. The student model is trained by minimizing the loss between the predictions produced by
the two models.
prompting during inference much easier. On the other hand, ﬁne-tuning LLMs with overly simpli-
ﬁed instructions may be harmful to the generalization of the models. Since simpliﬁed instructions
can lead to a loss of information, it is more likely that the LLMs will overﬁt the ﬁne-tuning data
and fail to generalize beyond those instructions. In scenarios involving both complex and simpli-
ﬁed instructions for ﬁne-tuning, this problem is more severe because the labeled data available for
ﬁne-tuning is usually limited, and accommodating a variety of instructions is costly.
An alternative way to adapt LLMs for simpliﬁed instructions is through knowledge distillation.
As an example, we consider the context distillation method [Snell et al., 2022]. The goal of this
method is to learn a student model that can make use of simpliﬁed instructions from a well-trained
instruction-following teacher model. Figure 3.4 shows an illustration of this approach. Building
the teacher model follows a standard ﬁne-tuning process: we ﬁrst collect a certain amount of
data that includes instructions, user inputs, and correct responses, and then we continue to train a
pre-trained model with this dataset. For building the student model, we need to construct a new
dataset D ′ where each sample is a tuple consisting of an instruction, a corresponding simpliﬁed
instruction, and a user input, denoted by x′ = (c, c′ , z). Knowledge distillation is performed by
minimizing a loss function deﬁned on the outputs of the teacher and student models

θ̂ = arg min
θ
x′ ∈D ′
Loss(Prt (·|·), Prsθ (·|·), x′ )
(3.13)
where Prt (·|·) denotes the pre-trained teacher model, and Prsθ (·|·) denotes the student model with
the parameters θ. To keep the notation simple we will write Loss(Prt (·|·), Prsθ (·|·), x) as Loss
for short. A commonly-used loss is the sequence-level loss, which has the basic form:
Loss =

y
Prt (y|c, z) log Prsθ (y|c′ , z)
(3.14)
But this function is computationally infeasible because it requires summing over an exponen-
tially large number of outputs. A variant of this method is to train the student model using outputs
generated by the teacher model. For each sample, we use the teacher model to produce an output146
Prompting
ŷ = arg maxy log Prt (y|c, z). Then we consider ŷ as the target for learning, and the loss function
is given by
Loss = log Prsθ (ŷ|c′ , z)
(3.15)
Alternatively, we can minimize the distances between the probability distributions outputted
by the two models [Askell et al., 2021]. For example, the loss function can be deﬁned as the KL
divergence between the two output distributions
Loss = KL(Pt || Psθ )(3.16)
Pt = Prt (·|c, z)(3.17)
Prsθ (·|c′ , z)(3.18)
where
Psθ
=
Although we have restricted ourselves to knowledge distillation for instructions, the approaches
discussed here are general. By learning from the outputs of the teacher model, the knowledge in
prompting can be distilled into the parameters of the student model. Therefore, the distilled model
can be considered as encoding some sort of soft prompt. This method can be applied to many
other problems in prompt learning, such as compressing long contexts and learning soft prompts
as speciﬁc components of LLMs.
3.3.2.2
Learning Soft Prompts for Parameter-efﬁcient Fine-tuning
Updating all parameters is a common method for adapting LLMs to tasks of interest. Although
ﬁne-tuning is considered computationally cheaper than pre-training, it is still costly to apply in
practice. This issue motivates the development of parameter-efﬁcient ﬁne-tuning methods, which
aim to minimize the number of parameters that need to be updated.
One approach, known as preﬁx ﬁne-tuning, is to append a series of trainable vectors, or
preﬁxes, at the beginning of the input of each Transformer layer [Li and Liang, 2021]. These
preﬁxes can be thought of as soft prompts that serve as additional context to guide the behavior
of the model under speciﬁc tasks. During ﬁne-tuning, we need only to learn the preﬁxes for
embedding task-speciﬁc knowledge. Thus, this method is efﬁcient because it only modiﬁes a
small part of the model rather than adjusting the entire set of model parameters.
l . The output of
Speciﬁcally, let the input of a layer at depth l be denoted by Hl = h0l h1l ▷▷▷hm
the layer can be expressed as
Hl+1 = Layer(Hl )
(3.19)
l by adding a few vectors at the begin-
In preﬁx ﬁne-tuning, we extend the sequence h0l h1l ▷▷▷hm
l
l
l
l
ning, which we denote as p0 p1 ▷▷▷pn . Hence H can be written in the form
l
Hl = pl0 pl1 ▷▷▷ pln h0l h1l ▷▷▷ hm


trainable
 


previous layer output
(3.20)3.3 Learning to Prompt
147
The output of the layer is the last m + 1 representations.
H
l+1
= Layer(Hl )[−m − 1 :]
l+1
= h0l+1 h1l+1 ▷▷▷ hm
(3.21)
where [−m − 1 :] denotes the slicing operation that extracts the last m + 1 elements of a sequence.
l+1
Given H , the input of the next layer can be expressed in the same form of Eq. (3.20):
pl+1
▷▷▷ pl+1
Hl+1 = pl+1
n H
0
1
l+1
l+1 l+1
l+1
= pl+1
pl+1
▷▷▷ pl+1
n h0 h1 ▷▷▷ hm
0
1
(3.22)
Here each pi ∈ Rd can be seen as a learnable parameter. During training, pl0 pl1 ▷▷▷pln are trained
as usual, and the parameters of the original Transformer model are kept ﬁxed.
Figure 3.5 shows an illustration of preﬁx ﬁne-tuning for a translation task. Here, only the preﬁx
vectors pl0 and pl1 are updated by receiving the error gradients from the output (i.e., the Chinese
translation). By adjusting these vectors for the translation task, the model adapts accordingly. This
makes pl0 and pl1 serve as prompts which activate the LLM to perform the task without needing
explicit input prompts like “Translate the following sentence from English to Chinese”. At test
time, we prepend the optimized pl0 and pl1 to the layer, and the LLM will then translate the input
sentence. Note that preﬁx ﬁne-tuning introduces additional L × n × d parameters, where L is the
number of layers, n is the number of preﬁxes, and d is the dimensionality of each preﬁx. However,
this number is much smaller compared to the total number of parameters in the LLM, making the
ﬁne-tuning process highly efﬁcient.
While preﬁx ﬁne-tuning is simple, it still requires modiﬁcations to LLMs. Alternatively, sep-
arating soft prompts from the LLMs allows us to preserve the original model architecture, making
it more efﬁcient for deployment across different tasks without the need to adjust the core model.
One such method is prompt tuning [Lester et al., 2021]. Like preﬁx ﬁne-tuning, prompt tuning
incorporates trainable vectors so that LLMs can adapt to given tasks by adjusting these vectors.
However, prompt tuning differs in that it modiﬁes only the embedding layer.
Recall that in LLMs each input token zi is represented by an embedding ei . These embeddings
are generally learned through a token embedding model and are then used as the real inputs to the
LLMs, replacing the symbolically represented tokens. In prompt tuning, a number of pseudo
embeddings p0 ▷▷▷pn are added at the beginning of the token embedding sequence. So the actual
input to the LLMs can be expressed as
p p1 ▷▷▷ pn e0 e1 ▷▷▷ em
0

trainable
 


token embeddings
Note that a pseudo embedding needs not to correspond to any token in natural language. Instead
these embeddings can be seen as “soft prompt embeddings” that serve to condition the LLMs.
By training soft prompt embeddings on task-speciﬁc data, they learn to interact adaptively with
the token embeddings e0 ▷▷▷em and guide the behavior of LLMs. Since prompt tuning does not
change the underlying parameters of pre-trained LLMs, it is considered a lightweight and efﬁcient
method of ﬁne-tuning, improving task-speciﬁc performance while maintaining their generalization
capabilities. See Figure 3.6 for an illustration of prompt tuning.148
Prompting
···
···
LossLoss
······
h3l+1h4l+1h5l+1
h2lh3lh4l
···
Layer l + 1
pl+1
0
pl+1
1
h0l+1
h1l+1
Layer l
trainable preﬁxes
pl0
pl1
pl−1
0
h0l
pl−1
1
h1l
Layer l − 1
h0l−1h1l−1h3l−1h4l−1h5l−1
···············
Lookout!小心!
User Input
Soft Prompt
LLM Prediction
Fig. 3.5: Illustration of preﬁx ﬁne-tuning for a translation task (Look out! → 小心!). For each layer, we add two
preﬁxes pl0 and pl1 at the beginning. The LLM is trained to minimize the loss on the predictions given the input.
During this process, only the preﬁxes are optimized while the rest of the parameters remain ﬁxed. Therefore, the model
can adapt to the given task in a very efﬁcient manner. At inference time, the LLM works with optimized preﬁxes, and
can perform the task without the need of explicit hard prompts.
Since p0 p1 ▷▷▷ pn is itself a sequence, we can employ sequence models to better represent
it. For example, a Transformer model can encode this sequence, and the resulting representation
can then be used as the input to the LLM. In other words, we can develop an additional model
for encoding soft prompts. Another way to improve prompting is by combining soft and hard
prompts, thereby taking advantage of both types [Liu et al., 2023b]. In the embedding sequence,
we can arrange or intersperse these prompts. This would result in different prompt patterns. For
example, a simple pattern that uses both two types of prompt is
Soft Prompt
p0
p1
···
User Input and Response
Hard Prompt
pn
q0q1···qm′e0e1···em
c0c1···cm ′z0z1···zm
where c0 ▷▷▷cm′ denotes the hard prompt and q0 ▷▷▷qm′ denotes the corresponding embedding se-
quence.
Here we have considered methods for inserting soft prompts in LLMs. But we skip the details
of training these soft prompts and assume that the reader is familiar with the standard supervised
learning process, that is, maximizing the likelihood of the correct model output given the model3.3 Learning to Prompt
149
···
···
···
···
LossLoss
·········
Layer l + 1
Layer l
Layer l − 1
trainable prompt
embeddings
·····················
p0p1e0e1e2e3e4
Lookout!小心!
Soft Prompt
User Input
LLM Prediction
Fig. 3.6: Illustration of prompt tuning for a translation task (Look out! → 小心!). Instead of using ﬁxed textual
prompts, soft prompts are learnable embeddings that are added at the beginning of the embedding sequence. During
ﬁne-tuning, only these prompt embeddings are optimized to efﬁciently adapt the LLM to the given task. Once opti-
mized, the prompt embeddings are used to instruct the LLM to perform the task as new data arrives.
input. In fact, learning soft prompts can be related to many issues in LLM ﬁne-tuning. For exam-
ple, if we consider it as a context compression problem, we can apply the knowledge distillation
methods described previously. In Mu et al. [2024]’s work, prompts are compressed and repre-
sented as a few pseudo tokens, which are appended to each input sequence. The embeddings of
these pseudo tokens are optimized to mimic the predictions of a standard-prompted model. In
other words, the prompting knowledge is distilled from a teacher model into the pseudo tokens.
Broadly speaking, many parameter-efﬁcient ﬁne-tuning methods can be thought of as learning
some sort of soft prompt [Lialin et al., 2023]. When we ﬁne-tune a part of an LLM for a task, this
process can essentially be seen as injecting task-related prompting information into that speciﬁc
part of the model. Another widely-used approach to parameter-efﬁcient ﬁne-tuning is to add an
adaptor layer between the existing model layers. This approach allows us to ﬁne-tune only the
adaptor layer on speciﬁc tasks without altering the underlying architecture or retraining the entire
model. In this sense, adaptor layers can be viewed as soft prompts that encode prompting and task-
related information and interact with the original LLM to help it adapt. To summarize, Figure 3.7
shows a comparison of different methods of using soft prompts in LLMs.
3.3.2.3
Learning Soft Prompts with Compression
Another approach to learning soft prompts is from the perspective of compression. As a simple
example, consider the problem of approximating a long context using a continuous representation
[Wingate et al., 2022]. Suppose we have a user input z and its context c (such as long instructions
and demonstrations). Now we want to develop a compressed representation of the context, denoted150
Prompting
LLM
LLM
LLM
(b) Soft Prompts as Inputs (Embeddings)
Layer
Adaptor
(a) Soft Prompts as Preﬁxes
LLM
(c) Fine-tuning Parts of the Model
(d) Fine-tuning the Adaptor
Fig. 3.7: Illustrations of using soft prompts in LLMs. Here tunable soft prompts are shown in blue, and components
whose parameters are ﬁxed during ﬁne-tuning are shown in gray. In sub-ﬁgure (a), soft prompts are preﬁxes appended
to each layer of the LLM. In sub-ﬁgure (b), soft prompts are used as input embeddings for the LLM. In sub-ﬁgures (c)
and (d), soft prompts are broadly treated as components of the model that are ﬁne-tuned for task adaptation.
by σ, such that the prediction based on z and σ is as close as possible to the prediction based on z
and c. This goal can be expressed in the form
σ̂ = arg min s(ŷ, ŷσ )
σ
(3.23)
where ŷ = arg maxy Pr(y|c, z) and ŷσ = arg maxyσ Pr(y|σ, z) are the LLM predictions given
the full context and the compressed context, respectively. The function s(·, ·) typically represents
a loss or similarity measure, aiming to minimize the difference in predictions between the two
context representations.
One general framework for achieving this is knowledge distillation, where ŷ and ŷσ can be
seen as the predictions of the teacher model and the student model, respectively. This formal-
ization links our discussion to the context distillation problem discussed earlier. The training
objective can be obtained by analogy with Eqs. (3.15) and (3.16). For example, a simple training
objective is given by
σ̂ = arg max log Pr(ŷ|σ, z)
σ
(3.24)
Alternatively, we can minimize the KL divergence between the output distributions, giving
σ̂ = arg min KL(Pr(·|c, z) || Pr(·|σ, z))
σ
(3.25)
The difference with the models in Eqs. (3.15) and (3.16) is that here the compressed context is
represented as real-valued vectors (call them prompt embeddings), rather than as normal tokens.
By applying the above methods, we distill the context from the token sequence c into the embed-
dings σ. Note that the teacher model Pr(·|c, z) and the student model Pr(·|σ, z) may not share
the same architecture or model settings. In practice, we generally wish for the teacher model to be3.3 Learning to Prompt
h1<i
151
h1<i
h1
h2
h3
h4
<i+1
σ1<i+1
σ2
Soft Prompts
at the Current Step
Transformer Layers
Soft Prompts
at Step i − 1
σ1<i
σ2<i
ei1ei2ei3ei4⟨e1 ⟩⟨e2 ⟩
z1iz2iz3iz4i⟨g1 ⟩⟨g2 ⟩
Fig. 3.8: Illustration of compressing a context segment into soft prompts (κ = 2 and mi = 4). The input to the
LLM includes the soft prompts from the previous step (σ1<i and σ2<i ), the tokens of the segment (z1 , z2 , z3 , and z4 ),
and the summary tokens (⟨g1 ⟩ and ⟨g2 ⟩). Given these, the LLM operates as usual. We then extract the outputs at the
last Transformer layer that correspond to the summary tokens. These outputs can be viewed as the soft prompts that
accumulated up to this segment.
stronger, while the student model should be smaller and more efﬁcient.
While compressing full context into continuous representations is a straightforward approach
to learning soft prompts, it requires a teacher model that can deal with long input sequences. In
many cases, however, the context is so long that applying an LLM is too costly or infeasible.
Modeling long input sequences can fall under the broad family of efﬁcient methods for long-
context LLMs. Many techniques have been developed to address this issue. For example, one can
use a ﬁxed-size KV cache to store the past information at each step during inference. Efﬁcient
Transformer architectures and long-context LLMs have been intensively discussed in this book.
For more detailed discussions of these topics, interested readers can refer to Chapter 2.
There are also methods speciﬁcally designed to compress long context into soft prompts. Here
we consider Chevalier et al. [2023]’s method as an example. The basic idea is that we learn
soft prompts gradually by accumulating the ﬁxed-size context representation over the context
sequence. Given a long context, we ﬁrst divide it into a number of segments z1 , ▷▷▷, zK . We
then process these segments in sequence, each time generating a representation of the context
we have processed so far, denoted by σ <i+1 . To do this, a few summary tokens ⟨g1 ⟩, ▷▷▷, ⟨gκ ⟩
i , along with the previous context
are introduced. At each step, we take a segment zi = z1i ▷▷▷zm
i
<i
representation σ and the summary tokens ⟨g1 ⟩, ▷▷▷, ⟨gκ ⟩ as input, and use an LLM to produce the
corresponding hidden representation sequence at the last Transformer layer. An example of this
process is illustrated in Figure 3.8.
Here σ <i is essentially a memory. The model operates in an RNN fashion. Each time we take
a segment and update this memory by encoding both the previous memory state and the segment.
Therefore, the σ <i produced at the last segment is a representation of the entire context sequence.
The Transformer model for learning these representations can be a standard LLM but we need to
ﬁne-tune it to adapt to this context representation task.
Note that here we simply consider prompt and context as similar terms, even though they are
not the same. Although we are somewhat “misusing” the concept prompt, we can often view it as
a type of context. From this perspective, the methods discussed here can be applied to general text
compression problems.152
Prompting
3.3.3
Prompt Length Reduction
While soft prompts provide dense, hidden representations, they are not directly interpretable. The
lack of interpretability can be a signiﬁcant barrier for users trying to understand how their inputs
inﬂuence LLM outputs. Moreover, although soft prompts are efﬁcient for ﬁne-tuning and de-
ployment, they are inﬂexible and do not allow for easy adjustments without extensive ﬁne-tuning
or modiﬁcation. This inﬂexibility can limit their utility in dynamic environments where prompt
changes are frequently needed.
One alternative way to develop efﬁcient prompts is to simplify the text used for prompting.
For example, below is a prompt for answering questions on healthcare and ﬁnance.
The task involves developing a language model capable of understanding and
responding to user inquiries across various domains, with a particular emphasis
on healthcare and ﬁnance. Considering the broad range of potential queries,
from the speciﬁcs of medical diagnoses to the nuances of ﬁnancial regulations,
the model must ensure a comprehensive understanding and accurate responses.
Question:
What are the best practices for using artiﬁcial intelligence in diagnosing cardio-
vascular diseases?
We can simplify the task description by deleting the unimportant parts.
The task involves developing a language model capable of understanding
and responding to user inquiries across various domains, with a particular
emphasis on healthcare and nance. Considering the broad range of potential
queries, from the specics of medical diagnoses to the nuances of nancial
regulations, The model must ensure a comprehensive understanding and ac-
curate responses.
We can also paraphrase it as a shorter text.
The task involves developing a language model focused on healthcare and
nance, capable of understanding and accurately responding to a wide range
of user inquiries.
This problem can be viewed as a classic NLP issue — text simpliﬁcation. So the methods used
can be general and not restricted to the problem of simplifying prompts. There are many ways to
achieve this. One simple method is to deﬁne some heuristics and identify redundant words that
can be eliminated without losing essential information. For example, we can examine each token
in a sequence in terms of its contribution to the overall meaning and remove those that provide
minimal value [Li et al., 2023c; Jiang et al., 2023b]. Another method involves framing the problem
as a sequence-to-sequence task. With labeled data for text simpliﬁcation, we can train an encoder-
decoder model to transform each input text into its simpliﬁed form. In addition, given that many3.4 Summary
153
LLMs have been ﬁne-tuned and aligned to perform text simpliﬁcation tasks, it is straightforward
to use these models to simplify prompts. For example, we can prompt an LLM to simplify a text
under certain constraints, such as limiting the length of the simpliﬁed text.
3.4
Summary
In this chapter, we have discussed a variety of issues related to LLM prompting. Our discussion
has focused mainly on two aspects:
• How to design basic prompts to guide the predictions of LLMs and reﬁne these prompts for
more effective and efﬁcient problem-solving?
• How to automate the design and representation of prompts?
Solutions to these issues involve both general prompt designs and more advanced techniques, such
as CoT and prompt learning, which have been explored extensively in recent research.
In NLP, prompting can be viewed as a technology that has evolved along with LLMs, and
in a sense, it has opened the door to the practical application of these models in an impressive
range of problem domains. In fact, if we expand the concept of prompts to some extent, it can be
traced back to the early days of machine learning and NLP. For example, many NLP systems use
hand-crafted features and templates to “prompt” speciﬁc tasks. Imagine developing a feature to
indicate whether a text is formal or informal. We can feed this feature into a machine translation
system to condition the translation on the type of the input text.
The widespread use of the modern concept of prompts began with the rise of large pre-trained
models in the ﬁeld of NLP. Initially, these models, such as BERT, were adapted to speciﬁc down-
stream tasks mainly through ﬁne-tuning. However, researchers soon discovered that by designing
speciﬁc "prompts" — adding certain words or sentences to the input — the models could be
triggered to respond to speciﬁc tasks without extensive ﬁne-tuning. This motivated the NLP com-
munity to develop and apply universal foundation models that can be prompted to address various
tasks without changing the underlying architecture and the pre-training procedure.
Prompting approaches were ﬁrst experimented with smaller models and later demonstrated
impressive capabilities with large models like GPT-3, which could generate high-quality text in
response to simple prompts across various tasks. As prompting technology evolved, prompt en-
gineering emerged as a critical area of research. As discussed in this chapter, it broadly involves
designing effective prompts to maximize model performance, encompassing both hand-crafted
and automatically generated prompts. More recent research has explored how to enhance the ef-
fectiveness of prompting through techniques like few-shot learning, zero-shot learning, and CoT
reasoning, enabling LLMs to work effectively across a wide range of scenarios. A general dis-
cussion of prompting can be very broad, and we cannot cover all details in this chapter. For
more advanced techniques of prompting, the reader can refer to recent surveys. Topics include
in-context learning [Li, 2023; Dong et al., 2022], CoT [Chu et al., 2023; Yu et al., 2023; Zhang
et al., 2023a], efﬁcient prompting [Chang et al., 2024], and general prompt engineering [Liu et al.,
2023c; Chen et al., 2023a].154
Prompting
Note that although we would ideally like to develop general prompting methods without ad-
justing model architectures and parameters, the results of prompting generally depend heavily
on the quality and size of the given LLMs. For stronger models, such as commercialized online
LLMs, simple prompts may be sufﬁcient to instruct these models to perform tasks correctly. In
this case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs
work properly. By contrast, if the LLMs are not powerful enough, we may need to carefully design
the prompts to achieve the desired results. In many cases, ﬁne-tuning is still necessary to adapt
the models to sophisticated prompting strategies.https://github.com/NiuTrans/NLPBook
https://niutrans.github.io/NLPBook
C HAPTER 4
Alignment
Alignment is not a new concept in NLP, but its meaning varies across different domains and over
time. In traditional NLP, the term alignment typically refers to the tasks that link corresponding
elements in two sets, such as aligning words between a Chinese sentence and an English sentence.
As LLMs become increasingly important in NLP research, this term is more broadly used to refer
to aligning model outputs with human expectations. The problem that alignment addresses is
that the output of a model may not align with the speciﬁc goals or contexts intended by users.
For example, pre-trained LLMs may not be able to follow user instructions because they were
not trained to do so. Another example is that LLMs may generate harmful content or perpetuate
biases inherent in their training data. This poses new challenges in ensuring that LLM outputs are
not only accurate and relevant, but also ethically sound and non-discriminatory.
Simply pre-training LLMs can result in a variety of alignment problems. Our ultimate goal
is to resolve or mitigate all these problems to ensure LLMs are both accurate and safe. There
is an interesting issue here: since large language models are trained on vast amounts of data,
we have reason to believe that if we have sufﬁcient data covering a variety of tasks and aligned
with human preferences, pre-training could make LLMs accurate and safe enough, perhaps even
eliminating the need for alignment. However, the reality is that it is nearly impossible to gather
data that encompasses all tasks or adequately represents human preferences. This makes it difﬁcult
to achieve model alignment through pre-training alone, or at least, at this stage, alignment remains
a very necessary and critical step in the development of LLMs.
In this chapter, we will focus on alignment methods for LLMs. We will begin by discussing the
general alignment tasks. Then we will consider two widely-used approaches, known as instruc-
tion alignment and human preference alignment, respectively. The former resorts to supervised
ﬁne-tuning techniques and guides the LLMs to generate outputs that adhere closely to user instruc-
tions. On the other hand, the latter typically relies on reinforcement learning techniques, where
the LLMs are trained based on feedback from humans. While these methods are motivated by
different goals, they are commonly used together to develop well-aligned LLMs.
4.1
An Overview of LLM Alignment
Alignment can be achieved in several different ways. We need different methods for LLM align-
ment because this problem is itself complicated and multifaceted, requiring a blend of technical
considerations. Here we consider three widely-used approaches to aligning LLMs.
The ﬁrst approach is to ﬁne-tune LLMs with labeled data. This approach is straightforward
as it simply extends the pre-existing training of a pre-trained LLM to adapt it to speciﬁc tasks.
An example of this is supervised ﬁne-tuning (SFT), in which the LLM is further trained on a
dataset comprising task-speciﬁc instructions paired with their expected outputs. The SFT dataset
is generally much smaller compared to the original training set, but this data is highly specialized.
The result of SFT is that the LLM can learn to execute tasks based on user instructions. For
example, by ﬁne-tuning the LLM with a set of question-answer pairs, the model can respond to
speciﬁc questions, even if not directly covered in the SFT dataset. This method proves particularly156
Alignment
Pre-training
Stage
Pre-training
Alignment
Stage
Instruction
Alignment
(e.g., SFT)
Training & Fine-tuning
Human Preference
Alignment
(e.g., RLHF)
Prompting
Inference
Fig. 4.1: Schematic illustration of the pre-train-then-align method for developing LLMs. In the pre-training stage, we
train an LLM on vast amounts of data using next token prediction. Then, in the alignment stage, we align the LLM
to user instructions, intents, and preferences. This includes instruction alignment, human preference alignment, and
prompting.
useful when it is relatively easy to describe the input-output relationships and straightforward to
annotate the data.
The second approach is to ﬁne-tune LLMs using reward models. One difﬁculty in alignment
is that human values and expectations are complex and hard to describe. In many cases, even for
humans themselves, articulating what is ethically correct or culturally appropriate can be challeng-
ing. As a result, collecting or annotating ﬁne-tuning data is not as straightforward as it is with SFT.
Moreover, aligning LLMs is not just a task of ﬁtting data, or in other words, the limited samples
annotated by humans are often insufﬁcient to comprehensively describe these behaviors. What we
really need here is to teach the model how to determine which outputs are more in line with human
preferences, for example, we not only want the outputs to be technically accurate but also to align
with human expectations and values. One idea is to develop a reward model analogous to a human
expert. This reward model would work by rewarding the LLM whenever it generates responses
that align more closely with human preferences, much like how a teacher provides feedback to a
student. To obtain such a reward model, we can train a scoring function from human preference
data. The trained reward model is then used as a guide to adjust and reﬁne the LLM. This frames
the LLM alignment task as a reinforcement learning task. The resulting methods, such as rein-
forcement learning from human feedback (RLHF), have been demonstrated to be particularly
successful in adapting LLMs to follow the subtleties of human behavior and social norms.
The third approach is to perform alignment during inference rather than during training or
ﬁne-tuning. From this perspective, prompting in LLMs can also be seen as a form of alignment,
but it does not involve training or ﬁne-tuning. So we can dynamically adapt an LLM to various
tasks at minimal cost. Another method to do alignment at inference time is to rescore the outputs
of an LLM. For example, we could develop a scoring system to simulate human feedback on the
outputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.
The three methods mentioned above are typically used in sequence once the pre-training is
complete: we ﬁrst perform SFT, then RLHF, and then prompt the LLM in some way during
inference. This roughly divides the development of LLMs into two stages — the pre-training stage
and the alignment stage. Figure 4.1 shows an illustration of this. Since prompting techniques have
been intensively discussed in the previous chapter, we will focus on ﬁne-tuning-based alignment
methods in the rest of this chapter.4.2 Instruction Alignment
4.2
157
Instruction Alignment
One feature of LLMs is that they can follow the prompts provided by users to perform various
tasks. In many applications, a prompt consists of a simple instruction and user input, and we want
the LLM to follow this instruction to perform the task correctly. This ability of LLMs is also
called the instruction-following ability. For example, below is a prompt where we want the LLM
to extract key points and provide a concise summary for a lengthy article.
Instruction
Input
Summarize this text in three sentences.
Daylight Savings Time (DST) - the process of moving clocks forward
by one hour in the summer - was started in Germany in 1916 ...
Output
This task requires the LLM to understand the instruction “Summarize this text in three sentences”
and perform the summarization accordingly. However, LLMs are typically trained for next-token
prediction rather than for generating outputs that follow instructions. Applying a pre-trained LLM
to the above example would likely result in the model continuing to write the input article instead
of summarizing the main points. The goal of instruction alignment (or instruction ﬁne-tuning) is
to tune the LLM to accurately respond to user instructions and intentions. The rest of this section
will discuss some issues related to instruction alignment, including ﬁne-tuning LLMs to follow
instructions, generating or collecting instruction data, and generalizing instruction alignment.
4.2.1
Supervised Fine-tuning
One straightforward approach to adapting LLMs to follow instructions is to ﬁne-tune these mod-
els using annotated input-output pairs [Ouyang et al., 2022; Wei et al., 2022a]. Unlike standard
language model training, here we do not wish to maximize the probability of generating a com-
plete sequence, but rather maximize the probability of generating the rest of the sequence given
its preﬁx. This approach makes instruction ﬁne-tuning a bit different from pre-training. The SFT
data is a collection of such input-output pairs (denoted by S), where each output is the correct
response for the corresponding input instruction. For example, below is an SFT dataset
x (instruction + user input)
Summarize the following article.
Article: In recent years, solar energy has seen
unprecedented growth, becoming the fastest-growing ...
Extract the main ﬁnancial ﬁgures from the following
earnings report.
Report: The company reported a revenue of $10 million
in the ﬁrst quarter with a proﬁt margin of 15% ...
Classify the following email as spam or not spam.
Text: Congratulations! You’ve won a $500 gift card.
Click here to claim now.
Provide a solution to the following technical issue.
Issue: my computer is running slow and often freezes.
y (output)
{∗summary∗}
Revenue: $10 million,
Proﬁt Margin: 15%
Spam
First, check for ...158
Alignment
where the instructions are highlighted. This dataset contains instructions and the corresponding
outputs for several different NLP problems, and so we can ﬁne-tune an LLM to handle multiple
tasks simultaneously.
Let x = x0 ▷▷▷xm be an input sequence (e.g., instruction + user input) and y = y1 ▷▷▷yn be the
corresponding output sequence. In SFT, we aim to maximize the probability of the output y given
the input x. Consider an LLM with pre-trained parameters θ̂. The ﬁne-tuning objective can then
be formulated as:

θ̃ = arg max
θ̂ +
(x,y)∈D
log Prθ̂+ (y|x)
(4.1)
where θ̃ denotes the parameters optimized via ﬁne-tuning, and θ̂+ represents an adjustment to θ̂.
Here we will omit the superscript + and use θ to represent θ̂+ to keep the notation uncluttered. But
the reader should keep in mind that the ﬁne-tuning starts from the pre-trained parameters rather
than randomly initialized parameters.
The objective function log Prθ (yi |x, y<i ) is computed by summing the log-probabilities of
the tokens in y, conditional on the input x and all the previous tokens y<i :
log Prθ (y|x) =
n

i=1
log Prθ (yi |x, y<i )
(4.2)
This formulation is equivalent to minimizing the cross-entropy loss.
Note that minimizing the conditional log-probability log Prθ (y|x) is not a standard language
model training problem. If we concatenate x and y as a single sequence, a more general form of
language modeling is based on the joint log-probability log Prθ (x, y), that is, we minimize the
loss over all tokens of the sequence seqx,y = [x, y]. We can write the probability of this sequence
using the chain rule
log Prθ (seq x,y ) = log Prθ (x, y)
= log Prθ (x) + log Prθ (y|x)


set to 0




(4.3)
loss computation
There are two terms on the right-hand side of the equation. We can simply set the ﬁrst term
log Prθ (x) to 0, focusing solely on the second term log Prθ (y|x) for loss computation. As a
result, the training can be implemented using standard LLMs. For the sequence seqx,y , we ﬁrst
run the forward pass as usual. Then, during the backward pass, we force the loss corresponding
to x to be zero. Figure 4.2 shows an illustration of this process.
By taking log Prθ (seq x,y ) as the objective function, we can describe SFT using a regular form
of language model training:
θ̃ = arg max
θ

(x,y)∈D
log Prθ (seq x,y )
(4.4)
The problem we considered above is fundamentally a single-round prediction problem, where
the LLM generates a response based on a single input without any further interaction or feedback
from the user. The input is processed, and the output is generated in one go. This is typical in4.2 Instruction Alignment
159
Loss = 0
Output
Input
Loss ̸= 0
x1x2x3y1y2x1x2x3y1y2
x0x1x2x3y1x0x1x2x3y1
(a) Forward Pass
(b) Backward Pass
Fig. 4.2: Illustration of supervised ﬁne-tuning for LLMs. We concatenate the input and the output into a single se-
quence. During the forward pass, we run the LLM as usual. During the backward pass, we compute the loss only for
the output part and simply set the loss for the input part to 0.
scenarios where a single question is asked, and a single answer is provided, with no follow-up
questions or clariﬁcations. However, in practice, we sometimes have to handle multi-round pre-
diction problems, for example, an LLM engages in a dialogue over multiple turns. In this setting,
the LLM not only generates responses based on the initial input but also incorporates subsequent
inputs that might reﬁne or expand on earlier interactions. For example, we can use the LLM to act
as a healthcare assistant chatbot and have a conversation with the user, like this
User
Chatbot
I’ve been feeling very tired lately.
I’m sorry to hear that. Besides feeling tired, have you noticed any
other symptoms?
UserYes, I’m also experiencing headaches frequently.
ChatbotHow long have these symptoms been going on?
User
Chatbot
User
Chatbot
About a week now.
It might be good to check in with a healthcare professional. Would
you like help setting up an appointment?
Yes, please. Can it be after work hours?
Sure, I can arrange that. There are slots available next Wednesday
and Thursday after 5 PM. Which day works better for you?
...
In this task, there are several rounds of conversation, each involving the generation of a re-
sponse based on the user’s request or question and the conversational history. Suppose we have
K rounds of conversation, denoted by {x1 , y1 , x2 , y2 , ▷▷▷, xK , yK }. Here xk and yk denote the
user request and the response, respectively, for each round k. The log-probability of generating
the response can be written as log Prθ (yk |x1 , y1 , ▷▷▷, xk ). Our goal is then to maximize the sum
of these log-probabilities
θ̃ = arg max
θ
K

k=1
log Prθ (yk |x1 , y1 , ▷▷▷, xk )
(4.5)160
Alignment
A straightforward implementation of this involves calculating the conditional probability for
each k. However, it requires running the LLM K times, each time with an increased conversa-
tional history to make predictions. A more efﬁcient method is to perform loss computation of all
responses in a single run of the LLM. To do this, we represent the conversation as a sequence
seqx1 ,y1 ,▷▷▷,xK ,yK = [x1 , y1 , ▷▷▷, xK , yK ] (or seq for short). The log-probability of this sequence
is given by
log Prθ (seq) = log Prθ (x1 , y1 , ▷▷▷, xK , yK )
= log Prθ (x1 ) + log Prθ (y1 |x1 ) + · · · +


set to 0



loss computation

log Prθ (xK |x1 , y1 , ▷▷▷, yK−1 ) +


set to 0
K 1
1
log Prθ (y |x , y , ▷▷▷, xK )


loss computation


(4.6)
The trick here is that we ignore the loss for generating user inputs, as illustrated in Figure 4.3.
Hence we only compute the probabilities of generating the responses given their conversational
histories, in other words, the value on the right-hand side of Eq. (4.6) is actually equal to the value
on the right-hand side of Eq. (4.5). As with Eq. (4.4), the training of this multi-round prediction
model can be achieved by maximizing the log likelihood over a training dataset D:
θ̃ = arg max
θ

seq∈D
log Prθ (seq)
(4.7)
While implementing the SFT methods introduced above seems trivial as they are fundamen-
tally the same as regular language model training, there are still issues that need to be considered
in practice. For example,
• SFT requires labeled data. This makes SFT quite different from pre-training, where raw text
is used as training data and is readily available. As in other supervised machine learning
problems, data annotation and selection in SFT are not simple tasks. In general, we wish
to develop SFT data that is both substantial in quantity and high in quality, and this data
should be highly relevant to the tasks the LLM will perform. On the other hand, there is
a need to ﬁne-tune LLMs with less data to minimize computational and data construction
costs. Often, the quality of LLMs is highly dependent on the data used in SFT. Thus, such
data must be carefully developed and examined. As we will see in later subsections, SFT
can be more efﬁcient and effective through more advanced techniques for data construction.
• SFT is still computationally expensive for LLMs due to their large size. As a result, main-
taining and updating such models is resource-intensive. For example, applying gradient up-
dates to billions of parameters within an LLM requires signiﬁcant computational power and
memory. This often requires high-performance computing environments, which are costly
to operate. To address these challenges, various optimization strategies, such as pruning,
quantization, and the use of more efﬁcient training algorithms, have been explored. In par-
ticular, there has been signiﬁcant interest in parameter-efﬁcient ﬁne-tuning methods which
are designed to maintain state-of-the-art performance without the need for extensive compu-
tational resources. We have seen in Chapter 3 that applying techniques like soft prompts can4.2 Instruction Alignment
Loss = 0
Prθ (x1 )
x1
161
Loss = 0
Loss ̸= 0
Prθ (y1 |x1 )
y1
Prθ (x2 |x1 , y1 )
x2
Loss ̸= 0
Prθ (y2 |x1 , y1 , x2 )
y2
···
User: I’ve been feeling very tired lately.
Chatbot: I’m sorry to hear that. Besides feeling tired,
have you noticed any other symptoms?
User: Yes, I’m also experiencing headaches frequently.
Chatbot: How long have these symptoms been going on?
..
.
Fig. 4.3: Illustration of supervised ﬁne-tuning for conversational models. Here the LLM acts as a chatbot to respond to
each request based on the conversational history. The conversation progresses by alternating between the user and the
chatbot. In SFT, we treat the entire conversation as a sequence, just like in standard LLMs, but compute the loss only
for the responses of the LLM.
make the ﬁne-tuning process more efﬁcient. For further discussion on parameter-efﬁcient
methods, the reader can refer to related papers on this issue [Houlsby et al., 2019; Hu et al.,
2022; Han et al., 2024].
• SFT can be regarded as a post-training step following pre-training. It is a separate training
phase designed to preserve the advantages of the initial pre-training while incorporating new
adjustments. This may seem paradoxical because updating a pre-trained LLM with further
data potentially causes the model to forget some of its prior knowledge. Imagine a scenario
where we have a large amount of SFT data and extensively ﬁne-tune the LLM. In this
case, the LLM could overﬁt the data, which in turn may reduce generalization performance
or cause catastrophic forgetting. A common strategy to mitigate this issue is to employ
regularization and early stopping techniques. Another practical approach is to use a smaller
learning rate to gently adjust the weights of the LLM. In addition, ﬁne-tuning with data from
diverse sources and problem domains can also be beneﬁcial. Nevertheless, in practice, the
SFT step is often carefully examined and requires substantial engineering and experimental
efforts to optimize.
4.2.2
Fine-tuning Data Acquisition
Fine-tuning data is so important that much recent work in LLM has focused on developing various
datasets for instruction ﬁne-tuning. As with most work in machine learning, there are generally
two approaches to data acquisition — manual data generation and automatic data generation.162
Alignment
4.2.2.1
Manually Generated Data
One straightforward method is to recruit human annotators to create input-output pairs for the
tasks of interest. Unlike data annotation in conventional NLP, such as text classiﬁcation, where
annotators simply assign labels to collected texts according to guidelines, creating ﬁne-tuning data
for LLMs requires more steps and effort, making it thus more challenging. Suppose we want to
obtain ﬁne-tuning data for the English-to-Chinese machine translation task. The ﬁrst step is to
write a prompt template to describe the task and format the problem clearly. For example,
InstructionTranslate the text from English to Chinese.
User Input{∗text∗}
Output
{∗translation∗}
Then, we collect pairs of source and target texts (i.e., Chinese texts and the corresponding
translations), and replace the variables {∗text∗} and {∗translation∗} to generate the ﬁne-tuning
samples. For example, given a pair of English and Chinese sentences
How’s the weather today?
{∗text∗}
→
今天天气怎么样
{∗translation∗}
we can generate a ﬁne-tuning sample using the prompt template, like this
InstructionTranslate the text from English to Chinese.
User InputHow’s the weather today?
Output
今天天气怎么样
That is,
x = Translate the text from English to Chinese.∩n How’s the weather today?
y =
今天天气怎么样
We can use this (x, y) pair to ﬁne-tune the LLM, as described in the previous subsection.
One difﬁculty here is that there are many, many different ways to write prompt templates
for the same task, and different people may produce prompt templates with varying qualities
and complexities. Sometimes, we may write prompt templates with overly complex or verbose
instructions. Sometimes, we may not even know exactly what the target task is and how to describe
it. A widely-adopted strategy is to create prompt templates for existing NLP tasks, given that there
have been so many well-established NLP problems and benchmarks [Bach et al., 2022; Wang
et al., 2022b; Mishra et al., 2022]. In this case, annotators can be given the original task description
and many examples. Then, they can use their own ways to express how to prompt the LLM to
perform the tasks. Note that, while such a method can ease the process of creating and writing
prompts, we still need annotation frameworks and crowdsourcing systems to manage the work
and conduct quality control. For example, we generally need to design annotation guidelines and4.2 Instruction Alignment
163
a uniﬁed format for writing prompt templates, especially when many annotators are contributing
to the same task. One advantage of inducing prompts from existing NLP tasks is that, once the
prompt templates have been developed, it is easy to generate prompts using the annotated samples
in the original tasks. For example, given a bilingual dataset for English-to-Chinese translation, we
can easily create a number of ﬁne-tuning examples by ﬁlling the slots in the above template with
the sentence pairs in this dataset.
Another approach is to directly use the naturally existing data available on the internet. A
common example is by collecting question-and-answer pairs from QA websites to ﬁne-tune LLMs
for open-domain QA tasks [Joshi et al., 2017]. Many benchmarks in QA are built in this way
because there are so many types of questions that it is impossible to think of them all by a small
group of people. Instead, using data from those websites can ensure that the LLM ﬁne-tuning data
is at a good or acceptable level in terms of quantity and quality.
In addition to employing existing resources, another straightforward way to develop a ﬁne-
tuning dataset is to crowdsource the data. A simple approach is to allow users to input any ques-
tion, after which responses are either manually given or automatically generated by an LLM and
then manually annotated and corrected. It is thus possible to capture real user behavior and conse-
quently gather inputs and outputs for a large number of “new” problems that traditional NLP tasks
do not cover.
An issue related to the construction of the ﬁne-tuning datasets is that we usually want the
data to be as diverse as possible. Many studies have found that increasing the diversity of ﬁne-
tuning data can improve the robustness and generalization ability of LLMs. For this reason, there
has been considerable interest in involving more diverse prompts and tasks in LLM ﬁne-tunining
datasets. We will provide further discussion on the generalization of ﬁne-tuning in Section 4.2.4.
4.2.2.2
Automatically Generated Data
One limitation of manual data generation is that the quality and diversity largely depend on human
experience and creativity. Therefore, if we want LLMs to handle a broad range of tasks, that
is, to effectively execute any instruction, relying on human-annotated data for LLM ﬁne-tuning
is often inefﬁcient. Moreover, the coverage of such data can be limited, and the data may even
contain biases introduced by the annotators themselves. An alternative approach is to generate data
automatically. For example, we can collect a number of questions through crowdsourcing, and
employ a well-tuned LLM to generate answers to the questions. These question-answer pairs are
then used as ﬁne-tuning samples as usual. This method, though very simple, has been extensively
applied to generate large-scale ﬁne-tuning data for LLMs.
The above way of producing synthetic ﬁne-tuning data is similar to those used in data aug-
mentation for NLP. If we have an LLM, we can produce a prediction in response to any input.
Repeating this process for different inputs allows us to create a sufﬁcient number of ﬁne-tuning
samples. Such a method is particularly useful for ﬁne-tuning new LLMs using a well-tuned LLM.
However, one disadvantage of this approach is that it relies on human-crafted or collected inputs
for data generation, which may turn out to be inappropriate for generalizing LLMs. In many LLM
applications, a signiﬁcant challenge arises from the broad range of users’ questions and requests,
many of which are not covered in existing NLP tasks and datasets. In these cases, it becomes
necessary to generate not only the predictions but also the inputs themselves.
Here we consider self-instruct as an example to illustrate how to generate LLM ﬁne-tuning164
Alignment
Initialization
Initialize the task pool with a number of instructions
and corresponding input-output samples.
Sample 1: (Instruction, User-input, Output)
Sample 2: (Instruction, User-input, Output)
Task Pool
···
Draw a few instructions from the pool
Sampling
Instructiona
Instructionb
sampling
Task Pool
Instructionc
Instruction
Generation
Sample
Generation
Filtering
Prompt the LLM to generate a new instruction based on
the drawn instructions.
You are provided several different instructions for performing
some tasks. Please generate an instruction based on these.
Task 1: Instructiona
Task 2: Instructionb
Task 3: Instructionc
New Task: Instructionnew
Given the newly-generated instruction and a few
input-output samples, generate a new sample.
You are provided with a set of input-output samples tasks,
each composed of an instruction, a user input, and an output.
Please generate a new sample based on these.
Sample 1: Samplea
Sample 2: Sampleb
New Sample: Instructionnew User-inputnew Outputnew
Filter out invalid and low-quality samples.
Add the remaining samples into the pool.
Fig. 4.4: Illustration of self-instruct [Wang et al., 2023b]. This method maintains a pool of instructions and correspond-
ing input-output samples. Initially, the pool contains a number of hand-crafted instructions and samples. Each time,
we draw a few instructions from the pool. An LLM is then prompted to generate new instructions and samples based
on those drawn. Finally, the newly-generated instructions and samples are ﬁltered and added to the pool.
samples [Wang et al., 2023d; Honovich et al., 2023]. The idea is that we can prompt an LLM to
create a new instruction by learning from other instructions. Given this instruction, the LLM can
then ﬁll in other ﬁelds (such as the user input) and produce the predictions. Figure 4.4 shows a
schematic illustration of self-instruct. Here we give a brief outline of the key steps involved.
• The self-instruct algorithm maintains a pool of tasks. Initially it contains a number of seed
hand-crafted tasks, each with an instruction and input-output sample. As the algorithm
proceeds, LLM-generated instructions and samples will be added to this pool.4.2 Instruction Alignment
165
• At each step, a small number of instructions are drawn from the instruction pool. For ex-
ample, we can randomly select a few human-written instructions and a few LLM-generated
instructions to ensure diversity.
• The selected instructions are then used as demonstration examples. Thus, the LLM can
in-context learn from these examples and produce a new instruction. Below is an example
template for prompting the LLM.
You are provided several different instructions for performing some tasks.
Please generate an instruction based on these.
Task 1: {instruction1}
Task 2: {instruction2}
Task 3: {instruction3}
Task 4: {instruction4}
New Task:
• Given the generated instruction, the LLM is then prompted to complete the sample by ﬁlling
in the remaining input ﬁelds and generating the corresponding output. Below is a prompt
template.
You are provided with a set of input-output samples, each composed of
an instruction, a user input, and an output. Please generate a new sample
based on these.
Sample 1: {instruction1}
Input: {user-input1}
Output: {output1}
Sample 2: {instruction2}
Input: {user-input2}
Output: {output2}
New Sample: {new-instruction}
• This newly-generated sample is examined by some heuristic rules (such as ﬁltering out
samples or instructions that are similar to those already in the pool). If it passes, the sample
and instruction are added to the pool.
This generation process can be repeated many times to obtain a sufﬁcient number of ﬁne-
tuning samples. Note that, above, we just show simple prompt templates for generating instruction
and ﬁne-tuning samples. Of course, we can develop better templates to generate more diverse and
accurate instruction and ﬁne-tuning samples. For example, for certain tasks like text classiﬁcation,166
Alignment
the LLM may tend to produce biased predictions, for example, most generated samples belong to
a single class. In such cases, we can adjust the order of generation of different ﬁelds. More
speciﬁcally, we can specify the output (i.e., the class) with some prior, and prompt the LLM
to generate user input given both the instruction and the output. This method resembles input
inversion, where the LLM generates the input based on the speciﬁed output [Longpre et al., 2023].
Using LLM-generated instructions and ﬁne-tuning samples has been a common method for
developing LLMs, especially given that manually developing such data is so expensive that most
research groups cannot afford it. In several well-tuned LLMs, their ﬁne-tuning datasets include
a certain amount of synthetic data, which has proved useful [Ouyang et al., 2022; Taori et al.,
2023; Chiang et al., 2023]. There have been further studies on synthetic data generation for LLM
ﬁne-tuning. For example, one can generate more diverse instructions by introducing evolutionary
algorithms [Xu et al., 2024], or use synthetic data as supervision signals in a more advanced ﬁne-
tuning process [Chen et al., 2024b]. More recently, there has also been considerable interest in
using synthetic data in the pre-training stage [Gunasekar et al., 2023; Allal et al., 2024].
In many applications, a real-world scenario is that, given a task, we can collect or annotate a
relatively small amount of ﬁne-tuning data, for example, we can recruit experts to create questions
for QA tasks in a speciﬁc domain. But the quantity and diversity of this data are in general not
sufﬁcient. In this case, we can use self-instruct techniques to generate more diverse question-
answer pairs, and thus augment the ﬁne-tuning data. This provides a way of bootstrapping the
LLM starting from a seed set of ﬁne-tuning samples. Note that using self-generated data is a com-
mon practice and has long been applied in NLP. For example, this approach has been successfully
used in parsing and machine translation [Charniak, 1997; Sennrich et al., 2016].
4.2.3
Fine-tuning with Less Data
With the increasing prominence of instruction ﬁne-tuning, there has been a surge in demand for
large-scale, high-quality ﬁne-tuning data. For example, the FLAN ﬁne-tuning dataset, which
is compiled from 1,836 tasks, contains 15 million samples [Longpre et al., 2023]. Fine-tuning
LLMs with such large datasets is typically a computationally expensive task, especially given
that updating the large number of parameters in LLMs is resource-intensive. One approach for
mitigating this issue is to explore efﬁcient model training methods, for example, one can use
parameter-efﬁcient methods to update only a small portion of the model. However, many ﬁne-
tuning datasets contain a large amount of synthetic data, where errors and biases are still inevitable.
Another approach to efﬁcient ﬁne-tuning is to consider only the most relevant and impactful
examples for ﬁne-tuning. We can thus reduce the amount of data that needs to be processed
while still maintaining the quality of the model updates. There are several methods to achieve
this. For example, Zhou et al. [2023a] built an instruction-following dataset containing only 1,000
samples by carefully crafting the prompts and collecting samples from a variety of NLP tasks.
They showed that the LLaMa 65B model ﬁne-tuned with this dataset could be competitive with
or even better than models with much more ﬁne-tuning effort. This suggests that LLMs can
be adapted to respond to diverse tasks without necessarily needing ﬁne-tuning on all types of
instruction-following data. Chen et al. [2024a] developed a system based on the GPT-3.5 model
to assess the quality of each instruction-following sample. Therefore, they could select high-
quality samples from existing datasets, showing better ﬁne-tuning performance with fewer ﬁne-
tuning samples. Researchers have also developed methods to either select or ﬁlter out data using4.2 Instruction Alignment
167
heuristics [Zhao et al., 2024; Ge et al., 2024], or to prioritize data that more signiﬁcantly inﬂuences
the ﬁne-tuning process [Xia et al., 2024]. In fact, most of these methods can be seen as instances
of larger families of data selection and ﬁltering methods. And it is often the case that using higher
quality (but maybe less) data is beneﬁcial for training NLP models.
The discoveries in instruction ﬁne-tuning somewhat differ from traditional views in NLP: the
ability of models to handle complex problems can be activated with a small amount of annotated
data, rather than requiring massive amounts of supervised data for extensive training. One possible
explanation is that the ability of generating correct responses given instructions has been learned
during pre-training, but such instruction-response mappings are not with high probabilities during
inference. Fine-tuning can slightly adjust the models to get them to follow instructions, requiring
signiﬁcantly less training effort than pre-training. This is closely related to what is known as
the superﬁcial alignment hypothesis, which suggests that learning primarily occurs during pre-
training, and the subsequent ﬁne-tuning or alignment phase does not signiﬁcantly contribute to the
underlying knowledge base of an LLM [Zhou et al., 2023a]. Since the core abilities and knowledge
of the model are already established from pre-training, effective ﬁne-tuning for alignment with
user needs can be achieved with relatively small training ﬁne-tuning effort. This implies the
possibility of ﬁne-tuning LLMs with very little data. In another direction, it may not be necessary
to restrict ﬁne-tuning to paired instruction-response data. For example, Hewitt et al. [2024] found
that instruction-following can be implicitly achieved by ﬁne-tuning LLMs only on responses,
without corresponding instructions.
A concept related to the discussion here is sample efﬁciency. A machine learning method
is called sample efﬁcient if it can learn effectively from a small number of training examples.
In this sense, instruction ﬁne-tuning is sample efﬁcient compared with pre-training. From the
perspective of machine learning, sample-efﬁcient methods can be seen as efﬁcient ways to sample
the space of data, and are advantageous as they make optimal use of scarce data. Therefore,
sampling-based learning techniques, such as many reinforcement learning algorithms, can beneﬁt
from these sample efﬁcient approaches. For example, in human preference alignment, we can
either efﬁciently sample preference data via reward models [Liu et al., 2024b] or improve the
sampling efﬁciency in policy learning [Wang et al., 2024].
4.2.4
Instruction Generalization
In many machine learning and NLP problems, training a model to generalize is a fundamental
goal. For example, in text classiﬁcation, we expect our model to correctly classify new texts that
were not seen during training. However, generalization poses additional challenges in instruction
ﬁne-tuning. We expect instruction-ﬁne-tuned LLMs to not only generate appropriate responses for
different inputs within a task but also to accurately perform various tasks as described by different
instructions. To illustrate this issue, consider an LLM Pr(y|c, z), where c is an instruction, z
is a user input, and y is the corresponding model output (i.e., the response). Suppose that the
performance of this model is evaluated in terms of a metric, written as Performance(Pr(y|c, z))
or P(c, z, y) for short. Informally, when we say this model can generalize within a given task
(indicated by the instruction c∗ ), we mean that there may be a value ϵ such that the average
performance on new inputs is above this value:
1 
P(c∗ , z′ , y′ ) > ϵ
|Z| z′ ∈Z
(4.8)168
Alignment
where Z is the set of new inputs, and z′ and y′ are an input in this set and the corresponding
output, respectively.
Likewise, we can say that this model can generalize across tasks if the average performance
over all instruction-input pairs is above some ϵ:

1
P(c′ , z′ , y′ ) > ϵ
|D| (c′ ,z′ )∈D
(4.9)
where D is the set of new instruction-input pairs.
Here, we need to deal with variations in two dimensions: instruction and user input. This
makes the generalization problem very complex, because, intuitively, a model needs to learn from
a vast number of tasks and different input-output pairs associated with each task to achieve good
generalization. As we have discussed several times in this book, achieving such generalization
incurs much lower cost than pre-training. In general, ﬁne-tuning LLMs with instruction-response
data to some extent can lead to models yielding instruction following on new tasks. Nevertheless,
it is typically believed that certain efforts are still needed to adapt LLMs to make them understand
and execute instructions broadly.
One way to generalize instruction ﬁne-tuning is to increase the diversity of the ﬁne-tuning
data. In earlier studies on instruction ﬁne-tuning, researchers developed many datasets, covering
a wide variety of NLP tasks and different instructions for each task [Wang et al., 2022b; Sanh
et al., 2022; Longpre et al., 2023]. By transforming these tasks into a uniﬁed format, one can ﬁne-
tune an LLM with a sufﬁciently large number of samples, for example, there have been several
instruction ﬁne-tuning datasets that involve over 100 NLP tasks and 1M samples. However, these
early datasets mostly focus on existing academic problems, but not those that users want to deal
with in real-world applications. Much recent work has shifted focus to addressing new and more
practical problems. For example, there has been considerable interest in constructing datasets
that contain large and complicated demonstrations and responses from SOTA models to real user
queries [Wang et al., 2023c; Teknium, 2023].
Perhaps the use of large and diverse ﬁne-tuning datasets has its origins in attempts to scale
LLMs in different dimensions. Indeed, scaling laws have been used broadly to motivate the de-
velopment of a wide range of different instruction-ﬁne-tuned LLMs. And it is reasonable to scale
instruction ﬁne-tuning to make an LLM follow broad instructions. From the perspective of LLM
alignment, however, scaling instruction ﬁne-tuning might not be efﬁcient to achieve generaliza-
tion.
One problem is that instruction ﬁne-tuning relies on supervised learning that learns to gener-
alize and perform tasks based on instruction-response mappings. However, such an approach does
not capture subtle or complex human preferences (e.g., tone, style, or subjective quality) because
these are hard to encode as explicit instruction-response data. Moreover, the generalization per-
formance is bounded by the diversity and quality of the instruction-response dataset. Given these
limitations, we would instead like to employ preference models as an additional ﬁne-tuning step
following instruction ﬁne-tuning, so the LLMs can generalize further (see Section 4.3).
Another view is that some instruction-response mappings may already be learned during pre-
training, and so the pre-trained LLMs have encoded such mappings. However, since we often do
not know exactly what data is used in the pre-training, it is hard to judge whether we need to learn
such mappings in the ﬁne-tuning. A related question is whether out-of-distribution generalization4.2 Instruction Alignment
169
is primarily achieved during pre-training or ﬁne-tuning. While directly answering this question is
beyond the scope of this chapter, it has been shown that pre-training on large and diverse datasets
is effective in improving out-of-distribution performance [Hendrycks et al., 2020; Radford et al.,
2021; Gunasekar et al., 2023]. This raises an interesting problem: if an LLM has been well pre-
trained at scale, ﬁne-tuning may not be as essential for out-of-distribution generalization, since the
model may have already encountered sufﬁcient distributional variation. This prompts researchers
to ﬁne-tune LLMs with modest effort or to explore new methods to achieve instruction-following.
As discussed in the previous sub-section, for example, instruction following can be yielded by
ﬁne-tuning on a small number of carefully selected instruction-response pairs [Zhou et al., 2023a],
or even by using methods that are not explicitly designed to do so [Kung and Peng, 2023].
The above discussion provides two different strategies: one requires scaling up ﬁne-tuning
datasets for larger diversity, the other requires small but necessary ﬁne-tuning datasets for efﬁcient
LLM adaptation. However, in practice, involving diverse instructions often helps. In many cases,
we need to adapt our LLM for speciﬁc purposes. But the LLM, which has possibly encoded broad
instruction-following mappings during pre-training, might tend to behave as a general-purpose
instruction executor even with modest ﬁne-tuning. An interesting phenomenon is that when ﬁne-
tuning on math data, the resulting LLM might not specialize in math outputs. Instead, this model
might respond normally to general instructions, for example, it could generate poetry if instructed
to do so [Hewitt, 2024]. This is not a bad thing, but it shows that LLMs may not easily change their
nature of following general instructions. In this case, additional adaptations with more diverse
data may help adjust the way the LLM follows instructions, particularly for those tasks we aim to
address.
4.2.5
Using Weak Models to Improve Strong Models
So far we have explored a variety of instruction ﬁne-tuning methods based on labeled data. One
of the limitations of many such methods is that they require the data to be annotated by humans or
generated by strong LLMs, which can provide accurate supervision signals in ﬁne-tuning. How-
ever, in many cases, the LLM we have in hand is already strong (or at least is advantageous in
speciﬁc aspects of problem solving), and thus it is not easy to ﬁnd a superior model for supervi-
sion. Even for human experts, when the problem becomes complex, providing correct and detailed
answers might be difﬁcult, or sometimes infeasible. For example, when faced with an extremely
long document, the experts would ﬁnd it challenging to identify any inconsistencies, subtle biases,
or missing key points without conducting an exhaustive and time-consuming review.
One may ask at this point: can we use weak LLMs to supervise strong LLMs? This seems
to be a signiﬁcant challenge, but it may reﬂect a future scenario where we need to supervise AI
systems that are smarter than humans or any other AI systems [Burns et al., 2023b]. The problem
of using smaller, less complex models to improve the training of larger, more complex models
is also called the weak-to-strong generalization problem. While there have not been mature
approaches to weak-to-strong generalization, using smaller models to assist stronger models has
indeed proven useful in several areas of LLMs.
For instruction ﬁne-tuning, one of the simplest ways of applying weak LLMs is to use these
models to generate synthetic ﬁne-tuning data. Suppose we have a collection of inputs X, where
each input includes an instruction and a user input if necessary. For each x ∈ X, we use a weak
LLM Prw (·) to generate a prediction ŷ = arg maxy Prw (y|x). Then, the strong LLM Prsθ (·) can170
Alignment
be trained on these generated predictions (see Eq. (4.1)):
θ̃ = arg max
θ

x∈X
log Prsθ (ŷ|x)
(4.10)
where θ is the model parameters.
The above form transforms the ﬁne-tuning problem into a knowledge distillation problem, in
other words, we distill knowledge from the weak model to the strong model. Consequently, we
can employ various knowledge distillation methods to achieve this goal. However, explaining
weak-to-strong ﬁne-tuning from the perspective of knowledge distillation is not straightforward.
A major concern is that the strong model may merely imitate or overﬁt the errors of the weak
model and fail to generalize. For example, the ﬁne-tuned strong model still cannot solve difﬁcult
problems that the weak model cannot accurately predict. Fortunately, preliminary experiments in
this line of research have shown positive and promising results. For example, Burns et al. [2023a]
found that ﬁne-tuning the strong pre-trained GPT-4 model with GPT-2-level supervision could
improve generalization across several NLP tasks. To measure how the weak model improves the
generalization of the strong model, we deﬁne the following terms:
• Weak Performance (Pweak ). This is the test-set performance of the weak model, which
can be regarded as the baseline performance.
• Weak-to-strong Performance(Pweak→strong ). This is the test-set performance of the strong
model that is ﬁne-tuned with the weak model.
• Strong Ceiling Performance (Pceiling ). This is the test-set performance of the strong model
that is ﬁne-tuned with ground truth data. For example, we ﬁne-tune the strong model with
human-annotated predictions and take the resulting model as a ceiling.
Then, the performance gap recovered (PGR) can be deﬁned as

PGR = max 0,
Pweak→strong − Pweak 
Pceiling − Pweak
(4.11)
This metric measures how much of the performance gap between the ceiling model and the
weak model can be recovered by the weak-to-strong model. A PGR of 1 indicates that the weak-
to-strong ﬁne-tuning can completely closes the performance gap, whereas a PGR of 0 indicates
no improvement. In Burns et al. [2023a]’s work, it is shown that PGR can be around 0▷8 on 22
NLP classiﬁcation tasks. It should be noted that, while the potential of weak-to-strong ﬁne-tuning
is promising, achieving substantial weak-to-strong generalization remains a challenging goal that
needs further investigation [Aschenbrenner, 2024].
Fine-tuning LLMs with weak supervision is just one choice for using small models to improve
large models. Although this section primarily focuses on ﬁne-tuning LLMs, we also mention
other methods here to give a more complete discussion (see Figure 4.5 for illustrations of these
methods).
• Instead of using small models to generate synthetic data, it is also straightforward to in-
corporate knowledge distillation loss based on these models. For example, a simple loss4.2 Instruction Alignment
171
Compute Loss & TrainCompute Loss & Train
ŷy
Small Model
Input
Predict
Dataset
x ŷ
=⇒
LM Loss
KD Loss
Small Model
Large Model
Dataset
=⇒
x y
Large Model
x
x
(a) Fine-tuning on data generated by a small model
(weak-to-strong generalization)
(b) Fine-tuning with KD Loss from a small model
(weak-to-strong generalization)
Dataset
y
Compute Loss & Train
Small Model
Dataset
x y
=⇒
Combination Model
y
Data
Selection
Large Model
Small Model 1Small Model 2Small Model 3
xxx
x
(d) Ensemble of multiple small models
(c) Data selection with a small model
If Step 1 is not satisfactory, go to Step 2
Step 1
Step 2
(cheap)
(expensive)
y2
y1
Large Model
Small Model
x
x
(e) Cascading (at inference time)
Fig. 4.5: Illustrations of using small models to improve large models in LLMs. One approach involves using smaller
models for the ﬁne-tuning or pre-training of larger models. This includes generating synthetic data (a), incorporating
auxiliary loss (b), and selecting appropriate data (c). Another approach involves combining small models and large
models. This includes learning a strong model by aggregating multiple small models (d), and cascading small models
with large models (e).
function that measures the difference between the small and large models can be deﬁned as:
Losskd = KL(Prw (·|x) || Prsθ (·|x))
(4.12)
Then, we can add this loss to the original loss of language modeling, and yield the following
training objective
θ̃ = arg max
θ

(x,y)∈D
log Prsθ (y|x) − λ · Losskd
(4.13)172
Alignment
where D is the set of input and output pairs, and λ is the coefﬁcient of the interpolation. This
method can be employed in either the pre-training or ﬁne-tuning phase. We can adjust λ to
control how much the small model inﬂuences the training. For example, we can gradually
decrease λ to make the training rely more on the original language modeling loss as the
large model becomes more capable.
• Another approach to involving small models in LLM pre-training and ﬁne-tuning is to use
them to do data selection or ﬁltering. Given a sequence, we can compute the likelihood
or cross-entropy using a small model. These quantities can then be used as criteria for
selecting or ﬁltering data. For example, sequences with low likelihood or high cross-entropy
might be excluded from the training set, as they are less aligned with the small model’s
learned distribution. Conversely, sequences with high likelihood or low cross-entropy can
be prioritized, ensuring that the training focuses on more relevant or high-quality data.
• Ensemble learning is a simple and effective way to build a strong model by combining mul-
tiple weak models. Applying this technique to LLMs is straightforward. We can aggregate
distributions predicted by multiple small models or specialized submodels, and derive the
ﬁnal prediction from the aggregated results. This aggregation can be done using methods
such as majority voting, weighted averaging, or stacking.
• Small models can also be employed at inference time to improve overall efﬁciency. Suppose
we have a large model that is slow but more accurate, and a small model that is fast but
less accurate. In model cascading, the small model ﬁrst processes the input data, quickly
generating preliminary results. If these results meet certain pre-deﬁned criteria, they can be
directly used. However, if the initial results are not sufﬁciently good, the input is then passed
to the larger, more accurate model to produce a better result. This approach signiﬁcantly
reduces computational costs and latency, as the small model can effectively handle many
inputs without access to the large model.
4.3
Human Preference Alignment: RLHF
So far in this chapter, we have focused on ﬁne-tuning LLMs using input-output paired labeled data.
This approach allows us to adapt LLMs for instruction-following via supervised learning. In many
applications, however, LLMs are required not only to follow instructions but also to act in ways
that are more aligned with human values and preferences. Consider a scenario where a user asks an
LLM how to hack into a computer system. If the LLM is not appropriately aligned, it may respond
by providing details on how to perform this illegal activity. Instead, a more desirable response
might be to advise the user against engaging in illegal activities and offer a general overview of
the consequences of such actions. The difﬁculty in achieving this is that the ethical nuances and
contextual considerations required for an LLM to respond appropriately in such scenarios are not
always straightforward to encode into a ﬁne-tuning dataset. What’s even more challenging is that,
often, humans themselves cannot precisely express their own preferences.
In this section, we discuss an alternative LLM ﬁne-tuning method, called reinforcement learn-
ing from human feedback or RLHF for short [Christiano et al., 2017; Stiennon et al., 2020]. The
basic idea behind RLHF is that LLMs can learn from comparisons of model outputs using reward4.3 Human Preference Alignment: RLHF
173
y
Objective (MLE):
Gold-standard Predictions
max Pr(y|x)
(n one-hot distributions)
LLM
Predicted Token Distributions
where
x: input
y: gold-standard output
(n token distributions)
x
(a) Supervised ﬁne-tuning (maximizing the prediction probability given the input)
Generate multiple
outputs via sampling
Human preference data
train
Reward Model
yy21
LLM
Prediction y1
Prediction y2
x
Objective (RL Loss Minimization):
min L(x, {y1 , y2 }, r)
where
L(·): loss function
r(·): reward model
(b) Reinforcement Learning from Human Feedback
Fig. 4.6: Supervised ﬁne-tuning vs. reinforcement learning from human feedback. In supervised ﬁne-tuning, we
optimize the LLM by maximizing the probability of the prediction given the input. In reinforcement learning from
human feedback, we ﬁrst train a reward model on human preference data (on each pair of predictions, evaluators are
asked to choose which one they prefer). Then, we use this reward model to supervise the LLM during ﬁne-tuning.
models (see Figure 4.6). To do this, we can recruit human experts who indicate their preferences
between pairs of outputs generated by the LLM. This preference data is used to train a reward
model that can predict the perceived quality of LLM outputs. Once trained, the reward model
provides feedback by assigning scores to new outputs that the LLM generates in response to the
inputs. The LLM uses these scores to update its parameters through reinforcement learning algo-
rithms. In the rest of this section, we will ﬁrst introduce the basic knowledge of reinforcement
learning to facilitate the discussion, and then discuss methods for training reward models and
aligning LLMs with these models.
4.3.1
Basics of Reinforcement Learning
We begin by looking at some basic concepts of reinforcement learning. Note that the notation used
here slightly differs from that used in the previous sections and chapters because we want to make
our description more consistent with those in the reinforcement learning literature. Nevertheless,
we will show how this notation corresponds to the language modeling notation. The reader who
is already familiar with reinforcement learning techniques may skip or skim this subsection.
A general reinforcement learning framework describes how an agent interacts with a dynamic
environment. This interaction is modeled as a sequence of actions taken by the agent in response
to the state of the environment. At each time step, the agent observes the current state, chooses an
action based on its policy, performs the action, and then receives feedback from the environment
in the form of a reward and a new state. This sequence of observe-act-receive feedback is repeated174
Alignment
until the agent achieves its goal.
A reinforcement learning system involves several components:
• Agent. This is the learner or decision-maker in reinforcement learning. In the context of
LLMs, it can be seen as the LLM itself.
• Environment. This includes everything external to the agent with which the agent interacts.
But the environment in LLMs is less about a physical or virtual space and more about the
framework within which the agent (e.g., an LLM) receives feedback and learns.
• State (s). A state represents the current situation of the environment. Given a sequence of
tokens for language modeling, a state at a time step can be viewed as the tokens we observed
so far, that is, the context tokens we take to predict the next token. For example, we can
deﬁne (x, y<t ) as the state when predicting the next token at the time step t.
• Action (a). Actions represent possible decisions the agent can make. We can see them as
possible predicted tokens in the vocabulary.
• Reward (R). The reward is the feedback from the environment that evaluates the success
of an action. For example, r(s, a, s′ ) denotes the reward the agent receives for taking the
action a at the state s and moving to the next state s′ . If the state-action sequence is given,
we can denote the reward at the time step t as rt = r(st , at , st+1 ). Also note that if the
decision-making process is deterministic, we can omit st+1 because it can be determined
by st and at . In such cases, we can use r(st , at ) as shorthand for r(st , at , st+1 ).
• Policy (π). For an LLM, a policy is deﬁned as the probability distribution over the tokens
that the LLM predicts, given the preceding context tokens. Formally, this can be expressed
as
π(a|s) = Pr(yt |x, y<t )
(4.14)
where a corresponds to the token yt , and s corresponds to the context (x, y<t ). Figure 4.7
illustrates how an LLM can be treated as a policy in the reinforcement learning framework.
• Value Function (V and Q). A state-value function (or value function, for short) assesses
the expected discounted return (i.e., accumulated rewards) for an agent starting from a par-
ticular state s and following a speciﬁc policy π. It is deﬁned as:


V (s) = E r(s0 , a0 , s1 ) + γr(s1 , a1 , s2 ) + γ 2 r(s2 , a2 , s3 ) + · · ·  s0 = s, π


= E r0 + γr1 + γ 2 r2 + · · ·  s0 = s, π
= E
∞

t=0

γ t rt  s0 = s, π



(4.15)
where γ ∈ [0, 1] is the discount factor that adjusts the importance of future rewards, s0 = s
indicates that the agent starts with the state s, and the expectation E is performed over all
possible trajectories (i.e., state-action sequences). Similarly, an action-value function (or4.3 Human Preference Alignment: RLHF
175
Feedback
Value Functions
Action at
y1
y2
▷▷▷
ytV ( st ) and Q( st , at )
yt−1Reward Model
Policy (LLM)
x0
x1
▷▷▷
xm
y1
▷▷▷
R( st , at )
State st (x and y<t )
Fig. 4.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a token yt given the model
input x and the previously-generated tokens y<t . This process can be framed as a reinforcement learning problem,
where yt serves as the action, (x, y<t ) as the state, and the predicted distribution Pr(yt |x, y<t ) as the policy. Once
yt is predicted, the LLM inputs both (x, y<t ) and yt to the reward model, which evaluates how effectively the chosen
token contributes to achieving the desired textual outcome. This evaluation generates reward scores which are used to
compute the value functions V (st ) and Q(st , at ). These functions then provide feedback to the LLM and guide the
policy training.
Q-value function) measures the expected return starting from a state s taking an action a
and thereafter following a policy π, given by
Q(s, a) = E
∞

t=0

γ t rt  s0 = s, a0 = a, π

(4.16)
where a0 = a indicates that the action taken at the initial state is a.
The goal of reinforcement learning is to learn a policy that maximizes the cumulative re-
ward (or return) the agent receives over the long run. Given a state-action sequence τ =
{(s1 , a1 ), ▷▷▷, (sT , aT )}1 , the cumulative reward over this sequence can be written as
T

R(τ ) =
(4.17)
rt
t=1
The expectation of this cumulative reward over a space of state-action sequences is given in
the form
J(θ) =
=
E
τ ∼D



τ ∈D
1

Prθ (τ )R(τ )
τ ∈D
=

R(τ ) πθ
Prθ (τ )
T

rt
(4.18)
t=1
We assume the state-action sequence begins with s1 and a1 , rather than s0 and a0 , to align with the notation
commonly used in this chapter, where the prediction y typically starts from y1 . Of course, it is also common to denote
a state-action sequence as {(s0 , a0 ), ▷▷▷, (sT , aT )} or {(s0 , a0 ), ▷▷▷, (sT −1 , aT −1 )} in the literature. But this variation
in notation does not affect the discussion of the models presented here.176
Alignment
where τ ∼ D indicates that τ is drawn from the state-action sequence space D, and the subscript
θ indicates the parameters of the policy. J(θ) is also called the performance function.
Then the training objective is to maximize J(θ):
θ̃ = arg max J(θ)
(4.19)
θ
Now, we have a simple reinforcement learning approach: 1) we sample a number of state-
action sequences; then, 2) we evaluate each sequence using the performance function; then, 3) we
update the model to maximize this performance function. If we take Eq. (4.18) and use gradient
descent to optimize the policy, this approach would constitutes a form of policy gradient methods
[Williams, 1992].
Note that in many NLP problems, such as machine translation, rewards are typically sparse.
For instance, a reward is only received at the end of a complete sentence. This means that rt = 0
for all t < T , and rt is non-zero only when t = T . Ideally, one might prefer feedback to
be immediate and frequent (dense), and thus the training of the policy can be easier and more
efﬁcient. While several methods have been proposed to address sparse rewards, such as reward
shaping, we will continue in our discussion to assume a sparse reward setup, where the reward is
available only upon completing the prediction.
The model described in Eqs. (4.17-4.19) establishes a basic form of reinforcement learning,
and many variants and improvements of this model have been developed. Before showing those
more sophisticated models, let us take a moment to interpret the objective function J(θ) from the
perspective of policy gradient. In gradient descent, we need to compute the gradient of J(θ) with
respect to θ:
∂J(θ)
∂θ
∂
=

τ ∈D Prθ (τ )R(τ )
∂θ
 ∂Prθ (τ )
=
τ ∈D

=
∂θ
Prθ (τ )∂Prθ (τ )◁∂θ
R(τ )
Prθ (τ )
Prθ (τ )∂ log Prθ (τ )
R(τ )
∂θ
τ ∈D

=
τ ∈D
R(τ )
(4.20)
In some cases, we will assume that every sequence in D is equally probable (i.e., Prθ (τ ) =
Prθ (τ )
1◁|D|). In this case we can simplify Eq. (4.20) and need only consider the terms ∂ log∂θ
and
R(τ ):
∂J(θ)
∂θ
=
1  ∂ log Prθ (τ )
R(τ )
m τ ∈D
∂θ
(4.21)
One advantage of this result is that R(τ ) does not need to be differentiable, which means that we
can use any type of reward function in reinforcement learning.
By treating the generation of the sequence τ as a Markov decision process, we can further4.3 Human Preference Alignment: RLHF
177
Prθ (τ )
derive ∂ log∂θ
, and obtain:
∂ log Prθ (τ )
∂θ
T

∂
log
πθ (at |st ) Pr(st+1 |st , at )
∂θ
t=1
=
T
T
∂ 
∂ 
log πθ (at |st ) +
log Pr(st+1 |st , at )

 ∂θ



∂θ t=1 
t=1
=
policy
(4.22)
dynamics
where the gradient is decomposed into two parts: the policy gradient and the dynamics gradient.
The policy component, log πθ (at |st ), determines the log-probability of taking action at given
state st , and it is parameterized by θ. The dynamics component, log Pr(st+1 |st , at ), represents
the log-probability of transitioning to state st+1 from state st after taking action at . In typical
reinforcement learning settings, the dynamics are not directly inﬂuenced by the policy parameters
θ, and thus, their derivatives are often zero. In this case, therefore, Eq. (4.22) can be simpliﬁed to:
∂ log Prθ (τ )
∂θ
=
T
∂ 
log πθ (at |st )
∂θ t=1
(4.23)
In other words, we only concentrate on optimizing the policy without concerning ourselves with
the underlying dynamics.
Substituting Eq. (4.23) into Eq. (4.21), and expanding R(τ ), we then obtain
∂J(θ)
∂θ
=
T
T


1  ∂ 
log πθ (at |st )
rt
|D| τ ∈D ∂θ t=1
t=1
(4.24)
While this policy gradient approach is straightforward, it suffers from the problem that the
variance of the estimated gradients can be very high, making the learning process noisy and inef-
ﬁcient. One reason for this high variance problem is that rewards can vary greatly across different
steps or scenarios. Imagine that in a sequence of action decisions, the reward model tends to assign
small rewards to good actions (e.g., Rt = 2) and large penalties to poor actions (e.g., Rt = −50).
Such varying reward scales for good and poor actions can result in a very low total reward for the
entire sequence, even if it includes good actions.
One simple method for reducing the variance of the gradient is to set a baseline b and subtract


it from Tt=1 rt , resulting in Tt=1 rt − b.2 Here, the baseline can be interpreted as a reference
point. By centering the rewards around this baseline, we remove systematic biases in the reward
signal, making the updates more stable and less sensitive to extreme ﬂuctuations in individual
rewards.
2
T
In fact, the use of a baseline b does not change the variance of the total rewards t=1 rt . However, it is important
to note that while introducing a baseline does not alter the overall variance of the rewards, it helps reduce the variance
of the gradient estimates. This is because subtracting the baseline from the total rewards effectively
T reduces ﬂuctuations
around their mean, which makes the gradient estimates more stable. In general, the operation t=1 rt − b centers the
T
rewards around zero (e.g., b is deﬁned as the expected value of t=1 rt ), which can lead to reduced variance in the
T
T
product t=1 log πθ (at |st )( t=1 rt − b).178
Alignment
This policy gradient model with a baseline can be given by
∂J(θ)
∂θ
T
T
 

1  ∂ 
log πθ (at |st )
rt − b
|D| τ ∈D ∂θ t=1
t=1
=
T
T


1  ∂ 
log πθ (at |st )
rk − b
|D| τ ∈D ∂θ t=1
k=1
=
T
t−1
T



1  ∂ 
log πθ (at |st )
rk +
rk − b
|D| τ ∈D ∂θ t=1
k=1
k=t
=


(4.25)

T
Here we write Tk=1 rk as the sum of two terms t−1
k=t rk to distinguish between the
k=1 rk and
rewards accrued before and after the action at time step t. Note that in Markov decision processes,
the future is independent of the past given the present. Therefore, the action taken at time step t
cannot inﬂuence the rewards received before t, or in other words, the rewards prior to t are already

“ﬁxed” by the time the action at t is chosen. The term t−1
k=1 rk does not contribute to the gradient
and can be omitted, leading to a simpliﬁed version of Eq. (4.25)
∂J(θ)
∂θ
Also note that removing
=
T
T


1  ∂ 
log πθ (at |st )
rk − b
|D| τ ∈D ∂θ t=1
k=t
(4.26)
T
k=t rk can further reduce the variance of the gradient.
There are many ways to deﬁne the baseline b. Here we consider the value function of the state
st , that is, the estimated value of being in state st : V (st ) = E(rt + rt+1 + · · · + rT ). Hence we
have
A(st , at ) =
=
T

k=t
T

k=t
rk − b
rk − V (st )
(4.27)

where Tk=t rk represents the actual return received, and V (st ) represents the expected return.
A(st , at ) (or At for short) is called the advantage at time step t, which quantiﬁes the relative
beneﬁt of the action at compared to the expected value of following the policy from the state st
onward.
By using the advantage function A(st , at ), the gradient of J(θ) can be written in the form
∂J(θ)
∂θ
=
T

1  ∂ 
log πθ (at |st )A(st , at )
|D| τ ∈D ∂θ t=1
(4.28)
This optimization objective corresponds to the advantage actor-critic (A2C) method in re-
inforcement learning [Mnih et al., 2016]. In this method, the actor aims at learning a policy. It
updates the policy parameters using Eq. (4.28) to help focus more on actions that are likely to
improve performance. The critic, on the other hand, updates its estimation of the value function,
which is used to calculate the advantage function A(st , at ), thus serving as the evaluator of the4.3 Human Preference Alignment: RLHF
179
policy being learned by the actor.
In the A2C method, A(st , at ) is typically expressed as the difference of the action-value func-
tion Q(st , at ) and the state-value function V (st )
A(st , at ) = Q(st , at ) − V (st )
(4.29)
At ﬁrst glance, this model may seem challenging to develop because it requires two separate sub-
models to calculate Q(st , at ) and V (st ) respectively. Fortunately, considering that Q(st , at ) can
be deﬁned as the return rt + V (st+1 ), we can rewrite Eq. (4.29) as
A(st , at ) = rt + V (st+1 ) − V (st )
(4.30)
or alternatively, introduce the discount factor γ to obtain a more general form
A(st , at ) = rt + γV (st+1 ) − V (st )
(4.31)
A(st , at ) = rt + γV (st+1 ) − V (st ) is also called the temporal difference (TD) error. What
we need is to train a critic network for the value function V (st ), and then use it to compute the
advantage function3 .
Up to this point, we have spent considerable space discussing the basics of reinforcement
learning, especially on how to derive the optimization objective for the A2C method. However,
reinforcement learning is a vast ﬁeld, and many technical details cannot be covered here. The in-
terested reader can refer to reinforcement learning books for more details [Sutton and Barto, 2018;
Szepesvári, 2010]. Nevertheless, we now have the necessary knowledge to further discuss RLHF.
In the subsequent subsections, we will return to the discussion on LLM alignment, demonstrating
how to use the A2C method for aligning with human preferences.
4.3.2
Training Reward Models
We have shown that reward models play a very important role in the general reinforcement learn-
ing framework and form the basis for computing value functions. We now consider the problem
of training these reward models.
In RLHF, a reward model is a neural network that maps a pair of input and output token
sequences to a scalar. Given an input x and an output y, the reward can be expressed as
r = Reward(x, y)
(4.33)
where Reward(·) is the reward model. r can be interpreted as a measure of how well the output y
aligns with the desired behavior given the input x. As discussed in the previous subsection, both x
3
The training loss for the value network (or critic network) in A2C is generally formulated as the mean squared
error between the computed return rt + γV (st+1 ) and the predicted state value V (st ). Suppose that the value network
is parameterized by ω. The loss function is given by
Lv (ω)
=
2
1 
rt + γVω (st+1 ) − Vω (st )
M
where M is the number of training samples, for example, for a sequence of T tokens, we can set M = T .
(4.32)180
Alignment
Reward (Scalar)
Wr
Representation
hx0
hx1
hx2
at Each Position
···
hxm
h y1
h y2
···hlast
···yn
Linear Map
Transformer Decoder (LLM)
x0
x1
x2
···
xm
y1
y2
(Last Token ⟨EOS⟩)
Fig. 4.8: Architecture of the reward model based on Transformer. The main component of this model is still an LLM.
We use the Transformer decoder as the sequence representation model. We extract the representation of the last position
of the decoder as the representation of the entire sequence [x, y]. We then map this representation to a scalar through a
linear transformation, which serves as the reward score for [x, y].
and y are assumed to complete texts. This means that the reward model evaluates the relationship
between inputs and outputs that provide full semantic content. For example, when applying the
reward model, it assigns a value of 0 (or another predetermined value) at each position t in the
output sequence y = y1 ▷▷▷yn . Only at the ﬁnal position, when t = n, does the reward model
generate the actual reward score. To keep the notation uncluttered, we will use r(x, y) to denote
the reward model Reward(x, y) from here on.
There are many ways to implement the reward model. One simple approach is to build the
reward model based on a pre-trained LLM. More speciﬁcally, we can concatenate x and y to form
a single token sequence seqx,y = [x, y]. We run a pre-trained LLM on this sequence, as usual,
and at each position, we obtain a representation from the top-most Transformer layer. Then, we
take the representation at the last position (denoted by hlast ) and map it to a scalar via linear
transformation:
r(x, y) = hlast Wr
(4.34)
where hlast is a d-dimensional vector, and Wr is a d × 1 linear mapping matrix. This architecture
of the reward model is illustrated in Figure 4.8.
To train the reward model, the ﬁrst step is to collect human feedback on a set of generated
outputs. Given an input x, we use the LLM to produce multiple candidate outputs {y1 , ▷▷▷, yN }.
Human feedback can be obtained in several ways:
• Pairwise Comparison (Pairwise Ranking). Given two different outputs, human experts
select which one is better.
• Rating. Human experts provide a score or rating to each output. This score is often a
continuous or discrete numerical value, such as a score on a scale (e.g., 1-5 stars, or 1-10
points). In some cases, the rating might be binary, indicating a “yes/no” or “positive/nega-
tive” preference.4.3 Human Preference Alignment: RLHF
181
• Listwise Ranking. Human experts are asked to rank or order the given set of possible
outputs.
Here we consider pairwise comparison feedback as it is one of the simplest and most com-
mon forms of human feedback used in RLHF. In this setting, each time, two outputs (ya , yb ) are
randomly drawn from the candidate pool {y1 , ▷▷▷, yN }. Human experts are then presented with
these pairs and asked to decide which output they prefer based on speciﬁc criteria, such as clarity,
relevance, and accuracy. The human feedback can be encoded as a binary label, ya ≻ yb for a
preference for ya , and yb ≻ ya for a preference for yb .
One simple and widely used model for describing such pairwise comparisons is the Bradley-
Terry model [Bradley and Terry, 1952]. It is a probabilistic model that estimates the probability
that one item is preferred over another. Adapting this model to the notation used here, we can
write the probability that ya is preferred over yb in the form
er(x,ya )
er(x,ya ) + er(x,yb )
er(x,ya )−r(x,yb )
= r(x,y )−r(x,y )
a
b + 1
e
= Sigmoid(r(x, ya ) − r(x, yb ))
Pr(ya ≻ yb |x) =
(4.35)
When training the reward model, we want to maximize this preference probability. A loss
function based on the Bradley-Terry model is given by

Lr (ϕ) = −E(x,ya ,yb )∼Dr log Prϕ (ya ≻ yb |x)

(4.36)
where (x, ya , yb ) is drawn from a human-annotated dataset Dr consisting of preference pairs of
outputs and their corresponding inputs. ϕ represents the parameters of the reward model, which
includes both the parameters of the Transformer decoder and the linear mapping matrix Wr . In
practice, assuming (x, ya , yb ) is uniformly sampled from Dr , we can replace the expectation with
a summation
Lr (ϕ) = −

1
log Prϕ (ya ≻ yb |x)
|Dr | (x,y ,y )∈D
a
b
(4.37)
r
The goal of training the reward model is to ﬁnd the optimal parameters ϕ̂ that minimize this
loss function, given by
ϕ̂ = arg min Lr (ϕ)
(4.38)
ϕ
Since the reward model itself is also an LLM, we can directly reuse the Transformer training
procedure to optimize the reward model. The difference from training a standard LLM is that we
only need to replace the cross-entropy loss with the pairwise comparison loss as described in Eq.
(4.37). After the training of the reward model, we can apply the trained reward model rϕ̂ (·) to
supervise the target LLM for alignment.
It is worth noting that although we train the reward model to perform pairwise ranking, we
apply it to score each input-output pair independently during the alignment process. The pairwise182
Alignment
ranking objective ensures that the reward model is sensitive to subtle differences between outputs,
but we rely on the continuous scores produced by the reward model to guide the optimization of
the LLM. An advantage of this approach is that we can choose from or combine various ranking
loss functions, and still apply the resulting reward models in the same way as we have done in this
subsection. This consistency ensures a uniﬁed framework for aligning the LLM, regardless of the
speciﬁc ranking loss used during reward model training.
4.3.3
Training LLMs
Having obtained the reward model, we then train the policy (i.e., the LLM) via the A2C method.
Recall from Section 4.3.1 that a state-action sequence or trajectory τ can be evaluated by the utility
function
U (τ ; θ) =
T

t=1
log πθ (at |st )A(st , at )
(4.39)
where A(st , at ) is the advantage of taking the action at given the state st . An estimate of A(st , at )
is deﬁned as the TD error rt + γV (st+1 ) − V (st ), where the value function V (st ) is trained with
the reward model.
Given this utility function, the A2C-based loss function can be written in the form

L(θ) = −Eτ ∼D U (τ ; θ)
= −Eτ ∼D
T

t=1

log πθ (at |st )A(st , at )

(4.40)
where D is a space of state-action sequences. As usual, the goal of training the policy is to
minimize this loss function
θ̃ = arg min L(θ)
(4.41)
θ
If we map the problem back to the language modeling problem and adopt the notation from
LLMs, the loss function can be written as:

L(θ) = −E(x,y)∼D U (x, y; θ)

(4.42)
where
U (x, y; θ) =
T

t=1
log πθ (yt |x, y<t )A(x, y<t , yt )
Here πθ (yt |x, y<t ) = Prθ (yt |x, y<t ) is the LLM parameterized by θ.
(4.43)
In general, we do not have a human annotated input-output dataset D in RLHF, but a dataset
containing inputs only. The outputs, in this case, are typically the predictions made by the LLM.4.3 Human Preference Alignment: RLHF
183
The loss function is then deﬁned as

L(θ) = −Ex∼D Ey∼πθ (·|x) U (x, y; θ)

(4.44)
where D denotes the input-only dataset, and y ∼ πθ (·|x) denotes that the output y is sampled by
the policy πθ (·|x).
The above formulation provides a basic form of the A2C method for LLMs. Improved versions
of this model are more commonly used in RLHF. In the following discussion, we will still use
the reinforcement learning notation to simplify the presentation and will get back the language
modeling notation later.
One common improvement of policy gradient methods is to use importance sampling to
reﬁne the estimation of U (τ ; θ). This can be written as
T

πθ (at |st )
U (τ ; θ) =
t=1
πθref (at |st )
A(st , at )
(4.45)
Here we replace the log-probability log πθ (at |st ) with the ratio ππθ θ (a(at |st |st )t ) . θref denotes the pa-
ref
rameters of the previous policy (such as an initial model from which we start the training). So
πθ (at |st )
πθref (at |st ) , also called the ratio function, can be interpreted as the log-probability ratio between
the current policy πθ and the previous policy πθref (call it the reference policy). By using the
ratio function we reweight the observed rewards based on the likelihood of the actions under the
current policy versus the reference policy. When ππθ θ (a(at |st |st )t ) > 1, the action at is more favored by
ref
the current policy compared to the reference policy. By contrast, when ππθ θ (a(at |st |st )t ) < 1, the action
ref
at is less favored by the current policy4 .
4
Consider a more general case where we wish to evaluate the policy using its expected reward (also see Eq. (4.18))
=
J(θ)

Eτ ∼πθ R(τ )

(4.46)
Here τ ∼ πθ means that the sequence τ is generated by the policy πθ . Alternatively, we can write J(θ) in another form
J(θ)
=
Eτ ∼πθref
 Pr (τ )
θ
Prθref (τ )
R(τ )

(4.47)
I