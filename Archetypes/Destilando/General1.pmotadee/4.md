t is not
that the right-hand sides of these equations are essentially
 difﬁcult to ﬁnd

 the same since


Prθ (τ )
Prθ (τ )
Eτ ∼πθref Prθ (τ ) R(τ ) = τ Prθref (τ ) Prθ (τ ) R(τ ) = τ Prθ (τ )R(τ ) = Eτ ∼πθ R(τ )
ref
ref
Note that this equivalence holds only when the expectation is performed over the entire sequence space. In practice,
however, we often only sample a relatively small number of sequences using a policy in policy learning. As a result,
the sampling method itself matters. Eq. (4.47) offers an interesting manner to separate the sampling and reward
computation processes: we ﬁrst use a baseline policy (with θref ) to sample a number of sequences, and then use the
target policy (with θ) to compute the expected reward. In this way, we separate the policy used for collecting the data,
and the policy used for computing the gradient. This approach avoids the need to directly sample from the policy we are
evaluating, which can be beneﬁcial in
sequences from the target policy is expensive or difﬁcult.
 cases where generating

In reinforcement learning, Eτ ∼πθref
Prθ (τ )
R(τ )
Prθ
(τ )
ref
is often called a surrogate objective.
Eq. (4.47) can also be interpreted from a policy gradient perspective. For Eτ ∼πθref
θ = θref is given by


Prθ (τ )
∂

Eτ ∼πθref
R(τ ) 
∂θ
Prθref (τ )
θ=θref
=
Eτ ∼πθref

 ∂Pr (τ )|
θ
∂θ
Prθ (τ )
R(τ )
Prθ
(τ )
ref
θ=θref
R(τ )


, the gradient at
(4.48)
The right-hand side is a standard form used in policy gradient methods, meaning that we compute the direction of184
Alignment
A problem with the model presented in Eq. (4.47) (as well as in Eq. (4.39)) is that the
variance in the gradient estimates is often high, making the learning process unstable. To mitigate
this issue, techniques such as clipping are often employed to bound the importance weights and
prevent large updates. A clipped version of the utility function (also called the clipped surrogate
objective function) is given by
Uclip (τ ; θ) =
Clip
 π (a |s ) 
θ
t
t
πθref (at |st )
T

Clip
t=1
= min
 π (a |s ) 
t
θ
t
πθref (at |st )
 π (a |s )
θ
t
t
πθref (at |st )
A(st , at )
, bound
 πθ (at |st )
πθref (at |st )
(4.49)
, 1 − ϵ, 1 + ϵ

(4.50)
Here the function bound( ππθ θ (a(at |st |st )t ) , 1 − ϵ, 1 + ϵ) constrains the ratio function to the range [1 −
ϵ, 1 + ϵ].
ref
A further improvement to the above model is to consider trust regions in optimization [Schul-
man et al., 2015]. In reinforcement learning, a large update to the policy can lead to instability,
where the agent may start performing worse after an update. A reasonable idea is to optimize the
model in the trust region, which refers to a region around the current parameter estimate where
the model is well-behaved. One approach to incorporating trust regions is to impose a constraint
on the size of the policy update, ensuring that the current policy does not deviate too signiﬁcantly
from the reference policy. This can be achieved by adding a penalty based on some form of di-
vergence between the current and reference policies to the objective function. A simple form of
such a penalty is given by the difference in the log-probability of the sequence τ under the current
policy versus the reference policy:
Penalty = log πθ (τ ) − log πθref (τ )
(4.51)
In practice, this penalty can be approximated by considering only the policy probabilities and
ignoring the dynamics. This gives
Penalty =
T

t=1
log πθ (at |st ) −
T

t=1
log πθref (at |st )
(4.52)
By including this penalty in the optimization objective, we encourage the current policy to remain
close to the reference policy, limiting very large updates that could destabilize learning.
We can incorporate this penalty into the clipped surrogate objective function, and obtain
Uppo-clip (τ ; θ) = Uclip (τ ; θ) − βPenalty
(4.53)
where β is the weight of the penalty. This training method is called proximal policy optimization
(PPO), which is one of the most popular reinforcement learning methods used in LLMs and many
other ﬁelds [Schulman et al., 2017].
Now we can write the objective of training LLMs in the form of PPO.
the parameter update at the point θ = θref on the optimization surface.4.3 Human Preference Alignment: RLHF
185
U (x, y; θ) = Uppo-clip (x, y; θ) − βPenalty
(4.54)
where
Uppo-clip (x, y; θ) =
T

t=1
Clip
 π (y |x, y
θ
t
<t )
πθref (yt |x, y<t )

A(x, y<t , yt )
(4.55)
Penalty = log Prθ (y|x) − log Prθref (y|x)
=
T

t=1
log Prθ (yt |x, y<t ) −
T

t=1
log Prθref (yt |x, y<t )
(4.56)
Although the notation here appears a bit tedious, the idea of PPO is simple: we develop an
objective by combining the clipped likelihood ratio of the target and reference policies with an
advantage function, and then impose a penalty that ensures policy updates are not too large. The
PPO-based RLHF is illustrated in Figure 4.9.
To summarize, implementing RLHF requires building four models, all based on the Trans-
former decoder architecture.
• Reward Model (rϕ (·) where ϕ denotes the parameters). The reward model learns from
human preference data to predict the reward for each pair of input and output token se-
quences. It is a Transformer decoder followed by a linear layer that maps a sequence (the
concatenation of the input and output) to a real-valued reward score.
• Value Model or Value Function (Vω (·) where ω denotes the parameters). The value func-
tion receives reward scores from the reward model and is trained to predict the expected
sum of rewards that can be obtained starting from a state. It is generally based on the same
architecture as the reward model.
• Reference Model (πθref (·) = Prθref (·) where θref denotes the parameters). The reference
model is the baseline LLM that serves as a starting point for policy training. In RLHF, it
represents the previous version of the model or a model trained without human feedback. It
is used to perform sampling over the space of outputs and contribute to the loss computation
for policy training.
• Target Model or Policy (πθ (·) = Prθ (·) where θ denotes the parameters). This policy
governs how the LLM decides the most appropriate next token given its context. It is trained
under the supervision of both the reward model and the value model.
In practice, these models need to be trained in a certain order. First, we need to initialize them
using some other models. For example, the reward model and the value model can be initialized
with a pre-trained LLM, while the reference model and the target model can be initialized with a
model that has been instruction ﬁne-tuned. Note that, at this point, the reference model is ready for
use and will not be further updated. Second, we need to collect human preference data and train the
reward model on this data. Third, both the value model and the policy are trained simultaneously
using the reward model. At each position in an output token sequence, we update the value model186
Alignment
Human preference data Dr = {(x, ya , yb )}
To Learn
Reward Model
Training
Minimizing the loss based on
the Bradley-Terry model

1
Reward Model
rϕ (x, y)
min − |D |
r
ϕ
(x,ya ,yb )∈Dr
To Learn
Fixed
Ref Model
To Learn
LLM Policy
Prθold (yt |x, y<t )
log σ(rϕ (x, ya ) − rϕ (x, yb ))
Value Function
Prθ (yt |x, y<t )
Vω (x, y<t )
Evaluate the state-action pair using the advantage
function or the TD error (based on the reward
model and the value function)
x1
x2
y1
···
y2
···yt
···yt−1
Action yt
(sampled with Prθ
old
)
LLM Policy
x0
Policy Training
x1
xm
···
y1
State (x, y<t )
LLM PolicyValue Function
Minimizing the clipped PPO loss
with the penaltyMinimizing the MSE between the
computed return and the predicted
state value
min −

θ
Clip

x∈D,y∼Prθ
old
(·|x)
 Prθ (yt |x,y<t ) 

Prθ
old
(yt |x,y<t )
T
t=1
At −
β · log Prθ (yt |x, y<t )−
log Prθold (yt |x, y<t )

1
min M

ω

x∈D
T
t=1
rt + γVω (x, y<t+1 ) − Vω (x, y<t )
2
∗∗ rt = r(x, y<t+1 ) denotes the reward received as step t.
∗∗ At denotes the advantage at step t, and can be deﬁned as rt + γVω (x, y<t+1 ) − Vω (x, y<t )
Input-only data D = {x}
Fig. 4.9: Illustration of RLHF. The ﬁrst step is to collect human preference data and train the reward model using this
data. Once the reward model is optimized, along with the reference model, we proceed to train both the policy and
the value function. At each prediction step, we compute the sum of the PPO-based loss and update the parameters of
the policy. This requires access to the reward model, the reference model, and the value function at hand. At the same
time, we update the parameters of the value function by minimizing the MSE loss.
by minimizing the MSE error of value prediction, and the policy is updated by minimizing the
PPO loss.4.4 Improved Human Preference Alignment
4.4
187
Improved Human Preference Alignment
In the previous section, we reviewed the basic concepts of reinforcement learning and the general
framework of RLHF. In this section, we will discuss some reﬁnements of RLHF and alternative
methods to achieve human preference alignment.
4.4.1
Better Reward Modeling
In Section 4.3.2, we highlighted the task of learning from human preferences as well as the use
of pairwise ranking loss for training reward models. Here we consider more methods for reward
modeling. Our discussion will be relatively general, and since the reward model is widely used in
many reinforcement learning problems, it will be easy for us to apply the methods discussed here
to RLHF and related applications.
4.4.1.1
Supervision Signals
The training of reward models can broadly be seen as a ranking problem, where the model learns
to assign scores to outputs so that their order reﬂects the preferences indicated by humans. There
are several methods to train a reward model from the perspective of ranking.
One approach is to extend pairwise ranking to listwise ranking. For each sample in a dataset,
we can use the LLM to generate multiple outputs, and ask human experts to order these outputs.
For example, given a set of four outputs {y1 , y2 , y3 , y4 }, one possible order of them can be
y2 ≻ y3 ≻ y1 ≻ y4 . A very simple method to model the ordering of the list is to accumulate the
pairwise comparison loss. For example, we can deﬁne the listwise loss by accumulating the loss
over all pairs of outputs:
Llist = −E(x,Y )∼Dr



1
log Pr(ya ≻ yb |x)
N (N − 1) y ∈Y,y ∈Y
a
(4.57)
b
yb
ya =
where Y is a list of outputs, and N is the number of outputs in the list. Pr(ya ≻ yb |x) can be
deﬁned using the Bradley-Terry model, that is, Pr(ya ≻ yb |x) = Sigmoid(r(x, ya ) − r(x, yb )).
Here we omit the ϕ superscript on the Pr(·) to keep the notation uncluttered.
An extension to the Bradley-Terry model for listwise ranking could involve a ranking mecha-
nism that takes into account the entire list of outputs rather than just pairwise comparisons. One
such model is the Plackett-Luce model, which generalizes the Bradley-Terry model to handle
multiple items in a ranking [Plackett, 1975]. In the Plackett-Luce model, for each item in a list,
we deﬁne a “worth” for this item that reﬂects its relative strength of being chosen over other items.
For the reward modeling problem here, the worth of y in the list Y can be deﬁned as
α(y) = exp(r(x, y))
(4.58)188
Alignment
Then the probability of selecting y from Y is given by
Pr(y is selected|x, Y ) =
=


α(y)
′
y′ ∈Y α(y )
exp(r(x, y))
′
y′ ∈Y exp(r(x, y ))
(4.59)
Suppose Y̊ is an ordered list yj1 ≻ yj2 ≻ · · · ≻ yjN . The overall log-probability of this
ordered list can be deﬁned as the sum of the conditional log-probabilities at each stage of selection,
given by
log Pr(Y̊ |x) = log Pr(yj1 ≻ yj2 ≻ · · · ≻ yjN |x)
= log Pr(yj1 |x, {yj1 , yj2 , ▷▷▷, yjN }) +
log Pr(yj2 |x, {yj2 , ▷▷▷, yjN }) +
··· +
log Pr(yjN |x, {yjN })
=
N

k=1
log Pr(yjk |x, Y̊≥k )
(4.60)
where Y̊≥k represents the subset of the list of outputs that remain unselected at the k-th stage, i.e.,
Y̊≥k = {yjk , ▷▷▷, yjN }. Given the log-probability log Pr(Y̊ |x), we can deﬁne the loss function
based on the Plackett-Luce model by

Lpl = −E(x,Y̊ )∼Dr log Pr(Y̊ |x)

(4.61)
There are also many other pairwise and listwise methods for modeling rankings, such as
RankNet [Burges et al., 2005] and ListNet [Cao et al., 2007]. All these methods can be cate-
gorized into a large family of learning-to-rank approaches, and most of them are applicable to the
problem of modeling human preferences. However, discussing these methods is beyond the scope
of this chapter. Interested readers can refer to books on this topic for more details [Liu, 2009; Li,
2011].
In addition to pairwise and listwise ranking, using pointwise methods to train reward models
offers an alternative way to capture human preferences. Unlike methods that focus on the relative
rankings between different outputs, pointwise methods treat each output independently. For ex-
ample, human experts might assign a score to an individual output, such as a rating on a ﬁve-point
scale. The objective is to adjust the reward model so that its outputs align with these scores. A
simple way to achieve pointwise training is through regression techniques where the reward of
each output is treated as a target variable. Let φ(x, y) be the score assigned to y given x by
humans. Pointwise reward models can be trained by minimizing a loss function, often based on
mean squared error or other regression losses, between the predicted reward r(x, y) and the actual
human feedback φ(x, y). For example, the loss function could be

Lpoint = −E φ(x, y) − r(x, y)
2
(4.62)
While pointwise methods are conceptually simpler and can directly guide the reward model to4.4 Improved Human Preference Alignment
189
predict scores, they might not always be the best choice in RLHF. A problem is that these methods
may struggle with high variance in human feedback, especially when different experts provide
inconsistent scores for similar outputs. Because they focus on ﬁtting to absolute scores rather than
relative differences, inconsistencies in scoring can lead to poor model performance. Moreover,
ﬁtting to speciﬁc scored outputs might discourage generalization, particularly given that training
data is often very limited in RLHF. In contrast, methods that consider relative preferences can
promote the learning of more generalized patterns of success and failure. Nevertheless, there are
scenarios where pointwise methods might still be suitable. For example, in tasks where training
data is abundant and the costs of obtaining accurate, consistent annotations are low, pointwise
methods can prove effective.
In fact, to make the supervision signal for training the reward model more robust, we can also
introduce additional regularization terms into training. For example, if we consider the ﬁrst term
Uppo-clip (x, y; θ) in Eq. (4.54) as a type of generalized reward, then the second term (i.e., the
penalty term) can be viewed as a form of regularization for the reward model, except that here
the goal is to train the policy rather than the reward model. Another example is that Eisenstein
et al. [2023] develop a regularization term based on the squared sum of rewards, and add it to the
pairwise comparison loss in RLHF:

2
Lreg = Lpair + (−E(x,ya ,yb )∼Dr r(x, ya ) + r(x, yb ) )

= −E(x,ya ,yb )∼Dr log Prϕ (ya ≻ yb |x)

−E(x,ya ,yb )∼Dr r(x, ya ) + r(x, yb )

2
(4.63)
Optimizing with this regularization term can help mitigate the underdetermination of reward mod-
els5 .
4.4.1.2
Sparse Rewards vs. Dense Rewards
As discussed in Section 4.3, the rewards in RLHF are very sparse: they are observed only at the
end of sequences, rather than continuously throughout the generation process. Dealing with sparse
rewards has long been a concern in reinforcement learning, and has been one of the challenges in
many practical applications. For example, in robotics, it often needs to shape the reward function
to ease optimization rather than relying solely on end-of-sequence rewards. Various methods
have been developed to address this issue. One common approach is reward shaping, where the
original function is modiﬁed to include intermediate rewards, thereby providing more immediate
feedback. Also, one can adopt curriculum learning to sequentially structure tasks in a way that the
complexity gradually increases. This can help models to master simpler tasks ﬁrst, which prepares
them for more complex challenges as their skills develop. There are many such methods that can
mitigate the impact of sparse rewards, such as Monte Carlo methods and intrinsic motivation. Most
of these methods are general and the discussion of them can be found in the broader literature on
reinforcement learning, such as Sutton and Barto [2018]’s book.
Although we do not discuss methods for mitigating sparse rewards in detail here, an interesting
question arises: why are sparse rewards so successful in RLHF? Recall from Section 4.3.1 that
the supervision signal received at each time step t is not the reward for the current action, but
5
A model is called underdetermined if there are multiple alternative sets of parameters that can achieve the same
objective.190
Alignment
rather some form of the accumulated rewards from t until the last time step. Such supervision
signals are dense over the sequence, because the reward obtained at the end of the sequence can
be transferred back to that time step, regardless of which time step it is. In other words, the sparse
rewards are transformed into the dense supervision signals. Furthermore, from the perspective of
reward shaping, Ng et al. [1999] show that the reward at t can be deﬁned as
r′ (st , at , st+1 ) = r(st , at , st+1 ) + f (st , at , st+1 )
(4.64)
where r′ (·) is the transformed reward function, r(·) is the original reward function, and f (·) is
the shaping reward function. To ensure the optimality of the policy under the transformed reward
function, the shaping reward function can be given in the form
f (st , at , st+1 ) = γΦ(st+1 ) − Φ(st )
(4.65)
where Φ(s) is called the potential value of the state s. If we deﬁne Φ(s) as the common value
function as in Eq. (4.15) and substitute Eq. (4.65) into Eq. (4.64), we obtain
r′ (st , at , st+1 ) = r(st , at , st+1 ) + γV (st+1 ) − V (st )
(4.66)
It is interesting to see that this function is exactly the same as the advantage function used in PPO.
This relates advantage-based methods to reward shaping: the advantage is essentially a shaped
reward.
On the other hand, one of the reasons for adopting end-of-sequence rewards lies in the nature
of the RLHF tasks. Unlike traditional reinforcement learning environments where the agent in-
teracts with a dynamic environment, RLHF tasks often involve complex decision-making based
on linguistic or other high-level cognitive processes. These processes do not lend themselves eas-
ily to frequent and meaningful intermediate rewards because the quality and appropriateness of
the actions can only be fully evaluated after observing their impact in the larger context of the
entire sequence or task. In this case, the reward signals based on human feedback, though very
sparse, are typically very informative and accurate. Consequently, this sparsity, together with the
high informativeness and accuracy of the human feedback, can make the learning both robust and
efﬁcient.
4.4.1.3
Fine-grained Rewards
For many applications, our objective will be more complex than merely evaluating an entire text.
For example, in sentiment analysis, we often do not just determine the sentiment of a text, but need
to analyze the sentiment in more detail by associating it with speciﬁc aspects of a topic discussed
in the text. Consider the sentence "The camera of the phone is excellent, but the battery life is
disappointing." In this example, we would need to separately analyze the sentiments expressed
about the camera and the battery. Such analysis, known as aspect-based sentiment analysis, helps
provide a ﬁner-grained understanding of the customer review compared to general sentiment anal-
ysis.
For the problem of reward modeling, we often need to model different parts of a sequence as
well. A simple and straightforward way to do this is to divide a sequence into different segments
and then compute the reward for each segment [Wu et al., 2023b]. Suppose that an output token4.4 Improved Human Preference Alignment
191
sequence y can be divided into ns segments {ȳ1 , ▷▷▷, ȳns } by some criterion. We can use the
reward model to evaluate each of these segments. By taking x, y and ȳk as input to the reward
model, the reward score for the k-th segment is given by
rk = r(x, y, ȳk )
(4.67)
Then the reward score for the entire output sequence is given by
ns

r(x, y) =
r(x, y, ȳk )
(4.68)
k=1
Here r(x, y) can be used to train the policy as usual.
A problem with this model is that training reward models at the segment level is not as straight-
forward as learning from human preferences on entire texts, as it is difﬁcult to obtain segment-level
human preference data. For rating-like problems (e.g., we rate a segment according to its level
of misinformation), one simple approach is to assign a rating score to each segment and train the
reward model using pointwise methods. For example, we can use a strong LLM to rate the se-
quences ȳ1 ▷▷▷ȳk−1 and ȳ1 ▷▷▷ȳk , and obtain the scores s(ȳ1 ▷▷▷ȳk−1 ) and s(ȳ1 ▷▷▷ȳk ). We can then
deﬁne the score of the segment ȳk as the difference between s(ȳ1 ▷▷▷ȳk ) and s(ȳ1 ▷▷▷ȳk−1 )
s(ȳk ) = s(ȳ1 ▷▷▷ȳk ) − s(ȳ1 ▷▷▷ȳk−1 )
(4.69)
Using these segment-level scores, we can train the reward model with a regression loss func-
tion

Lrating = −Eȳk s(ȳk ) − r(x, y, ȳk )
2
(4.70)
Sometimes, alignment can be treated as a classiﬁcation problem, for example, we assess
whether a segment has ethical issues. In this case, the segment can be labeled as ethical or
unethical, either by humans or using additional classiﬁers. Given the label of the segment, we
can train the reward model using some classiﬁcation loss function. For example, suppose that
r(x, y, ȳk ) = 1 if the segment is classiﬁed as unethical, and r(x, y, ȳk ) = −1 otherwise6 . The
hinge loss of training binary classiﬁcation models is given by
Lhinge = max(0, 1 − r(x, y, ȳk ) · r̂)
(4.71)
where r̂ ∈ {1, −1} denotes the ground truth label.
The remaining issue here is how to split y into segments. One approach is to deﬁne a ﬁxed-
length segmentation, where y is divided into equal-length chunks. However, this may not always
be ideal, as the content of the sequence may not align well with ﬁxed boundaries. An alternative
approach is to segment y based on speciﬁc linguistic or semantic cues, such as sentence bound-
aries, topic shifts, or other meaningful structures in the text. Such a segmentation can be achieved
by using linguistic segmentation systems or prompting LLMs to identify natural breaks in the se-
quence. Another approach is to use dynamic segmentation methods based on the complexity of
6
To allow the reward model to output categories, we can replace the linear layer described in Section 4.3.2 with a
Softmax layer.192
Alignment
the sequence. For example, segments could be deﬁned where there is a signiﬁcant change in the
reward score, which might correspond to shifts in the task being modeled.
4.4.1.4
Combination of Reward Models
A reward model can be viewed as a proxy for the environment. Since the true environment is often
too complex or unknown, developing a perfect proxy for the environment is generally not possible.
As a result, over-aligning LLMs with this imperfect proxy might lead to decreased performance,
known as the overoptimization problem [Stiennon et al., 2020; Gao et al., 2023a]7 . We can also
explain this through Goodhart’s law, which states: when a measure becomes a target, it ceases to
be a good measure [Goodhart, 1984].
Addressing the overoptimization problem is not easy, and there is no mature solution yet. The
ideal approach might be to develop an oracle reward model that can perfectly capture the true
objectives of the task and prevent the agent from “tricking”. However, creating such a model is
extremely difﬁcult due to the complexity of the real-world environment, as well as the challenge
of deﬁning all the relevant factors that contribute to the desired outcome. Instead, a more practical
approach is to combine multiple reward models, thereby alleviating the misalignment between
the training objective and the true objective that arises from using a single, speciﬁc reward model
[Coste et al., 2024].
Given a set of reward models, combining them is straightforward, and in some cases, we can
simply treat this problem as an ensemble learning problem. A simple yet common approach is to
average the outputs of these models to obtain a more precise reward estimation:
rcombine =
K
1 
wk · rk (x, y)
K k=1
(4.72)
where rk (·) is the k-th reward model in the ensemble, wk is the weight of rk (·), and K is the
number of reward models. This combined reward can then be used to supervise the training of
a policy. In fact, there are many ways to combine different models, for example, one can make
predictions using Bayesian model averaging or develop a fusion network to learn to combine the
predictions from different models. Alternatively, one can frame this task as a multi-objective
optimization problem, and use multiple reward models to train the policy simultaneously. These
methods have been intensively discussed in the literature on optimization and machine learning
[Miettinen, 1999; Bishop, 2006].
In addition to model combination methods, another important issue is how to collect or con-
struct multiple different reward models. One of the simplest approaches is to employ ensemble
learning techniques, such as developing diverse reward models from different subsets of a given
dataset or from various data sources. For RLHF, it is also possible to construct reward models
based on considerations of different aspects of alignment. For example, we can develop a reward
model to evaluate the factual accuracy of the output and another reward model to evaluate the
7
This problem is also called reward hacking or reward gaming [Krakovna et al., 2020; Skalse et al., 2022; Pan
et al., 2022], which refers to the phenomenon where the agent attempts to trick the reward model but fails to align its
actions with the true intended objectives of the task. Imagine a student who is assigned homework and is rewarded
with points or praise for completing it. The student might then ﬁnd ways to ﬁnish the homework with minimal effort
to maximize the reward, such as copying and pasting solutions from the internet or previous assignments, rather than
solving the problems themselves.4.4 Improved Human Preference Alignment
Value Function
Preference
Data
ya ≻ yb
193
training with MLE
Training
with PPO
Reward Model
Policy
(a) RLHF (PPO)
Preference
Data
ya ≻ yb
training with MLE
Policy
(b) DPO
Fig. 4.10: Standard RLHF (PPO) vs. DPO. In RLHF, the human preference data is used to train a reward model, which
is then employed in training the policy as well as the value function. In DPO, the use of human preference data is more
direct, and the policy is trained on this data without the need for reward model training.
completeness of the output. These two models are complementary to each other, and can be com-
bined to improve the overall evaluation of the output. Another approach is to employ different
off-the-shelf LLMs as reward models. This approach is simple and practical, as there have been
a lot of well-developed LLMs and we just need to use them with no or little modiﬁcation. An
interesting issue, though not closely related to the discussion here, arises: can an LLM that aligns
with other LLMs outperform those LLMs? Probably not at ﬁrst glance. In part, this is because
the target LLM merely imitates other LLMs based on limited supervision and thus cannot capture
well the nuances of the behaviors of these supervisors. However, given the strong generalization
ability of LLMs, this approach can, in fact, be quite beneﬁcial. For example, using open-sourced
or commercial LLMs as reward models has demonstrated strong performance in aligning LLMs,
even achieving state-of-the-art results on several popular tasks [Lambert et al., 2024].
4.4.2
Direct Preference Optimization
Although learning reward models is a standard step in reinforcement learning, it makes the entire
training process much more complex than supervised training. Training a reliable reward model
is itself not an easy task, and a poorly trained reward model can greatly affect the outcome of
policy learning. We now consider an alternative alignment method, called direct preference op-
timization (DPO), which simpliﬁes the training framework by eliminating the need to explicitly
model rewards [Rafailov et al., 2024]. This method directly optimizes the policy based on user
preferences, rather than developing a separate reward model. As a result, we can achieve human
preference alignment in a supervised learning-like fashion. Figure 4.10 shows a comparison of
the standard RLHF method and the DPO method.
Before deriving the DPO objective, let us ﬁrst review the objective of policy training used in
RLHF. As discussed in Section 4.3.3, the policy is typically trained by optimizing a loss function194
Alignment
with a penalty term. The DPO method assumes a simple loss function where the quality of the
output y given the input x is evaluated by the reward model r(x, y). The training objective is thus
given by

θ̃ = arg min Ex∼D Ey∼πθ (·|x) −r(x, y) +β (log πθ (y|x) − log πθref (y|x))
θ


loss



penalty


(4.73)
Note that in this optimization problem, only the term πθ (y|x) depends on the target policy πθ (·).
Both the reward model r(x, y) and the reference model πθref (y|x) are assumed to be ﬁxed given
x and y. This is a strong assumption compared with PPO, but as will be shown later, it simpliﬁes
the problem and crucial for deriving the DPO objective.
Since θ is the variable we want to optimize, we rearrange the right-hand side of Eq. (4.73) to
isolate πθ (y|x) as an independent term:

θ̃ = arg min Ex∼D Ey∼πθ (·|x) β log πθ (y|x) − β log πθref (y|x) − r(x, y)
θ


1
r(x, y)
β
θ

1

= arg min Ex∼D Ey∼πθ (·|x) log πθ (y|x) − log πθref (y|x) exp r(x, y)



β
θ


= arg min Ex∼D Ey∼πθ (·|x) log πθ (y|x) − log πθref (y|x) +
dependent on θ


not dependent on θ
(4.74)

This equation deﬁnes the objective function as the difference between the log-probability dis-
tribution function of y and another function of y. This form of the objective function seems not
“ideal”, as we usually prefer to see the difference between two distributions, so that we can in-
terpret this difference as some kind of divergence between the distributions. A simple idea is
to convert the second term (i.e., log πθref (y|x) exp( β1 r(x, y))) into a log-probability distribution
over the domain of y. If we treat πθref (y|x) exp( β1 r(x, y)) as an unnormalized probability of y,
we can convert it into a normalized probability by dividing it by a normalization factor:
Z(x) =

y
πθref (y|x) exp
1
β
r(x, y)

(4.75)
Hence we can deﬁne a probability distribution by
π (y|x) =
∗

πθref (y|x) exp β1 r(x, y)
Z(x)

(4.76)4.4 Improved Human Preference Alignment
195
We then rewrite Eq. (4.74) as

θ̃ = arg min Ex∼D Ey∼πθ (·|x) log πθ (y|x) − log
θ

− log Z(x)

πθref (y|x) exp β1 r(x, y)
Z(x


= arg min Ex∼D Ey∼πθ (·|x) log πθ (y|x) − log π ∗ (y|x) − log Z(x)
θ


= arg min Ex∼D Ey∼πθ (·|x) log πθ (y|x) − log π ∗ (y|x)
θ


−Ey∼πθ (·|x) log Z(x)




= arg min Ex∼D KL πθ (·|x) || π ∗ (·|x) − log Z(x)
θ



KL divergence



constant wrt. θ




(4.77)
Since log Z(x) is independent of θ, it does not affect the result of the arg minθ operation,
and can be removed from the objective. Now we obtain a new training objective which ﬁnds the
optimal policy πθ by minimizing the KL divergence between πθ (·|x) and π ∗ (·|x)


θ̃ = arg min Ex∼D KL πθ (·|x) || π ∗ (·|x)
θ

(4.78)
Clearly, the solution to this optimization problem is given by
πθ (y|x) = π ∗ (y|x)

πθref (y|x) exp β1 r(x, y))

=
Z(x
(4.79)
Given this equation, we can express the reward r(x, y) using the target model πθ (y|x), the
reference model πθref (y|x), and the normalization factor Z(x):

πθ (y|x)
r(x, y) = β log
+ log Z(x)
πθref (y|x)

(4.80)
This is interesting because we initially seek to learn the policy πθ (·) using the reward model
r(x, y), but eventually obtain a representation of the reward model based on the policy. Given the
reward model deﬁned in Eq. (4.80), we can apply it to the Bradley-Terry model to calculate the
preference probability (also see Section 4.3.2):
Prθ (ya ≻ yb |x) = Sigmoid(r(x, ya ) − r(x, yb ))
 

πθ (ya |x)
+ log Z(x) −
= Sigmoid β log
πθref (ya |x)


πθ (yb |x)
β log
+ log Z(x)
πθref (yb |x)


πθ (ya |x)
πθ (yb |x)
= Sigmoid β log
− β log
πθref (ya |x)
πθref (yb |x)
(4.81)196
Alignment
This formula is elegant because it converts the difference in rewards into the difference in
ratio functions, and we do not need to calculate the value of Z(x). A direct result is that we no
longer need a reward model, but only need the target policy and reference model to calculate the
probability of preferences. Finally, we can train the target policy by minimizing the following
DPO loss function

Ldpo (θ) = −E(x,ya ,yb )∼Dr log Prθ (ya ≻ yb |x)

(4.82)
The form of this loss function is very similar to that used in training reward models in RLHF (see
Eq. (4.36)). But it should be noted that the loss function here depends on the parameters of the
policy (i.e., θ) rather than the parameters of the reward model (i.e., ϕ).
The main advantage of DPO lies in its simplicity and efﬁciency. The DPO objective is very
straightforward — it directly optimizes for preference-based feedback, rather than relying on sep-
arately developed reward models. Moreover, DPO is generally more sample-efﬁcient, as it learns
from a ﬁxed dataset without the need for the computationally expensive sampling process used
in PPO. This makes DPO a popular method for human preference alignment, especially when
developing and applying reward models via reinforcement learning is challenging.
DPO can broadly be viewed as an ofﬂine reinforcement learning method, where the training
data is pre-collected and ﬁxed, and there is no exploration. In contrast, online reinforcement learn-
ing methods like PPO, which require exploring new states through interaction with the environ-
ment (using the reward model as a proxy), also have their unique advantages. One of the beneﬁts
of online reinforcement learning is that it allows the agent to continuously adapt to changes in
the environment by learning from real-time feedback. This means that, unlike ofﬂine methods,
online methods are not constrained by the static nature of pre-collected data and can discover
new problem-solving strategies. In addition, exploration can help the agent cover a wider range of
state-action pairs, thus improving generalization. This could be an important advantage for LLMs,
as generalization is considered a critical aspect in applying such large models.
4.4.3
Automatic Preference Data Generation
Although learning from human preferences is an effective and popular method for aligning LLMs,
annotating preference data is costly. Using human feedback does not only faces the problem of
limited scalability, but it may also introduce bias because human feedback is inherently subjective.
As a result, one can turn to AI feedback methods to address these scalability and consistency issues
without the limitations associated with human annotators.
As with data generation for instruction ﬁne-tuning, generating preference data using LLMs is
straightforward. Given a set of inputs, we ﬁrst use an LLM to generate pairs of outputs. Then, we
prompt the LLM to label the preference between each pair of outputs, along with its corresponding
input. Below is an example of prompting the LLM to generate a preference label for a pair of
consumer service responses.4.4 Improved Human Preference Alignment
197
Consider a customer service scenario where a customer poses a request. You
will review two responses to this request. Please indicate which response is
preferred. Note that a good response should be courteous, clear, and concise. It
should address the customer’s concern directly, provide helpful information or a
solution, and maintain a positive tone.
Request:
Hello, I noticed that my order hasn’t arrived yet, though it was scheduled to
arrive several days ago. Could you please update me on its status? Thank you!
Response A:
I’m very sorry for the delay and understand how disappointing this can be. We’re
doing our best to sort this out quickly for you.
Response B:
Hey, stuff happens! Your package will get there when it gets there, no need to
stress.
Response A is preferred.
Once we collect such preference labels, we can use them, along with the output pair and input,
to train the reward model. Of course, we can consider demonstrating a few examples or using
advanced prompting techniques, such as CoT, to improve labeling performance. For example, we
can include in the prompt an example showing how and why one of the two responses is preferred
based on a CoT rationale.
In addition to preference labels, we can also obtain the probability associated with each label
[Lee et al., 2023]. A simple method is to extract the probabilities for the label tokens, such as “A”
and “B”, from the probabilities output by the LLM. We can then use the Softmax function or other
normalization techniques to re-normalize these probabilities into a distribution over the labels.
These probabilities of preferred labels can serve as pointwise supervision signals for training the
reward model, as discussed in Section 4.4.1.
For data generation, although it is easy to scale up, it is often necessary to ensure the data is
accurate and diverse. Here, the data quality and diversity issues involve not only the labeling of
preferences but also the inputs and outputs of the model. Therefore, we often need to use a variety
of techniques to obtain large-scale, high-quality data. For example, one can generate diverse
model outputs and annotations by using different LLMs, prompts, in-context demonstrations, and
so on [Cui et al., 2024]. Dubois et al. [2024] report that the variability in pairwise preference data
is important for training LLMs from either human or AI feedback.
While learning from AI feedback is highly scalable and generally objective, this method is
more suited to well-deﬁned tasks where objective performance metrics are available. By contrast,
learning from human feedback is more advantageous when aligning AI systems with human val-
ues, preferences, and complex real-world tasks that require understanding of subtle or subjective
context. These methods can be combined to train LLMs that beneﬁt from both human insights
and the scalability of AI feedback.198
Alignment
4.4.4
Step-by-step Alignment
So far, our discussion of alignment has primarily focused on the use of reward models for evalu-
ating entire input-output sequence pairs. These methods can be easily adapted to scenarios where
the correctness of an output can be examined by checking whether the desired result is included.
For example, in the task of calculating a mathematical expression, a reward model can provide
positive feedback if the answer is correct, and negative feedback if the answer is wrong. How-
ever, in many problems that require complex reasoning, simply examining the correctness of the
ﬁnal result is insufﬁcient for learning. Imagine a student who is only given the ﬁnal answer to
a challenging math problem. Knowing whether the ﬁnal answer is right or wrong does not help
the student ﬁgure out where they went wrong and how to calculate the correct answer. A better
approach would be to guide the student with a step-by-step breakdown of the problem-solving
process and encourage understanding of the underlying concepts and logic behind these steps.
In Chapter 3, we studied CoT methods to prompt LLMs to explicitly write out intermediate
steps or the reasoning process needed to reach a conclusion or solve a problem. We saw that
breaking down a problem into smaller parts could make it easier to understand the solution path
and increase the accuracy of the output. These methods can be naturally extended to the alignment
of LLMs, that is, we supervise the model during the intermediate steps of reasoning. Consider a
reasoning task where an LLM produces a sequence of reasoning steps y = {ȳ1 , ▷▷▷, ȳns } for the
given input. The result of the reasoning is assumed to be included in the last step ȳns , and can
be easily veriﬁed. For this reasoning problem, Uesato et al. [2022] categorize LLM ﬁne-tuning
approaches into two classes:
• Outcome-based Approaches. Supervision occurs only when the end result is veriﬁed. This
is a standard method for learning from human feedback we have discussed in this chapter.
For example, the LLM is optimized to maximize some form of the reward r(x, y).
• Process-based Approaches. Supervision is involved in all intermediate steps in addition to
the last step. To do this, we need to develop a model to give a supervision signal at each
step, and develop loss functions that can make use of such supervision signals.
Figure 4.11 shows two LLM outputs for an example math problem. Although the LLM gives
the correct ﬁnal answer in both cases, it makes mistakes during the problem-solving process in the
second output. Outcome-based approaches overlook these mistakes and give positive feedback for
the entire solution. By contrast, process-based approaches can take these mistakes into account
and provide additional guidance on the detailed reasoning steps.
An important issue for process-based approaches is that we need to get step-level feedback
during a (potentially) long reasoning path. We can collect or generate reasoning paths correspond-
ing to problems from existing datasets. Human experts then annotate each step in these paths for
correctness. These annotations can be used to directly train LLMs or as rewards in reward mod-
eling. However, in practice, richer annotations are often introduced [Lightman et al., 2024]. In
addition to the correct and incorrect labels, a step can also be labeled as neutral to indicate that
while the step may be technically correct, it might still be problematic within the overall reason-
ing process. Furthermore, to improve the efﬁciency of data annotation, techniques such as active
learning can be employed. Identifying obvious errors usually does not signiﬁcantly contribute to4.4 Improved Human Preference Alignment
199
Question (from The International Junior Math Olympiad):
Jessica has a lot of apps on her phone. The number of apps she has is divisible by 5. If she
downloaded 10 more apps, she would still have less than 50 apps. What is the maximum
original number of apps Jessica could have?
Response 1:
Response 2:
ȳ1 : x must be a multiple of 5, i.e.,
x
x ≡ 0 (mod 5).
total number of apps should still be less
x
than 50: x + 10 < 50. Solving this
inequality: x < 40.
ȳ3 : Since x must be a multiple of 5 and
x
less than 40, the largest value of x is 35.
ȳ4 : Verify the result. If x = 35, then after
ȳ5 : The maximum original number of
apps is 35.
so x must be a multiple of 5.
x
ȳ2 : If she downloads 10 more apps, her
ȳ2 : After downloading 10 apps, the total
downloading 10 apps, Jessica would
have: 35 + 10 = 45 which satisfy
45 < 50.
ȳ1 : The number of apps is divisible by 5,
x
new total will be x + 10, and this must
x
be less than 50. Therefore, x + 10 < 50.
Simplifying this gives x < 40.
ȳ3 : x must be less than or equal toproblematic
ȳ4 : But x + 10 should not be moreproblematic
the largest multiple of 5, which is 40.
than or equal to 50. So we need to
subtract 5 from 40.
ȳ5 : Therefore, the ﬁnal result is 35.
x
x
x
x
correct
correct
Fig. 4.11: Two LLM responses to a math problem. In response 1, both the ﬁnal result and all the reasoning steps are
correct. In response 2, the ﬁnal result is correct, but there are mistakes in the reasoning process (highlighted in red).
For outcome-based approaches, both responses are considered correct. For process-based approaches, the mistakes in
response 2 can be considered in reward modeling.
learning from reasoning mistakes. Instead, annotating steps that the model conﬁdently considers
correct but are actually problematic is often more effective.
Given a set of step-level annotated reasoning paths and corresponding inputs, we can train
a reward model to provide feedback for supervising policy learning. The reward model can be
treated as a classiﬁcation model, and so its architecture can be a Transformer decoder with a
Softmax layer stacked on top. At step k, the reward model takes both the problem description
(denoted by x) and the reasoning steps generated so far (denoted by ȳ≤k ) as input and outputs
a probability distribution over the label set {correct, incorrect} or {correct, incorrect, neutral}.
Then the learned reward model is used to evaluate reasoning paths by assessing the correctness of
each step. A simple method to model correctness is to count the number of steps that are classiﬁed
as correct, given by
r(x, y) =
ns

k=1
δ(correct, C(x, ȳ≤k ))
(4.83)
where C(x, ȳ≤k ) denotes the label with the maximum probability. We can also use log-probabilities200
Alignment
of classiﬁcation to deﬁne the reward of the entire path
r(x, y) =
ns

k=1
log Pr(correct|x, ȳ≤k )
(4.84)
where Pr(correct|x, ȳ≤k ) denotes the probability of the correct label generated by the reward
model. The reward score r(x, y) can then be used to train the policy in RLHF as usual.
While we restrict our discussion to math problems, the approaches described here are general
and can be applied to a wide variety of tasks that involve multi-step reasoning and decision-
making. Moreover, we can consider various aspects when assessing the quality of a step, rather
than just its correctness. For example, in dialogue systems, responses must not only be accurate
but also contextually appropriate across multiple turns of conversation. If a model provides a
correct response but fails to maintain coherence in the context of the ongoing dialogue, step-
level feedback could help the model identify and correct such discrepancies. Also note that the
process-based approaches are related to the ﬁne-grained reward modeling approaches discussed
in Section 4.4.1.3. All these approaches essentially aim to provide more detailed supervision to
LLMs by breaking their outputs into smaller, more manageable steps. However, process-based
feedback focuses more on evaluating the correctness of a step based on its preceding steps, while
the approaches in Section 4.4.1.3 emphasize evaluating each step independently.
The idea of aligning LLMs step by step has great application potential, especially considering
the recent shift towards more complex reasoning tasks in the use of LLMs. For example, both
the GPT-o1 and GPT-o3 models are designed with more advanced reasoning techniques (such
as long internal CoT) to solve challenging problems like scientiﬁc and mathematical reasoning
[OpenAI, 2024]. These tasks often rely on long and complex reasoning paths, and therefore, it
seems essential to introduce detailed supervision signals in the reasoning process. Moreover, from
a practical perspective, effective supervision on long reasoning paths not only improves reasoning
performance, but it also helps the model eliminate redundant or unnecessary reasoning steps,
thereby reducing reasoning complexity and improving efﬁciency.
4.4.5
Inference-time Alignment
In this section we explored a variety of methods to align models with human preferences and an-
notations. However, one of the signiﬁcant limitations of many such methods is that LLMs must
be ﬁne-tuned. For RLHF and its variants, training LLMs with reward models can be computa-
tionally expensive and unstable, leading to increased complexity and costs when applying these
approaches. In this case, we can consider aligning models at inference time, thus avoiding the
additional complexity and effort involved.
One simple way to achieve inference-time alignment is to use the reward model to select
the best one from N alternative outputs generated by the LLM, a method known as Best-of-N
sampling (BoN sampling). We can consider BoN sampling as a form of reranking. In fact,
reranking methods have been widely used in NLP tasks, such as machine translation, for a long
time. They are typically applied in situations where training complex models is costly. In such
cases, directly reranking the outputs allows for the incorporation of these complex models at a
very low cost8 .
8
Reranking methods can also help us explore what are known as model errors and search errors, although these4.5 Summary
201
In the BoN sampling process, the LLM takes the input sequence x and generates N different
output sequences {ŷ1 , ▷▷▷, ŷN }:
{ŷ1 , ▷▷▷, ŷN } = argTopN [Pr(y|x)]
y
(4.85)
where the argTopN operation returns the top-N outputs that maximize the function Pr(y|x).
These outputs can be generated in a variety of ways, depending on the search algorithm used by
the model (e.g., sampling or beam search). Once the N -best output candidates are generated, the
reward model is used to evaluate and select the best one:
ŷbest = max{r(x, ŷ1 ), ▷▷▷, r(x, ŷN )}
(4.86)
It is worth noting that the result of BoN sampling is also inﬂuenced by the diversity of the
N -best list. This is a common issue with most reranking methods. Typically, we wish the N -best
output candidates to have relatively high quality but be sufﬁciently different from each other. In
many text generation systems, the N -best outputs are very similar, often differing by just one
or two words. The diversity issue is even more challenging in LLMs, as the N -best outputs
generated by an LLM can be different in their wordings, yet their semantic meanings are often
quite similar. In practice, one can adjust the model hyperparameters and/or adopt different LLMs
to generate more diverse output candidates for reranking. Nevertheless, as with many practical
systems, we need to make a trade-off between selecting high-quality candidates and ensuring
sufﬁcient variation in the generated outputs.
BoN sampling can be used for training LLMs as well. A closely related method is rejection
sampling. In this method, we ﬁrst select the “best” outputs from the N -best lists via the reward
model, and then take these selected outputs to ﬁne-tune the LLM. In this way, we can introduce
human preferences into the training of LLMs via a much simpler approach compared to RLHF.
Many LLMs have adopted rejection sampling for ﬁne-tuning [Nakano et al., 2021; Touvron et al.,
2023b].
4.5
Summary
In this chapter, we have explored a range of techniques for aligning LLMs. In particular, we
have discussed ﬁne-tuning methods that enable LLMs to follow instructions and align them with
human preferences. One of the beneﬁts of ﬁne-tuning LLMs is computation efﬁciency. Unlike
pre-training based on large-scale neural network optimization, ﬁne-tuning is a post-training step
and so is less computationally expensive. Moreover, it is better suited to address problems that are
not easily solved in pre-training, such as human value alignment. The widespread attention to the
alignment issue has also led to a surge of research papers on this topic, which has posed challenges
in writing this chapter, as it is difﬁcult to cover all the latest techniques. However, we have tried
to provide a relatively detailed introduction to the fundamental approaches to alignment, such as
issues are not often discussed in the context of LLMs. For example, suppose we have an old model and a new, more
powerful model. We can use the new model to select the best output from the N -best list of the old model as the oracle
output. The performance difference between the oracle output and the top-1 output of the original N -best list reﬂects
the performance gain brought by the new model. If the performance gain is signiﬁcant, we can say that the old model
has more model errors. If the gain is small, it may indicate that the issue lies in search errors, as the best candidates
were not found.202
Alignment
instruction ﬁne-tuning and RLHF.
While we have focused on LLM alignment techniques in this chapter, the term AI alignment
is a wide-ranging concept. It generally refers to the process of ensuring that the behavior of an AI
system aligns with human values, goals, and expectations. The idea of AI alignment can be traced
back to the early days of AI. A widely cited description of AI alignment comes from an article by
the mathematician and computer scientist Norbert Wiener [Wiener, 1960]. The quote is as follows
If we use, to achieve our purposes, a mechanical agency with whose operation
we cannot efciently interfere ... we had better be quite sure that the purpose
put into the machine is the purpose which we really desire.
At that time, AI alignment was a distant concern for researchers. But today, it greatly inﬂu-
ences the design of various AI systems. For example, in robotics, alignment is critical to ensur-
ing that autonomous robots safely interact with humans and their environments. In autonomous
driving, cars must not only follow trafﬁc laws but also make complex, real-time decisions that
prioritize human safety, avoid accidents, and navigate ethical dilemmas.
In current AI research, alignment is usually achieved by developing a surrogate objective that
is analogous to the real goal and steering the AI system towards this objective. However, designing
the objective of AI alignment is very difﬁcult. One reason is that human values are diverse and
often context-dependent, making it difﬁcult to distill them into a single, universally applicable
objective function. Also, the complexity of real-world environments, where values and goals often
conﬂict or evolve over time, further complicates alignment efforts. Even if we could deﬁne an
appropriate objective, AI systems may ﬁnd unintended ways to achieve it, leading to “misaligned”
outcomes that still technically satisfy the objective but in a harmful or counterproductive way.
These challenges have motivated and are motivating AI research towards more aligned sys-
tems, either through developing new mechanisms for perceiving the world or more efﬁcient and
generalizable methods to adapt these systems to given tasks. More importantly, as AI systems
become more powerful and intelligent, especially given that recent advances in LLMs have shown
remarkable capabilities in dealing with many challenging problems, the need for AI alignment
has become more urgent. Researchers have started to be concerned with AI safety and warn the
community that they need to develop and release AI systems with great caution to prevent these
systems from being misaligned [Russell, 2019; Bengio et al., 2024].https://github.com/NiuTrans/NLPBook
https://niutrans.github.io/NLPBook
C HAPTER 5
Inference
Once we have pre-trained and ﬁne-tuned an LLM, we can apply it to make predictions on new
data. This process is called inference, in which the LLM computes the probabilities of differ-
ent possible outputs given an input, and selects the output that maximizes the probability. The
inference problem is generally expressed in the following form:
ŷ = arg max Pr(y|x)
y
(5.1)
where x is the input sequence, y is a possible output sequence, and ŷ is the best output sequence.
This is perhaps one of the most widely adopted formulas in NLP, and dates back to the early
days of speech recognition and machine translation systems based on probabilistic models. Al-
though for some applications, such as predicting a token using a very small language model, solv-
ing this optimization problem seems trivial, for most situations the computational challenges arise
from both calculating Pr(y|x) and performing the arg max operation. The problems we therefore
wish to address in this chapter involve: 1) computing the prediction probability efﬁciently given a
trained LLM, and 2) devising an efﬁcient (suboptimal) search for ŷ.
At a high level, these are fundamental issues in artiﬁcial intelligence, which have been ex-
tensively studied. So many well-established techniques can be directly applied, for example, one
can use greedy search algorithms to implement an efﬁcient inference system. On the other hand,
model-speciﬁc optimizations, such as efﬁcient attention models for Transformers, can be consid-
ered to further improve efﬁciency. But, in many practical applications, we still need to make a
trade-off between accuracy and efﬁciency, by carefully combining various techniques.
The importance of the inference problem in LLMs also lies in the fact that many application
scenarios require processing extremely long sequences. Recent studies have found that injecting
additional prompts and contextual information, such as long chain-of-thought prompts, during
inference can signiﬁcantly improve the performance of LLMs. This provides a new approach to
scaling LLMs: better results can be achieved by increasing the compute at inference time. For
instance, through inference-time scaling, OpenAI [2024]’s o1 and Deepseek [2025]’s R1 systems
have demonstrated impressive performance on complex reasoning and programming tasks. This,
in turn, has encouraged the NLP ﬁeld to focus more on the issue of efﬁcient inference.
In this chapter, we will introduce basic concepts and algorithms of LLM inference, including
preﬁlling-decoding frameworks, search (decoding) algorithms, and evaluation metrics of infer-
ence performance. We will then present methods for improving the efﬁciency of LLM inference,
covering a range of techniques for speeding up the system and compressing the model. Finally,
we will discuss inference-time scaling, which is considered an important application of inference
optimization.204
Inference
5.1
Preﬁlling and Decoding
In this section, we present the preﬁlling-decoding framework, which is the most commonly used
for interpreting and implementing LLM inference processes. We ﬁrst introduce the notation and
background knowledge, and then describe the details of the framework, such as the decoding
algorithms for LLM inference.
5.1.1
Preliminaries
Although we have described LLMs many times in this book, we begin by brieﬂy deﬁning the
notation to facilitate the subsequent discussion, and to make this chapter self-contained.
x: The input token sequence. It is conceptually equivalent to a “prompt”,
which includes instructions, user inputs, and any additional context in-
tended as input to the LLM. x comprises m + 1 tokens, denoted by
x0 ▷▷▷xm , where x0 is the start symbol ⟨SOS⟩.
y: The output token sequence, also called the response to the input. y com-
prises n tokens, denoted by y1 ▷▷▷yn .
y<i : The output tokens that precede position i, that is, y<i = y1 ▷▷▷yi−1 .
Pr(y|x): The probability of generating y given x using the LLM. If the LLM is
parameterized by θ, we can write it as Prθ (y|x).
[x, y]: The concatenated token sequence of x and y. That is, [x, y] =
x0 ▷▷▷xm y1 ▷▷▷yn . Occasionally, we use the notation seqx,y to represent
[x, y].
Pr([x, y]): The probability of generating the token sequence [x, y] using the LLM.
As described in Eq. (5.1), the goal of LLM inference is to maximize Pr(y|x). Modeling this
conditional probability is common in NLP. At ﬁrst glance, it seems to be a sequence-to-sequence
problem, where we transform a sequence into another using encoding-decoding models. How-
ever, we are not discussing sequence-to-sequence problems or encoding-decoding architectures.
Instead, as discussed in earlier chapters, this modeling problem can be addressed by using decoder-
only models. To do this, we can interpret the log-scale probability log Pr(y|x) as the difference
between log Pr([x, y]) and log Pr(x)
log Pr(y|x) = log Pr([x, y]) − log Pr(x)
(5.2)
where log Pr([x, y]) and log Pr(x) can be obtained by running the LLM on the sequences [x, y]
and x, respectively. For example, we can calculate the probability of generating x using the chain5.1 Prelling and Decoding
205
rule
log Pr(x) = log Pr(x0 ▷▷▷xm )

= log Pr(x0 ) Pr(x1 |x0 ) · · · Pr(xm |x0 ▷▷▷xm−1 )
= log Pr(x0 ) +
=

m

j=1

=0

m

j=1
log Pr(xj |x<j )

log Pr(xj |x<j )
(5.3)
In other words, we calculate the token prediction log-probability at each position of x, and sum
all these log-probabilities.
In common implementations of LLMs, however, we do not need to compute the log-probability
of the input sequence, but use the LLM to directly compute the log-probability of the output se-
quence in the following form
log Pr(y|x) =
n

i=1
log Pr(yi |x, y<i )
(5.4)
where [x, y<i ] represents the context for predicting yi . We use Pr(yi |x, y<i ) to denote Pr(yi |[x, y<i ]),
following the commonly used notation in the literature.
Now, we have two sub-problems in addressing the inference issue described in Eq. (5.1):
• Model Computation: we model Pr(yi |x, y<i ) and compute it in an efﬁcient manner.
• Search: we ﬁnd the optimal (or sub-optimal) output sequence in terms of log Pr(y|x).
The second sub-problem is a classic issue in NLP. We will show in Section 5.1.3 that there
are several well-studied algorithms that can be applied to efﬁciently search the space of possible
output sequences. The ﬁrst sub-problem requires a language model to produce a distribution over
a vocabulary V given a sequence of context tokens. We can do this by training a Transformer
decoder, which outputs the distribution
Pr(·|x, y<i ) = Softmax(HWo )m+i(5.5)
H = Dec([x, y<i ])(5.6)
Here Dec(·) produces a sequence of representations, each corresponding to a position of the input
sequence. So, if we input [x, y<i ] to the LLM, H is an i′ × d matrix, where d is the dimensionality
of each representation, and i′ = m + i is the number of context tokens. We can then use a Softmax
layer to transform these representations into distributions of tokens. Wo ∈ Rd×|V | is the linear
mapping matrix of the Softmax layer, and HWo transforms the d-dimensional representations
in H into the |V |-dimensional representations. The use of the subscript m + i indicates that the
Softmax function is performed only on the representation at position m + i. See Figure 5.1 for an
illustration of this architecture.206
Inference
Pr(·|x)
Pr(·|x, y1 )
···
Pr(·|x, y<i )
Softmax Layer
decoder output
···
···
FFN
······
······
L layers
Self-attention
Linear Mapping
···
x0
x1
···
Embedding Layer
xm
y1
···
···
yi−1
Fig. 5.1: The decoder-only architecture for LLMs. The decoder consists of an embedding layer and a stack of Trans-
former layers. In each Transformer layer, the input passes through a linear mapping, a self-attention network, and
an FFN. The output of the decoder is a sequence of representations that are taken as input to a Softmax layer, which
generates a distribution of tokens for each position.
Dec(·) is a Transformer decoding network that consists of an embedding network and a num-
ber of stacked self-attention and FFN networks. We will not discuss Transformers in detail here,
as readers can easily learn about these models from the literature. However, it is worth point-
ing out that the difﬁculty of inference is in part from the use of the self-attention mechanism in
Transformers. Recall that a general form of single-head self-attention is given by
q i′ K T
Attqkv (qi′ , K, V) = Softmax( √ )V
d
(5.7)
where qi′ ∈ Rd is the query at the position i′ (i.e., position of yi ), and K and V ∈ Ri ×d are the
keys and values up to i′ , respectively.
′
At each step during inference, we call the self-attention function Attqkv (·), followed by an
FFN, to generate a d-dimensional representation that integrates information from both the current
token and its left context. This process is repeated through L layers of self-attention and FFN,
forming a stack of Transformer layers. The output of the L-th layer in this stack is the ﬁnal
representation.
Each time, the model attends position i′ to all previous positions, which results in 2i′ vector
q ′ KT
) and V). Hence,
products (i′ times for qi′ KT and i′ times for the product of Softmax( i√
d
generating a sequence of length len has a time complexity of O(L × len2 ) for the self-attention
network. Clearly, the inference of this model is slow for long sequences due to its quadratic
time complexity with respect to sequence length. Therefore, many improvements to Transformers
and alternative models have focused on efﬁcient methods that are faster than this quadratic time
complexity, such as sparse attention mechanisms and linear-time models. A detailed discussion
of efﬁcient Transformers can be found in the previous chapters, and this section will focus on the5.1 Prelling and Decoding
207
standard Transformer architecture.
Note that in self-attention, the queries, keys, and values of a layer are linear mappings from
the same input (i.e., the output of the previous layer). Once a new key-value pair is generated, it
is repeatedly used in subsequent inference steps. Rather than regenerating these key-value pairs
during inference, a more desirable way is to store them in a structure, called the key-value cache,
or the KV cache. Thus, (K, V) can straightforwardly be considered a KV cache. This cache is
updated as follows
K = Append(K, ki′ )
V = Append(V, vi′ )
(5.8)
(5.9)
where (ki′ , vi′ ) is the newly generated key-value pair at position i′ , and Append(a, b) denotes a
function that appends a row vector b to a matrix a. Figure 5.2 shows how a Transformer decoder
works with a KV cache.
Finally, the process of computing log Pr(y|x) is summarized as follows:
1. We concatenate x and y into a sequence [x, y]. For each position i′ of this sequence, we
perform the following steps.
(a) We compute the embedding of the token at position i′ , and feed the resulting embed-
ding as an initial representation into the stack of Transformer layers.
(b) In each Transformer layer, we pass the input representation through the self-attention
network ﬁrst and then through an FFN. In the self-attention network, the input repre-
sentation is transformed into qi′ , ki′ , and vi′ . Then, we update the KV cache (K, V)
using ki′ and vi′ (see Eqs. (5.8-5.9)). Then, we compute the output of the attention
model by attending qi′ to (K, V) (see Eq. (5.7)).
(c) If i′ > m (i.e., i = i′ − m ≥ 0), we take the output of the Transformer stack and
compute the token prediction probability Pr(yi |x, y<i ) via the Softmax layer (see Eq.
(5.5)).
2. When reaching the end of the sequence, we obtain log Pr(y|x) by summing log Pr(yi |x, y<i )
over i ∈ [1, n] (see Eq. (5.4)).
5.1.2
A Two-phase Framework
As we have seen, language modeling is a standard autoregressive process, where each token is
generated one at a time, conditioned on the previous tokens. For Transformers, this requires the
model to maintain a KV cache that stores past representations, and attend the newly generated rep-
resentation to this KV cache. If we think of the model Pr(y|x) from the perspective of computing
the KV cache, it is natural to divide inference into two phases:
• Preﬁlling. The preﬁlling phase computes the KV cache for the input sequence x. It is
called preﬁlling because the model prepares and stores the key-value pairs for each token208
Inference
attention
k1k2···ki′ −1k i′
v1v2···vi′ −1v i′
KV Cache (positions 1 to i′ − 1)
q i′
query
key
value
v i′
k i′
Linear maps
Input
Input at position i′
(a) Updating the KV Cache at Position i′
attention
k1k2···ki′ −1k i′ki′ +1
v1v2···vi′ −1v i′vi′ +1
KV Cache (positions 1 to i′ )
qi′ +1 query
key
value
vi′ +1
ki′ +1
Linear maps
Input
Input at position i′ + 1
(b) Updating the KV Cache at Position i′ + 1
Fig. 5.2: Illustration of the KV cache. We update the KV cache at a position, perform the attention operation, and then
move to the next position to repeat the process.
in the input before the actual inference begins. The process of preﬁlling in an LLM can be
expressed as
cache = Deckv (x)
(5.10)
where Deckv (·) is the decoding network (i.e., the same as Dec(·)), but it returns the KV5.1 Prelling and Decoding
209
cache in self-attention instead of the output representations. cache is a list, given by
cache = {cache1 , ▷▷▷, cacheL }
(5.11)
where cachel represents the key-value pairs for the l-th layer.
• Decoding. The decoding phase continues generating tokens based on the KV cache, as
illustrated in Figure 5.2. When a new token is input into the decoder, we update the KV
cache in each layer by adding the new key-value pair. The updated cache is then used for
self-attention computation. The token generation stops when some stopping criterion is met,
such as when the generated token is the end symbol. The goal of decoding is to ﬁnd the best
predicted sequence, which is given by
ŷ = arg max Pr(y|cache)
y
(5.12)
Here we use Pr(y|cache) instead of Pr(y|x) to emphasize that the decoding process actu-
ally relies on the KV cache rather than x.
The preﬁlling and decoding processes are illustrated in Figure 5.3. Note that both these pro-
cesses are autoregressive. However, as shown in Table 5.1, they differ in several aspects, which
lead to very different implementations in practice.
In essence, while the underlying model of preﬁlling is based on token prediction, it can be
considered an encoding process. This is because our goal is not to generate tokens, but to build a
context representation (i.e., the KV cache) for the subsequent steps in the decoding phase. In this
sense, it is similar to BERT, where we encode the input sequence into a sequence of contextualized
token representations. On the other hand, unlike BERT which generates bidirectional sequence
representations, preﬁlling is based on standard language modeling tasks, and is thus unidirectional.
Note that, since the entire sequence x is input into the model all at once, all queries can be packed
together and the self-attention operation is performed on x in parallel. Let Q be the queries that
are packed into one matrix. The self-attention model in preﬁlling can be deﬁned as
QKT
Attqkv (Q, K, V) = Softmax( √ + Mask)V
d
(5.13)
where Q, K, V ∈ Rd×(m+1) . Mask ∈ R(m+1)×(m+1) is a mask that ensures that each token
only attends to itself and the tokens that precede it in the sequence. It is represented by setting the
values in the mask corresponding to future tokens to a large negative number, for example, for the
query qi and the key kj , we set the value of the entry (i, j) to −∞ if i < j. One advantage of
processing the sequence with a single self-attention computation is that we can make better use of
the parallel computing capabilities of modern GPUs, and so speed up preﬁlling. In general, the
preﬁlling process is considered compute-bound. This is because merging multiple computational
operations into one operation reduces the number of data transfers and the performance bottleneck
usually comes from the computational capacity rather than memory bandwidth.
Decoding is a standard left-to-right text generation process. The token sequence is generated
autoregressively by predicting one token at a time based on the KV cache. Each time a new token
is generated, we need to attend it to previous tokens, following Eq. (5.7). Therefore, the decoding210
Inference
Self-attention in a Layer
Transformer Decoder
···
···
queriesq0q1···
keys
valuesk0k1···
v0
v1
qm−1
km−1
vm−1
···
···
···
···
Embedding Layer
x1
x0
xm−1
···
Processed all at once
(a) Preﬁlling
Pr(y1 |x) Pr(y2 |x, y1 ) · · · Pr(yn |x, y<n )
Softmax Layer
Self-attention in a Layer
Transformer Decoder
···
···
keys
values
k0
v0
k1
v1
···
···
queriesqmqm+1
km−1kmkm+1
vm−1
vm
vm+1
···
···
···
qm+n
km+n
vm+n
···
···
···
Embedding Layer
xm
y1
···
yn−1
Processed step by step (n steps)
(b) Decoding (at the n-th step)
Fig. 5.3: Illustration of the preﬁlling and decoding processes. In preﬁlling, the entire input sequence is processed
together and the KV cache is ﬁlled. In decoding, the LLM generates the output sequence step by step based on the
preﬁlled KV cache.
process is memory-bound due to its frequent access to the KV cache. The cost of decoding grows
signiﬁcantly as more tokens are generated. In most cases, decoding is computationally more
expensive than preﬁlling. Note that this is not just because, in decoding, the LLM generates tokens
one by one and repeatedly updates the KV cache. As we will see in the following subsection,5.1 Prelling and Decoding
211
PreﬁllingDecoding
Set up initial context x.Continue generating tokens
y after the initial input.
All-at-once VisibilityTokens in x are presented all
at once.Tokens in y are presented
sequentially, that is,
predicting a token requires
waiting for the previous
tokens to be predicted ﬁrst.
Context UseBuild the context or encoded
representation of the input.Use the cached key-value
pairs (from preﬁlling) to
generate further tokens.
Resource LimitationCompute-boundMemory-bound
Computational CostHighVery High
Goal
Table 5.1: Preﬁlling vs Decoding.
we may need to explore multiple different token sequences during decoding, which makes the
problem more complex and increases its cost further.
5.1.3
Decoding Algorithms
So far our discussion of LLM inference has primarily focused on the model computation problem,
that is, how to compute Pr(y|x). Now we turn to the discussion of the search problem. The
problem can be stated as: given an LLM Pr(y|x), how do we efﬁciently search for the best output
sequence ŷ given the input sequence x (or the generated KV cache)? Naively, we can consider all
of the output sequences, compute the prediction probability for each, and then select the output
sequence having the highest probability. This method can guarantee the globally optimal solution,
but direct exhaustive search is impractical for LLMs as the number of possible output sequences
grows exponentially with the length of y.
In practice, various heuristic search algorithms, such as greedy search and sampling-based
search, are commonly employed to approximate the solution. Each of these methods offers trade-
offs between search quality and computational efﬁciency. The search problem, therefore, becomes
a balancing act between exploration and exploitation, where the goal is to ﬁnd an efﬁcient strategy
that produces high-quality outputs without exploring the entire space.
Before giving a more detailed discussion of these methods, let us ﬁrst informally deﬁne what
a search space is and how it is represented. In LLM inference, we deﬁne a hypothesis as a tuple
of input and output sequences. Since x is ﬁxed during inference, we can simply consider each
hypothesis as an output sequence. The search space, denoted by Y, is then the set of all possible
hypotheses (i.e., output sequences) that the model can generate. The search problem for LLM
inference can be re-expressed as
ŷ = arg max Pr(y|x)
y∈Y
(5.14)
In NLP, Y is commonly represented in a tree data structure to facilitate search. Figure 5.4
shows an example of the search tree resulting from a small vocabulary. In this example, a node212
Inference
Path: node 0 → node 3 → node 9 → node 11 → node 17
Output: cats are playful.
Probability:
node 0 → 0
node 3 → log Pr("cats"|x)
node 9 → log Pr("are"|x, "cats")
node 11 → log Pr("playful"|x, "cats are")
node 17 → log Pr("."|x, "cats are playful")
1
2
0
root
3
4
5
playful
.
cats
are
is
6
7
8
9
10
16
17
playful
.
cats
are
is
playful
11
12
13
14
15
playful
.
cats
18
19
20
.
cats
are
is
are
is
Fig. 5.4: A search tree for decoding. At each node, we expand the tree by considering all possible tokens, each
leading to a new node representing a potential continuation of the text. Here we highlight a path through nodes 0, 3,
9, 11, and 17. The path represents the output sequence “cats are playful.”, whose log-probability can be computed by
accumulating the log-probabilities of these nodes.
represents a preﬁx subsequence that can be shared by many sequences. The search starts with the
root of the tree, which can be regarded as the beginning of all sequences that can be generated1 .
Each child node extends the preﬁx of its parent node by adding one token from the vocabulary
to the sequence, along with the probability of predicting the token given the preﬁx. This pro-
cess continues as each node further branches out into additional child nodes, each representing a
new possible extension of the sequence with another token. The search tree thus grows deeper
and wider, representing an ever-increasing number of potential sequences as more tokens are ap-
pended. This structure allows us to efﬁciently traverse through possible sequences, evaluating
each in terms of the log-probability accumulated over the path from the root to that node. For
example, in Figure 5.4, the path from the root to the node 17 corresponds to the output sequence
“Cats are playful.”. The prediction log-probability log Pr(y|x) is the sum of the log-probabilities
of all the nodes on this path.
In general, the search tree is organized as levels, where each level consists of all nodes that
are the same distance from the root node. Thus, a breadth-ﬁrst search over the tree essentially
performs left-to-right generation of tokens. Nodes in the same level correspond to sequences of
the same length. As the search progresses, new tokens are appended to these sequences, expanding
them incrementally.
Let Yi be the set of the sequences that the LLM generates at step i. Yi can be obtained by
expanding each sequence in Yi−1 with all possible next tokens in the vocabulary V , given in the
1
Here, since the predictions in LLMs are based on x, we can think of the root as a representation of x.5.1 Prelling and Decoding
213
following recursive form
Yi = Yi−1 × V
(5.15)
where Yi−1 × V denotes the Cartesian product of Yi−1 and V (i.e., each sequence in Yi−1 is
concatenated with each token in V ). Note that if a sequence in Yi−1 is complete (e.g., ending
with the ⟨EOS⟩ token), it will not be expanded any further. Let Ψ(Yi ) be the set of all complete
sequences in Yi . Then, the search space can be expressed as
Y
= Ψ(Y1 )

Ψ(Y2 )

···

Ψ(Ynmax )
(5.16)
where nmax is the maximum length of a sequence.
Most decoding algorithms follow this level-by-level search process. However, Y consists of
an exponentially large number of sequences, and a direct search in such a vast space is computa-
tionally infeasible. Therefore, practical decoding algorithms often rely on strategies to prune the
search space and avoid exploring low-quality sequences. For example, at each decoding step, Yi
can be obtained in the following way
Yi = Prune(Yi−1 × V )
(5.17)
where Prune(·) is a function that selectively removes sequences less likely to result in high-quality
outcomes. In general, we expect that |Yi | << |Yi−1 | · |V |. Thus we can drastically reduce the
number of sequences under consideration at each step, ensuring that the computational load does
not grow exponentially with the sequence length.
Next, we will introduce these decoding algorithms. Some of them have already been discussed
in sequence-to-sequence models, while others are more commonly used in LLMs.
5.1.3.1
Greedy Decoding
Greedy search (or greedy decoding) is one of the most widely used decoding methods in NLP,
particularly in text generation tasks like machine translation. The idea behind greedy search is
straightforward: at each step in generation, it selects the next token that has the highest prediction
probability. For each sequence y = y1 ▷▷▷yi ∈ Yi−1 × V , we can evaluate it using log Pr(y|x).
This log-probability can be easily computed by noting that
log Pr(y|x) = log Pr(y1 ▷▷▷yi |x)
=
log Pr(y<i |x)



accumulated up to the parent node
+
log Pr(yi |x, y<i )



(5.18)
newly computed for the current node
Here the ﬁrst term is the sum of the log-probabilities of the path from the root to the parent node,
which has been computed in the previous decoding steps. At step i, we only need to compute the
second term which is the standard token prediction log-probability produced by the LLM.214
Inference
The “best” token at step i is then chosen as
yitop1 = arg max log Pr(y1 ▷▷▷yi |x)
yi ∈V

= arg max log Pr(y<i |x) + log Pr(yi |x, y<i )
yi ∈V


ﬁxed wrt. yi

= arg max log Pr(yi |x, y<i )
yi ∈V

(5.19)
Thus, the “best” sequence generated up to step i is given by
ytop1 = y1 ▷▷▷yi−1 yitop1
(5.20)
Finally, Yi contains only this sequence
Yi = {ytop1 }
(5.21)
The greedy choice in one decoding step is illustrated in Figure 5.5 (a). Greedy search offers
computational efﬁciency and simplicity in implementation for LLM inference. Its primary dis-
advantage, however, lies in its suboptimal nature — high-quality sequences are likely pruned at
early stages of decoding. Therefore, greedy search is appealing for tasks that demand speed and
simplicity. For tasks that require better search results, alternative strategies such as beam search,
which explores multiple potential paths simultaneously, are preferable.
5.1.3.2
Beam Decoding
Beam search (or beam decoding) is a natural extension of greedy search. Instead of selecting
the single most probable token at each step, beam search maintains a ﬁxed number of the best
candidates at each step, known as the “beam width”. See Figure 5.5 (b) for an illustration of beam
search.
Let K be the beam width. Given a parent node, which corresponds to the preﬁx y1 ▷▷▷yi−1 , we
can select the top-K next tokens by
{yitop1 , ▷▷▷, yitopK } = argTopK Pr(yi |x, y<i )
yi ∈V
(5.22)
where argTopK is a function that ranks the prediction probabilities of all possible next tokens and
selects the top K candidates. Given these tokens, the top-K sequences for step i are given by
ytop1 = y1 ▷▷▷yi−1 yitop1
..
.(5.23)
ytopK= y1 ▷▷▷yi−1 yitopK(5.24)
Yi = {ytop1 , ▷▷▷, ytopK }(5.25)
Then, we can deﬁne Yi as5.1 Prelling and Decoding
① Expansion
5
6
4
is
cute
on
8
9
② Ranking
.
7
215
are
sick
6
7
9
8
5
③ Output (1-best)
6
=⇒
cutePr = ▷34 ok
onPr = ▷32 pruned
sickPr = ▷21 pruned
arePr = ▷12 pruned
.Pr = ▷01 pruned
cute
(a) Greedy search
① Expansion
5
6
4
is
.
cute
7
on
8
9
② Ranking
are
sick
6
7
9
8
5
③ Output (K-best)
cutePr = ▷34 ok=⇒6
onPr = ▷32 ok=⇒7
sickPr = ▷21 ok=⇒9
arePr = ▷12 pruned.Pr = ▷01 pruned
cute
on
beam width (K) = 3
sick
(b) Beam search
① Expansion
5
6
4
is
.
cute
7
on
8
9
② Ranking
are
sick
6
7
9
8
5
③ Selection & Sampling ④ Output
cutePr = ▷34 ok=⇒6
onPr = ▷32 ok=⇒7
sickPr = ▷21 ok=⇒9
arePr = ▷12 pruned.Pr = ▷01 prunedcutePr = ▷39 pruned
onPr = ▷36 ok
sickPr = ▷25 pruned
=⇒
7
on
select top-k hypotheses (k = 3),
renormalize their proababilities,
and select one via sampling.
(c) Top-k Sampling
① Expansion
5
6
4
is
7
8
9
② Ranking
.
cute
on
are
sick
6
7
9
8
5
③ Selection & Sampling ④ Output
cutePr = ▷34 ok=⇒6
onPr = ▷32 ok=⇒7
sickPr = ▷21 prunedarePr = ▷12 pruned.Pr = ▷01 prunedcutePr = ▷51 pruned
onPr = ▷49 ok
=⇒
7
on
select top-ranked hypotheses
whose probability sum ≥ p = 0▷6,
renormalize their proababilities,
and select one via sampling.
(d) Top-p Sampling
Fig. 5.5: Illustrations of greedy decoding, beam decoding, top-k decoding and top-p decoding methods (in one decod-
ing step).
We can adjust the beam width K to balance search efﬁciency and accuracy. But a very large
beam width might not be helpful. In many practical applications, selecting a relatively small
number for K, such as K = 2 or K = 4, is often sufﬁcient to achieve satisfactory performance in
LLM inference.216
Inference
5.1.3.3
Sampling-based Decoding
Both greedy and beam search generate deterministic outputs, that is, given an LLM, the output of
the model will always be the same every time it processes the same input. The deterministic nature
of greedy and beam search ensures predictability and reliability in applications where consistent
outcomes are critical, such as in formal document generation, where varying outputs could cause
confusion or errors. On the other hand, one disadvantage of these methods is the lack of diver-
sity and ﬂexibility. For example, in creative tasks like story generation or conversational agents,
generic or repetitive outputs generated by deterministic systems are often less engaging.
To add variation into LLM outputs, we can use sampling-based decoding methods. There are
two commonly used methods.
• Top-k Sampling. This method selects the next token from the top-k most likely candidates
at each step of the generation process [Fan et al., 2018]. Let V i be the selection pool for
top-k sampling. We can deﬁne it as
V i = {yitop1 , ▷▷▷, yitopk }
(5.26)
where {yitop1 , ▷▷▷, yitopk } are the top-k tokens selected based on their prediction probabili-
ties (see Eq. (5.22)). Once the selection pool is determined, we recompute the prediction
probability distribution over V i . One of the simplest ways to do this is to renormalize their
probabilities:
Pr(yi |x, y<i ) =

Pr(yi |x, y<i )
yj ∈V i Pr(yj |x, y<i )
(5.27)
Alternatively, we can calculate the distribution by using the Softmax function:
Pr(yi |x, y<i ) =

exp(uyi )
yj ∈V i exp(uyj )
(5.28)
where uyi is the logit for token yi . Then, we sample a token ȳi from this distribution:
ȳi ∼ Pr(yi |x, y<i )
(5.29)
The corresponding sequence is ȳ = y1 ▷▷▷yi−1 ȳi , and Yi is given by
Yi = {ȳ}
(5.30)
• Top-p Sampling. This sampling method, also known as nucleus sampling, follows a pro-
cedure similar to that of top-k sampling. Instead of drawing from a ﬁxed size candidate
pool, it selects the next token from the smallest set of tokens that together have a cumulative
probability higher than a predeﬁned threshold p [Holtzman et al., 2020]. In this way we
prevent the prediction from choosing from low-probability tokens in the long tail that could
lead to incoherent or nonsensical outputs. To obtain the candidate pool in the top-p sam-
pling method, we can sort all tokens by their predicted probabilities. Then, starting with the
token with the highest probability, we continue to add tokens to the candidate pool until the5.1 Prelling and Decoding
217
(a) β = 0▷1
(b) β = 0▷8
(c) β = 2▷0
Fig. 5.6: Histogram estimates of the distributions generated by the Softmax function with different values of the
temperature parameter β.
cumulative probability of the tokens in the pool reaches or exceeds p (we denote the size of
the candidate pool at this point as kp ). The candidate pool can then be expressed as
topkp
V i = {yitop1 , ▷▷▷, yi
}
(5.31)
The subsequent steps, such as the renormalization of the distribution and sampling, are the
same as in the top-k sampling method (see Eqs.(5.27-5.30)).
See Figure 5.5 (c-d) for illustrations of the top-k and top-p sampling methods. By limiting the
choices to a smaller set of high-probability tokens, these methods strike a balance between ran-
domness and coherence. They allow for more diverse outputs while still maintaining a reasonable
level of relevance and ﬂuency. However, the value of k or p must be carefully chosen: if k or p is
too small, the output may still be overly deterministic (more like greedy decoding), and if k or p
is too large, the LLM might produce degenerate outputs.
In order to further control the randomness of the token selection process, the renormalized dis-
tribution Pr(·) is typically obtained by using the Softmax function with the temperature parameter,
given by
Pr(yi |x, y<i ) =

exp(uyi ◁β)
yj ∈V i exp(uyj ◁β)
(5.32)
Here β is a temperature parameter β that controls the sharpness of the probability distribution
derived from logits. In Figure 5.6, we show simple examples involving distributions generated by
the above function with different temperatures. When the temperature is set to a higher value, the
resulting probability distribution becomes more uniform, as the differences between the logits are
diminished. This means that each token in the candidate pool has a more equal chance of being
selected, leading to greater diversity in the generated output. By contrast, when the temperature
is set to a lower value, the distribution becomes sharper, making the high-probability tokens even
more likely to be chosen, which often results in more deterministic outputs. For example, if we set
p to 1 and β to a very small number (approaching zero), the top-p sampling method will become
equivalent to the greedy search method.
5.1.3.4
Decoding with Penalty Terms
One common improvement to decoding methods in text generation is to modify the search objec-
tive. For example, one can replace maximum a posteriori (MAP) decoding with minimum Bayes
risk (MBR) decoding [Kumar and Byrne, 2004], where the focus shifts from selecting the single218
Inference
most probable output to choosing an output that minimizes the expected risk over a distribution of
possible outputs. Here we explore methods that incorporate penalty terms into decoding. These
methods offer a simple but effective way to make decoding more controllable.
Recall from Eq. (5.14) that the goal of decoding is to maximize the likelihood of the output
sequence. With penalty terms, the objective is extended to include additional factors that penalize
or reward certain behaviors in the generated text. A general form of the new objective is given by

ŷ = arg max Pr(y|x) − λ · Penalty(x, y)
y∈Y

(5.33)
where Penalty(x, y) is a function that quantiﬁes the degree to which the generated sequence y
violates certain constraints or exhibits undesirable behaviors given the input x. The design of
Penalty(·) is very ﬂexible, thus allowing us to incorporate a wide range of constraints or prior
knowledge into it. Below, we present some common types of penalty functions.
• Repetition Penalty. A repetition penalty discourages the model from generating repetitive
or redundant text. The penalty function might measure the frequency of repeated tokens or
phrases in the generated sequence and impose a penalty proportional to their occurrence.
• Length Penalty. A length penalty ensures that the generated sequence adheres to a de-
sired length. For example, in text summarization tasks, the penalty function could penalize
outputs that are too short or too long.
• Diversity Penalty. A diversity penalty promotes variation in the generated text. For ex-
ample, in beam search, we can measure the similarity between generated hypotheses, and
encourage the model to explore different hypotheses.
• Constraint-based Penalty. A constraint-based penalty enforces speciﬁc constraints related
to the content or style of the generated text. For example, in machine translation, the penalty
function could penalize outputs that deviate from a desired tone or terminology.
In general, we can consider Penalty(x, y) as a function that deﬁnes the cost of generating the
surface form of the output sequence y given the input sequence x. Alternatively, this function can
be deﬁned to assess the hidden states of an LLM when generating y. For example, Su et al. [2022]
develop a penalty term that calculates the maximum distance between the representation of the
predicted token and the representations of the previously generated tokens. Therefore, the search
objective will penalize degenerated outputs, such as texts with many repetitions.
The method described in Eq. (5.33) is general and can be easily adapted to different search
algorithms. For example, in greedy search, we can keep the single sequence that maximizes
Pr(y|x) − λ · Penalty(x, y) at each decoding step; in sampling-based search, we can rank and
select the top-ranked sequences based on Pr(y|x) − λ · Penalty(x, y) to form the candidate pool.
5.1.3.5
Speculative Decoding
Speculative decoding stems from the concept of speculative execution, where a system makes
educated guesses about future actions and performs them in advance. If the guess is correct, the5.1 Prelling and Decoding
219
results are immediately available, which speeds up processing. In the case of LLM inference,
suppose we have two models. One is a smaller, faster model (called draft model), and the other
is the full, more accurate model (called veriﬁcation model). These two models represent two
baselines in LLM inference: the draft model is efﬁcient but not very accurate; the veriﬁcation
model is usually the one we want to run, but it is very slow. Given a preﬁx, we ﬁrst use the draft
model to speculatively predict a sequence of likely future tokens. This is a standard autoregressive
decoding process, but it is still fast in practice due to the high efﬁciency of the draft model. Then,
the veriﬁcation model evaluates the speculated tokens in parallel. It checks whether the predicted
tokens are correct or need to be adjusted. Note that, since we can deal with these tokens all at
once, the veriﬁcation can be done in a single step for all the tokens simultaneously, rather than in
a token-by-token manner. If the speculated tokens are correct, they are accepted, and the process
continues with the next set of tokens. If they are incorrect, the incorrect speculations are discarded,
and the veriﬁcation model is used to generate the correct tokens.
To be more speciﬁc, let us see the speculative decoding method presented in Leviathan et al.
[2023]’s work. In this method, the draft model is a small language model, denoted by Prq (yi |x, y<i ),
while the veriﬁcation model is a normal LLM, denoted by Prp (yi |x, y<i ). The goal is that, given a
preﬁx, we use the draft model to autoregressively predict up to τ tokens. The veriﬁcation model is
then employed to generate the last token at the point where errors begin to occur in the speculative
predictions. Figure 5.7 illustrates one step in this decoding process.
The speculative decoding algorithm can be summarized as follows.
• Given the preﬁx [x, y≤i ], we use the draft model to predict the next τ consecutive tokens,
denoted by {ŷi+1 , ▷▷▷, ŷi+τ }. This is a token-by-token generation process, given by
ŷi+t = arg max Prq (yi+t |x, y≤i , ŷi+1 ▷▷▷ŷi+t−1 )
yi+t
(5.34)
• We evaluate {ŷi+1 , ▷▷▷, ŷi+τ } using the veriﬁcation model, that is, we compute {Prp (ŷi+1 |x, y≤i )
, ▷▷▷, Prp (ŷi+τ |x, y≤i , ŷi+1 ▷▷▷ŷi+τ −1 )}. Note that we can compute these probabilities in par-
allel, and so this veriﬁcation step is efﬁcient.
• We determine the maximum number of accepted speculated tokens. In order to keep the no-
tation uncluttered, we denote Prq (ŷi+t |x, y≤i , ŷi+1 ▷▷▷ŷi+t−1 ) and Prp (ŷi+t |x, y≤i , ŷi+1 ▷▷▷ŷi+t−1 )
simply by q(ŷi+t ) and p(ŷi+t ), respectively. We then deﬁne that, if q(ŷi+t ) ≤ p(ŷi+t ), then
we accept this speculation. By contrast, if q(ŷi+t ) > p(ŷi+t ), we reject this speculation with
i+t )
probability 1 − p(ŷ
q(ŷi+t ) . Starting from ŷi+1 , the maximum number of accepted consecutive
speculated tokens is deﬁned as
na

p(ŷi+t )
= min t − 1|1 ≤ t ≤ τ, rt >
q(ŷi+t )

(5.35)
where rt is a variable drawn from the uniform distribution U (0, 1).
• Given na , we keep the speculated tokens {ŷi+1 , ▷▷▷, ŷi+na }. We then use the veriﬁcation
model to make a new prediction at i + na + 1
ȳi+na +1 = arg max Prp (yi+ns +1 |x, y≤i , ŷi+1 ▷▷▷ŷi+ns )
yi+ns +1
(5.36)220
Inference
Context (x, y<i )
ŷi+1
ŷi+2
ŷi+3
ŷi+4
ŷi+5
predict
Draft Model Prq (·)
(a) Predict the next τ tokens given the context using the draft model (τ = 5)
Evaluation Model Prp (·)
evaluate
Context (x, y<i )
ŷi+1
ŷi+2
ŷi+3
ŷi+4
ŷi+5
Draft Model Prq (·)
(b) Evaluate the predicted tokens using the evaluation model
Evaluation Model Prp (·)
accepted
Context (x, y<i )
ŷi+1
ŷi+2
rejected
ŷi+3
ŷi+4
ŷi+5
Draft Model Prq (·)
(c) Determine the number of accepted tokens
Evaluation Model Prp (·)
Context (x, y<i )
ŷi+1
ŷi+2
ŷi+3
ȳi+4
Draft Model Prq (·)
(d) Predict a new token following the accepted tokens using the evaluation model
Fig. 5.7: Illustration of one step of speculative decoding. The goal is to predict as many next tokens as possible using
the draft model. There are four sub-steps. Given the context, we ﬁrst use the draft model to predict the next τ tokens.
Then, we evaluate these predictions in parallel using the evaluation model. Next, we determine the maximum number
of predicted tokens that can be accepted. Finally, we use the evaluation model to predict a new token following these
accepted tokens.
• Above, we have described one step of speculative decoding. The result sequence (including
both the context and predicted tokens) is illustrated as follows5.1 Prelling and Decoding
221
[x, y<i ] ŷi+1 ▷▷▷ŷi+na
Context
na tokens
predicted using
the draft model
ȳi+na +1
One token
predicted using
the veriﬁcation model
Once we have ﬁnished this step, we add the predicted tokens {ŷi+1 , ▷▷▷, ŷi+na , ȳi+na +1 } to
the context, and repeat the above process.
In practice, we usually wish to use a smaller draft model so that predicting {ŷi+1 , ▷▷▷, ŷi+na }
would be computationally cheaper. But a very small draft model is less accurate and can result in
smaller na . We therefore need to carefully select the draft model to make the trade-off between
the computational efﬁciency and accuracy.
5.1.3.6
Stopping Criteria
Stopping criteria are a critical component of LLM inference. They typically involve rules or
conditions that specify when the model should stop generating text during decoding. Most LLMs
are trained to generate an end-of-sequence token (e.g., ⟨EOS⟩ or ⟨◁s⟩) to signal the end of the
generated text. So one of the simplest strategies is that the generation process stops when this
token is produced. For beam search, which explores multiple hypotheses simultaneously, the
process can continue until a given number of complete sequences have been generated.
In practical applications, it will generally be undesirable to generate very long sequences,
and so we need to reduce the decoding cost and unnecessary verbosity. One commonly-used
stopping criterion is the maximum length of the output. The model stops generating text once it
has produced a predetermined number of tokens. Alternatively, we can stop the decoding based
on the real cost, such as the computational resources or time constraints. For example, in real-
time applications like chatbots, decoding may need to stop after a certain time limit to ensure
responsiveness.
Another approach is to design stopping criteria based on the behavior of LLMs. For example,
decoding can be stopped if the probability of predicting the next token falls below a certain thresh-
old. In addition to probability-based stopping, a repetition detection module can be implemented
to trigger the model to stop if it begins repeating tokens or phrases beyond a predeﬁned limit. This
helps prevent redundant or incoherent outputs.
5.1.4
Evaluation Metrics for LLM Inference
Evaluating the performance of LLMs during inference involves a variety of metrics to assess how
well these models meet desired standards, such as accuracy, robustness, usability, and efﬁciency.
As with most NLP systems, we can evaluate LLMs using accuracy-based metrics, such as perplex-
ity and F1 score. We can also examine their robustness by testing how well they handle ambiguous
or challenging inputs, including adversarial, perturbed, or out-of-distribution data. Additionally,
usability can be assessed by measuring how well the generated outputs align with user expec-
tations in terms of ﬂuency, coherence, relevance, and diversity. Human evaluators can rate the
naturalness of the text or assess whether the responses are contextually appropriate and logically
consistent. Ethical and fairness metrics can also be included to ensure LLMs avoid perpetuating
biases or generating harmful content.222
Inference
All of the evaluation metrics mentioned above essentially focus on assessing the quality of the
outputs. Given the high cost of deploying and applying LLMs, efﬁciency metrics are also very
important for practitioners. Below are some commonly used efﬁciency metrics [Nvidia, 2025]:
• Request Latency. This metric measures the total time taken from when a request is sent
to the LLM until the complete response is received. This includes the time taken for data
transmission, processing by the model, and the return of the output to the user.
• Throughput. It refers to the number of tokens or requests the model can process per second.
• Time to First Token (TTFT). This metric measures the time it takes from the beginning of
a request being sent to the generation of the ﬁrst token of the response. If data transmission
does not consume too much time, then TTFT is mainly the time for preﬁlling and predicting
the ﬁrst token.
• Inter-token Latency (ITL). This metric refers to the time taken to generate each subsequent
token after the ﬁrst one. It reﬂects the efﬁciency of the decoding process.
• Tokens Per Second (TPS). This metric quantiﬁes the number of tokens that the model can
generate per second.
• Resource Utilization. This involves measuring the computational resource usage (e.g.,
CPU and GPU utilization) and memory consumption of the model during inference.
In addition to these metrics, energy efﬁciency and cost efﬁciency are practical considerations
for deploying LLMs at scale. Energy efﬁciency measures the amount of electrical power con-
sumed by the model during inference. Cost efﬁciency, on the other hand, evaluates the total
expenses related to deploying and maintaining the model.
In general, choosing the right evaluation metrics depends on the speciﬁc task and application.
While quality-focused metrics are essential for assessing LLMs, efﬁciency metrics are equally
crucial for their effective deployment in real-world applications. A comprehensive evaluation
framework should include both sets of metrics to accurately estimate an LLM’s performance and
practicality.
5.2
Efﬁcient Inference Techniques
In practical applications, we often wish a system to be as efﬁcient as possible. For LLM inference,
this typically involves two types of improvements: reducing memory requirements and acceler-
ating the system. For example, we can modify the Transformer architecture to avoid memory
explosion when processing very long input sequences. Another example is that we can compress
input sequences to reduce computational overhead while preserving their semantic information. In
addition, techniques like quantization and pruning can be employed to further optimize memory
usage and inference speed.
Efﬁcient inference is a wide-ranging topic that overlaps with several sub-ﬁelds of LLMs, such
as architecture design and model compression. Most of these topics have been covered in previous5.2 Efcient Inference Techniques
223
chapters. For example, in Chapter 2, we discussed efﬁcient Transformer architectures and long-
context LLMs; and in Chapter 3, we discussed prompt compression methods for reducing prompt
length. In this section, we focus on techniques that are commonly used in LLM deployment and
serving.
5.2.1
More Caching
In real-world applications, it is common practice to store frequent requests and their corresponding
responses in a cache. When a new request hits the cache, the system can retrieve the response
directly from the cache instead of recomputing the result. One straightforward implementation is
a key-value datastore (e.g., a hash table) that maps input sequences to their LLM-generated output
sequences. In the simplest case, we can collect frequent queries, generate their responses using the
LLM, and store these query-response pairs in the datastore. This creates a basic sequence-level
caching mechanism that allows the system to bypass LLM computation when the input sequence
exactly matches a cached query.
A straightforward extension of the caching mechanism is to cache preﬁxes and their corre-
sponding hidden states. Given an input sequence x in a dataset D, we can process it as in the
standard preﬁlling phase. Thus, we obtain a sequence of preﬁxes and their corresponding KV
cache states:
x0 (x<1 ) ⇒ cache<1
x0 x1 (x<2 ) ⇒ cache<2
▷▷▷
x0 x1 ▷▷▷xm−1 (x<m ) ⇒ cache<m
where cache<i denotes the KV cache for the preﬁx x<i (see also Eq. (5.10)). All these mappings
can be stored in the preﬁx cache for efﬁcient reuse.
When processing a new sequence that shares a common preﬁx with a previously seen sequence
in D, we can load the corresponding cached hidden states instead of recomputing them. Speciﬁ-
′
cally, if a new input x′ has x<k (i.e., x<k
= x<k for some k ≤ m), we can initialize the KV cache
′ .
with cache<k and only compute the hidden states for the remaining tokens x≥k
As usual, we can maintain a key-value datastore that maps frequently encountered preﬁxes to
their precomputed KV caches. The lookup can be performed using a hash of the preﬁx tokens,
allowing constant-time access to the cached states. Care must be taken to manage memory usage,
as storing all possible preﬁxes may be infeasible for large datasets. Practical systems often employ
least recently used (LRU) caching methods or other strategies to balance between computational
savings and memory constraints.
5.2.2
Batching
Batching in LLM inference refers to the process of processing multiple input sequences simultane-
ously as a group (called a batch) rather than one at a time. Because modern GPUs excel at parallel
processing, batching allows them to compute multiple sequences in a single forward pass, keeping
the hardware fully occupied. Therefore, when serving LLMs at scale, batching is important for224
Inference
preﬁlling
decoding
preﬁlling
decoding
11111111
pad pad2
pad
222222
pad pad pad33333
444444
4
4
(a) batch size = 1
4
1
4
4
4
(b) batch size = 4
transfer the KV cache
preﬁlling
decoding
pad1111111111
pad2222222222
33333333333
44444444444
1
4
(c) batch size = 4 (similar sequence lengths)
engine 1engine 2
preﬁllingdecoding
pad11111111333222222
1
44555533366666644455555
66666
3
6
(d) disaggregation of preﬁlling and decoding
Fig. 5.8: Illustrations of basic batching methods. We use a 2D layout to illustrate the batch, where each square
represents a token. Red squares indicate tokens in the preﬁlling stage, blue squares represent tokens in the decoding
stage, green squares denote padding tokens, and gray squares correspond to meaningless tokens. Subﬁgures (a) and (b)
compare the cases where the batch size is 1 and 4, respectively. Subﬁgure (c) shows the strategy of grouping sequences
with similar lengths into the same batch. Subﬁgure (d) illustrates the disaggregation of preﬁlling and decoding. In
this approach, we can make better use of the parallelism of GPUs by concatenating multiple short sequences into a
single long sequence for joint processing. This allows us to maximize the number of tokens processed in a batch while
minimizing the number of padding tokens. However, as a trade-off, we need to copy the KV cache to the decoding
engine and reorganize it after the preﬁlling phase, which introduces additional data transfer overhead.
improving computational efﬁciency and maximizing hardware utilization2 .
To illustrate the idea of batching, Figure 5.8 (a-b) show simple examples with batch sizes of
1 and 4, respectively. When using a batch size of 1 (i.e., without batching), the GPU processes
one input sequence at a time. Thus, the processing is sequential: the next sequence must wait for
the current computation to ﬁnish. By contrast, when using a batch size of 4, the GPU can process
four sequences simultaneously in a single forward pass. As the input sequences vary in length, we
need to standardize their length using padding techniques. Here we use left padding, which adds
dummy tokens to the beginnings of short sequences, so all the sequences in the batch would have
the same length for preﬁlling. For decoding, tokens are generated simultaneously for all these
sequences, and the generation process continues until the longest sequence reaches completion.
The above examples imply a trade-off between throughput and latency, which is a very impor-
tant consideration in designing and implementing LLM inference systems. If we choose a smaller
batch size, the latency would be lower, as fewer tokens need to be processed in a single run of
inference. Imagine that we have only one sequence. The result becomes available immediately
after generation completes, with no additional computational overhead. However, this low-latency
advantage comes at the cost of underutilizing parallel computing resources, as the parallelism of
2
See
https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/
index.html#understand-perf for a simple evaluation.5.2 Efcient Inference Techniques
225
GPUs remains largely idle during sequential processing. On the other hand, if we use a larger
batch, we can make better use of the parallelism, as GPUs can be occupied by large-scale ma-
trix computations. As a result, we can process more tokens in the same period of time and the
throughput is improved. However, since the result is obtained only when the last token in the batch
is predicted, the latency would be higher.
In practice, we usually prefer to use a slightly larger batch, but try to ﬁll the batch with se-
quences of similar lengths to reduce the number of padding tokens and improve device utilization.
For example, we can group the incoming user requests in a short period of time into buckets, each
of which contains sequences with similar lengths. Then, we can ﬁll the batch with sequences in
the same bucket, so that we can minimize wasted computational resources, as illustrated in Figure
5.8 (c).
Another approach to implementing batching in LLMs is to disaggregate the preﬁlling and de-
coding processes [Wu et al., 2023a; Patel et al., 2024; Zhong et al., 2024]. For example, we can
perform preﬁlling on one GPU, and perform decoding on another GPU. One advantage of disag-
gregation is that we can rearrange the input sequences in the batch to better ﬁll it, because there is
no interference between preﬁlling and decoding. For example, we can concatenate multiple short
sequences into a longer one, thus ensuring that the lengths of sequences in the batch are as con-
sistent as possible, as illustrated in Figure 5.8 (d). In this way, we can maximize the throughput
of the preﬁlling phase. However, as a trade-off, we need to transfer the KV cache to the devices
performing decoding, which also incurs extra communication overhead. Typically, this method
requires a high-bandwidth, low-latency network to achieve optimal performance.
In this section, we will discuss several improvements to the above basic batching strategies.
Most of them are based on an aggregated architecture, that is, decoding and preﬁlling can be
considered as different stages of a model executed on the same device.
5.2.2.1
Scheduling
A practical LLM inference system typically consists of two components:
• Scheduler. Its primary role is to efﬁciently queue and dispatch tasks (i.e., input sequences)
to the inference engine based on the current system load and task priorities. This often
involves a variety of batching strategies that group certain requests together to maximize
processing efﬁciency in some way.
• Inference Engine. It is responsible for the actual execution of the LLMs, processing the
queued requests as they come in. As discussed previously, this engine involves both preﬁll-
ing and decoding processes.
This architecture is illustrated in Figure 5.9. Incorporating scheduling into batch processing
provides a ﬂexible way to optimize both the system’s throughput and latency, thereby achieving a
better balance between them. For example, the batching methods shown in Figure 5.8 (a) and (b)
can be considered one of the simplest scheduling strategies, called request-level scheduling. In
this strategy, once a batch is ﬁlled and sent to the engine, the processing of the entire batch cannot226
Inference
Request Pool
x1 , x2 , x3 , ...
batch
Inference
Engine
Scheduler
batch (after processing)
Predictions
y2 , y1 , y3 , ...
Fig. 5.9: Illustration of the LLM inference architecture involving a scheduler and an inference engine. Each time, the
scheduler selects a number of user requests to form a batch and sends it to the inference engine. The scheduler can
interact with the inference engine and adjust the batch at certain points during inference, such as at the beginning of
batch processing and at the start of each token prediction.
be interrupted. The scheduler waits for this batch to be processed before handling the next batch
[Timonin et al., 2022].
A more sophisticated scheduling strategy, called iteration-based scheduling, interacts with
the inference engine at each token prediction step rather than at the sequence level. This approach
allows dynamic batch adjustment during inference, as illustrated in Figure 5.10. Such ﬁne-grained
control lets the system prioritize critical tokens or sequences in real-time. For instance, if an urgent
request arrives at some decoding step, the scheduler can add this request into the batch so that it
can be processed as early as possible. In the following subsections, we will discuss batching
methods based on iteration-based scheduling.
5.2.2.2
Continuous Batching
Continuous batching is an iteration-based scheduling method used in the Orca system [Yu et al.,
2022]. In this method, an iteration refers to either the entire preﬁlling procedure or a single
decoding step. For example, given an input sequence x = x0 ▷▷▷xm and an output sequence y =
y1 ▷▷▷yn , there are n + 1 iterations in total: one for preﬁlling, and n for generating the output tokens
(one per token). During scheduling, the batch can be adjusted between iterations. For example,
we can either add a new input sequence to the batch, or remove a complete sequence from the
batch at some iteration, even if the batch processing is not yet ﬁnished.
The general process of continuous batching includes the following steps:
• Initially, a batch is created with one or more input sequences, based on both the inference
engine’s processing capacity and the current user requests. The batch is then fed into the
inference engine.5.2 Efcient Inference Techniques
227
Requests
x1 , x2 , x3 arrived
Requests
x1 , x2 , x3 arrived
Begin
Iteration 1
(preﬁlling)
1
Begin
1
Iteration 1
(preﬁlling)
2
3
33111
22
33
Iteration 2
3
3
1
2
3
3311122333111222
33
Iteration 2
3
Request
x4 arrived
Request
x4 arrived
1
Iteration 3
3
1
3
11222
333
Iteration 3
333
444
one decoding step
for x1 , x2 , x3
preﬁlling for x4
1
Iteration 4
3
3
11222
1
333
3
End
x4 is added to
the next batch
(a) Request-level Scheduling
Iteration 4
11
222
33333
4444
3
More iterations
End
(b) Iteration-level Scheduling
Fig. 5.10: Illustrations of request-level scheduling and iteration-based scheduling. In request-level scheduling, once
a batch is created and sent to the inference engine, we cannot adjust the batch. In other words, scheduling only
occurs after the processing of a batch ﬁnishes. In iteration-level scheduling, we can perform scheduling during batch
processing. For example, if a new request arrives at some point during inference, we can add it to the batch and continue
processing.
• The inference engine processes the batch iteration by iteration. After each iteration, the
scheduler may adjust the batch in one of the following ways:
– If a sequence in the batch completes generation (i.e., generates the end-of-sequence
symbol), that sequence is removed from the batch.
– If a new user request arrives and the inference engine has additional processing capac-
ity, it is added to the batch.
– If no sequences are added to or removed from the batch, the batch remains unchanged.
• The processing terminates only when all sequences have been completed and no new user
requests arrive.
See Figure 5.11 for an example of continuous batching. In this example, we start with two user228
Inference
input
input
x1 , x2
arrived
batch
Scheduler
1 1 1
2 2
Scheduler
preﬁlling for x1 and x2 .
output
batch
output
(a) Iteration 1
1 1 1 1
2 2 2
one decoding step for
x1 and x2 .
(b) Iteration 2
input
input
x3 arrived
batch
Scheduler
1 1 1 1 1
2 2 2 2
Scheduler
one decoding step for
x1 and x2 .
output
batch
output
(c) Iteration 3
1 1 1 1 1 1
2 2 2 2 2
3 3
one decoding step for
x1 and x2 , and preﬁlling
for x3 .
(d) Iteration 4
input
input
x4 , x5
arrived
complete
continue the second sequence
in the batch with x4
batch
Scheduler
output
y2
1 1 1 1 1 1 1
2 2 2 2 2 2
3 3 3
one decoding step for
x1 , x2 and x3 .
x2 ’s prediction completes.
(e) Iteration 5
batch
Scheduler
output
1 1 1 1 1 1 1 1
4 4 4
3 3 3 3
one decoding step for
x1 and x3 , and preﬁlling
for x4 .
(f) Iteration 6
Fig. 5.11: Illustration of batch adjustment in continuous batching. Instead of ﬁxing a batch of input sequences and
processing them to completion (as in request-level batching), continuous batching dynamically updates the batch during
inference. The system continuously accepts and adds new requests (e.g., x3 and x4 ) into the current batch as long as
there is available compute capacity.
requests, x1 and x2 . These two sequences are packed into a batch and sent to the inference engine
for processing. After the engine completes two iterations, a new user request, x3 , arrives. At this
point, the scheduler adjusts the batch by adding x3 to it. The inference engine then continues
processing the updated batch. Note that the inference engine now processes different sequences
in different ways: x1 and x2 proceed with the decoding process (i.e., predicting the next tokens),
while x3 undergoes the preﬁlling process. After some time, the generation for x2 completes. As
it happens, two more user requests, x4 and x5 , arrive. The scheduler removes the completed
sequence x2 from the batch and, considering the current load of the inference engine, adds x4 to
the batch. However, x5 must wait until another sequence in the batch ﬁnishes before it can be
added.5.2 Efcient Inference Techniques
229
The idea behind continuous batching is to keep the inference engine fully utilized by process-
ing as many sequences as possible, thereby maximizing computational resource usage. A key dif-
ference between continuous batching and standard batching (see Figure 5.8) lies in the fact that, in
continuous batching, preﬁlling and decoding can occur simultaneously across different sequences,
whereas in standard batching, these two phases are performed sequentially for the entire batch. As
discussed in Section 5.1.2, preﬁlling is considered a compute-bound process, while decoding is
considered a memory-bound process. The intuition behind overlapping preﬁlling and decoding is
to reduce idle times for both computation and data transfer. Consider two mini-batches: one for
preﬁlling and one for decoding. While the preﬁlling mini-batch keeps the GPUs occupied, the
decoding mini-batch can perform memory transfers concurrently.
Another difference between continuous batching and standard batching is that continuous
batching is preﬁlling-prioritized, while standard batching is decoding-prioritized [Agrawal et al.,
2024]. In continuous batching, once the inference engine has spare computational resources, the
scheduler will add new requests to the batch. In other words, these newly added requests will be
processed for preﬁlling as early as possible. This approach improves system throughput, but at the
cost of increased latency, as the newly added requests extend the processing time of earlier ones.
In contrast, in standard batching, once the batch is created, we must wait for the last sequence
in the batch to complete before processing new requests. This ensures relatively low latency, but
results in lower device utilization and system throughput.
It is important to note that the cost of continuous batching is that we need to continuously
reorganize the batches, which involves rearranging the data in memory. Each time a new request
is added, the scheduler needs to reassess and optimize the current batch structure. This dynamic
adjustment can incur additional memory and computational overhead, especially when the batches
are frequently adjusted. Therefore, while this method can improve throughput, it may also lead to
increased memory fragmentation and, in some cases, introduce additional latency.
5.2.2.3
PagedAttention
PagedAttention (or paged KV caching) is a technique used in the vLLM system [Kwon et al.,
2023]. Inspired by operating system paging, it optimizes memory usage during LLM inference
— particularly for the KV cache — by addressing fragmented memory allocation in dynamic
batching scenarios with variable-length sequences. The idea behind PagedAttention is to break
down large memory requirements for KV caching into more manageable "pages" or chunks of
memory. In this way, we do not need to store the KV cache of the full sequence in a continuous
memory. Instead, the KV cache is divided into ﬁxed-size blocks (analogous to memory pages
in an operating system), which can be non-contiguously allocated in physical memory. One ad-
vantage of PagedAttention is that it enables ﬂexible memory management, supporting dynamic
sequence growth without requiring expensive reallocation or copying of large contiguous memory
regions. Note that PagedAttention is not speciﬁcally designed for batching. But it indeed helps
improve memory efﬁciency in batched inference scenarios, where memory management is more
demanding and complicated.
Consider a simple example of memory allocation in Figure 5.12 in which self-attention is per-
formed for a batch consisting of two sequences. For each sequence, we need to attend the current
token to the key-value pairs in the KV cache of this sequence, as required by self-attention. In the230
Inference
attend
KV Cache
Sequence 1
Sequence 2
⟨SOS⟩
⟨pad⟩
Ithinkthismoive
⟨pad⟩⟨pad⟩⟨SOS⟩I
is
really
better
like
than
reading
(a) Two sequences in a batch
Physical Memory Blocks
used
fragmented memory
than
reading
⟨SOS⟩Ithink
movieisbetter
⟨pad⟩⟨pad⟩⟨pad⟩
I
really
this
⟨SOS⟩
like
(b) Memory allocation for KV caching in standard self-attention
Physical Memory Blocks
used
than
reading
⟨SOS⟩Ithinkthis
⟨pad⟩⟨pad⟩⟨pad⟩⟨SOS⟩
movieisbetterIreallylike
(c) Memory allocation for KV caching in PagedAttention
Fig. 5.12: Illustration of memory allocation in PagedAttention. There are two sequences in the batch, as illustrated in
sub-ﬁgure (a). Since the memory is fragmented, the KV cache is stored in a large unused block of memory in standard
self-attention (see sub-ﬁgure (b)), but the fragmented memory is not used. By contrast, in PagedAttention (see sub-
ﬁgure (c)), the KV cache is divided into smaller blocks and thus ﬁts into fragmented memory.
standard implementation of self-attention, the KV cache is stored in a contiguous block of mem-
ory, allowing us to efﬁciently access this continuous memory. However, in a paged KV caching
system, the KV cache is divided into smaller, ﬁxed-size memory blocks which are not necessar-
ily contiguous. These smaller KV cache blocks can be more effectively allocated to fragmented
memory regions, thereby improving memory utilization. Another beneﬁt of distributing chunks of5.2 Efcient Inference Techniques
231
the KV cache across different memory blocks is that it enables parallelization of the caching pro-
cess. For example, if the input sequence is long and the memory bandwidth is sufﬁcient, it would
be beneﬁcial to write and read the key and value vectors of different segments of the sequence in
parallel across multiple memory blocks.
In general, storing contiguous data in non-contiguous regions can cause issues, for example,
accessing fragmented data requires additional seek time, which reduces I/O efﬁciency. However,
when handling large-scale data (e.g., performing multiplication on extremely large matrices), we
typically do not process all the data at once but instead divide it into smaller blocks for block-level
computation. From this perspective, it is also reasonable to partition the attention computation. If
the paging strategy is well designed, the additional overhead in memory access can be minimal,
while the improvement in memory utilization can be signiﬁcant.
5.2.2.4
Chunked Preﬁlling
We have seen that, in iteration-level scheduling, preﬁlling and decoding for different sequences
can occur simultaneously. This can be seen as a preﬁlling-prioritized strategy which can maximize
the throughput. However, one such iteration can take a long time if the input sequence is very long
and the preﬁlling process dominates the computation. In this case, decoding for other sequences
has to wait until the preﬁlling completes, leading to increased latency for generating output tokens.
Therefore, while preﬁlling-prioritized strategies are effective for maximizing hardware utilization,
they may introduce signiﬁcant variability in token generation latency, particularly when the system
is handling a mix of long and short input sequences.
A simple way to reduce decoding latency is to make computations for different sequences in
the batch comparable. One such method is to divide sequences into chunks and perform preﬁlling
chunk by chunk. This approach, often referred to as chunked preﬁlling, processes smaller portions
of each sequence at a time, allowing the system to better balance the computational load across
sequences [Agrawal et al., 2023]. By choosing an appropriate chunk size, we can ensure that when
preﬁlling and decoding overlap for two sequences, their processing within the same iteration tends
to take a similar amount of time. As a result, decoding idle time is reduced and overall throughput
is improved.
Figure 5.13 shows an illustration of chunked preﬁlling in a few iterations. In this example, the
batch contains two sequences. The whole preﬁlling process of the ﬁrst sequence is divided into
three preﬁlling steps, giving rise to the chunks denoted P11 , P12 and P13 . Each chunk corresponds
to one iteration and can thus overlap with one decoding step. In this way, during the preﬁlling of
the ﬁrst sequence, we can perform three decoding steps, rather than only a single decoding step, as
is the case in standard iteration-level scheduling. As a result, the idle time of the decoding process
is reduced, and the output tokens can be generated earlier.
Chunked Preﬁlling improves decoding efﬁciency by overlapping preﬁlling and decoding, but
at the cost of additional memory overhead and scheduling complexity. In standard preﬁlling, we
process the whole input sequence once, building the KV cache in one go. By contrast, in chunked
preﬁlling, each chunk needs a separate forward pass to compute its attention outputs and update
the KV cache. As a result, we need to maintain the KV cache of early chunks while processing
later chunks. This also compromises the parallelism of completing the preﬁlling for the entire
sequence in a single pass. In practice, it is usually possible to balance throughput and latency by
choosing an appropriate chunk size.232
Inference
Preﬁlling in One Go
Iter. 1
Sequence 1
Sequence 2
P21
D21
Iter. 2Iter. 3Iter. 4Iter. 5P11D11D12D13···
D22D23D24···
Idle Time
The prediction of the
second output token
is delayed.
(a) Simple Iteration-level Scheduling
Iter. 1
Sequence 1
Sequence 2
P21
Chunk 1
Iter. 2Chunk 2
Iter. 3Chunk 3
Iter. 4Iter. 5Iter. 6P11P12P13D11D12···
D21D22D23D24D25···
The second output token
can be predicted during
preﬁlling for sequence 1.
(b) Chunked Preﬁlling
Fig. 5.13: Comparison of simple iteration-based scheduling and chunked preﬁlling. Pxy denotes the y-th preﬁlling
step for sequence x, and Dxy denotes the y-th decoding step for sequence x. In simple iteration-based scheduling (or
preﬁlling-prioritized scheduling), since preﬁlling is treated as a single iteration, D22 has to wait for the completion of
the preﬁlling of sequence 1. In chunked preﬁlling, the preﬁlling process can be divided into multiple steps. Thus, D22
can execute during preﬁlling for sequence 1 (i.e., during P12 ).
It is worth noting that the methods discussed in this subsection can broadly be categorized as
priority-based scheduling methods. In these methods, we can give priority to certain requests, or
to certain preﬁlling or decoding steps, so that system resources are allocated in a way that better
aligns with speciﬁc performance goals. As presented above, for example, we may prioritize de-
coding over preﬁlling to minimize token generation latency, or prioritize preﬁlling over decoding
to maximize overall throughput in batch-processing scenarios. Practitioners can design custom
priority policies for speciﬁc needs and operational constraints in real-world applications, such as
request deadlines and importance levels deﬁned by users.
5.2.3
Parallelization
Parallelization is a widely used approach to scale up LLM inference, especially for large-scale
deployments. In Chapter 1, we have discussed several common parallelization strategies to paral-
lelize LLM pre-training, such as model parallelism, tensor parallelism, and pipeline parallelism.
We have also discussed efﬁcient architectures that are easy to deploy in distributed computing
systems. For example, in MoE models, we assigns different experts to different devices3 . Only
the active experts for a given input are executed, which signiﬁcantly improves computational efﬁ-
ciency while maintaining model quality. Many of these methods can be directly applied to LLM
inference with minimal modiﬁcations.
3
In LLMs, the experts are typically modular FFNs. So each expert is a part of the FFN component in the Transformer
architecture.5.2 Efcient Inference Techniques
233
However, applying these parallelization techniques to inference poses new challenges com-
pared to pre-training. These issues become especially pronounced in real-time or low-latency
inference scenarios, where load imbalance across devices and communication overhead can sig-
niﬁcantly impact performance. For example, unlike pre-training, where batches can be prepared
in advance, inference must handle variable-length sequences in real time. This makes it harder
to maintain optimal device utilization and complicates scheduling across heterogeneous compu-
tational resources. A related issue is load balancing. When a large number of requests arrive
in a short period of time, the system must efﬁciently distribute workloads across available de-
vices. For example, real-world requests typically exhibit highly variable computational demands
due to differences in task types and prompt lengths. Such variability renders simple static load
balancing approaches ineffective, and so we need to use ﬁner-grained strategies that can adapt to
runtime conditions. The problem becomes even more complicated when we deploy the system on
heterogeneous hardware and there are strict latency constraints.
In the development of LLMs, parallelization is closely related to LLM serving. Generally,
building a high-quality LLM serving system is not a simple task — it typically requires the com-
bination of multiple techniques, such as architectural design, workload distribution, and LLM-
speciﬁc hardware/software optimizations. As such, LLM serving constitutes an exceptionally
broad subject that often demands substantial engineering expertise. Here, we will not go into the
details of LLM serving. For related concepts and techniques, readers may refer to relevant open-
source systems (such as vLLM4 , TensorRT-LLM5 and TGI6 ) and papers [Pope et al., 2023; Li
et al., 2024a].
5.2.4
Remarks
We have considered many methods for improving the efﬁciency of LLMs in this and previous
chapters. Although these approaches address different issues, most of them essentially explore
trade-offs between various performance factors. One important trade-off is between inference
speed and accuracy. For example, techniques like quantization, pruning, and knowledge dis-
tillation can signiﬁcantly reduce computational overhead and latency but may introduce minor
degradations in model performance. Conversely, preserving full precision or using larger models
enhances accuracy but at the cost of slower inference and higher resource demands.
Another important consideration in LLM inference is the memory-compute trade-off. As in
computer system design, we need to consider the balance between memory usage and computation
required to generate the output. In particular, storing intermediate results such as KV caches dur-
ing inference can signiﬁcantly reduce redundant computation, but at the cost of increased memory
usage. In KV caching, storing past attention states avoids recomputation of self-attention over
previous tokens, thereby reducing compute time per token. However, as the number of tokens
grows, so does the memory footprint of the KV cache, especially when processing very long se-
quences or multiple sequences in parallel. In response, various techniques have been developed to
reduce memory consumption by partially recomputing intermediate states. For instance, chunked
or windowed attention limits the attention span to a recent subset of tokens, reducing KV cache
size at the cost of reduced context or additional compute if past information must be reprocessed.
4
https://github.com/vllm-project/vllm
https://github.com/NVIDIA/TensorRT-LLM
6
https://github.com/huggingface/text-generation-inference
5234
Inference
Note that considering the memory-compute trade-off is a very general principle. It can be ex-
tended beyond attention mechanisms and Transformers to other components in system design. An
example is the choice of data precision. Using lower-precision formats such as FP16 or INT8 can
reduce both memory usage and memory bandwidth requirements, effectively alleviating pressure
on the memory subsystem. However, lower precision may lead to numerical instability or slight
accuracy degradation, requiring careful calibration or retraining. Thus, this trade-off can also be
seen as a memory-compute-accuracy triangle, where improvements in one dimension may come
at the expense of another.
Beyond speed, accuracy, and memory, several other dimensions also inﬂuence LLM inference
efﬁciency. Some of these dimensions have been discussed in this chapter, while others have not.
Here we outline them as follows.
• Throughput vs. Latency: In large-scale multi-user LLM serving scenarios, we often aim
to maximize system throughput. For example, as discussed in this section, we can batch
multiple requests together to increase the number of tokens processed at the same time.
However, batching increases waiting time and may lead to higher per-request latency, espe-
cially for short or interactive requests. By contrast, optimizing for low latency often requires
serving requests individually or in smaller batches, which underutilizes hardware resources
and reduces throughput. Achieving a good balance depends on the quality-of-service re-
quirements and user interaction patterns.
• Generalization vs. Specialization: General-purpose LLMs are trained to perform a wide
range of tasks with a single set of parameters. While ﬂexible, they may be less efﬁcient
or accurate for speciﬁc tasks. Specialized models can yield better performance and lower
inference costs for targeted applications. However, maintaining multiple specialized models
increases system complexity and storage requirements. The trade-off between maintaining a
single general model versus multiple specialized models is an important system-level design
choice.
• Energy Efﬁciency vs. Performance: High-performance inference often requires running
large models at high throughput on powerful accelerators, which consumes considerable
energy. This may be problematic for edge deployments or energy-sensitive environments.
Techniques like model compression can improve energy efﬁciency, but usually with some
degradation in output quality or increase in latency. Energy constraints thus introduce an-
other important dimension in optimizing LLM inference.
5.3
Inference-time Scaling
Scaling laws can be considered one of the fundamental principles guiding the development of
LLMs. In previous chapters, we discussed several times that scaling up training data, model size,
and compute can effectively improve the performance of pretraining. In fact, scaling laws also
apply to downstream stages such as ﬁne-tuning and inference (see Figure 5.14). Here we con-
sider inference-time scaling, which has been widely employed by recent LLMs to solve complex5.3 Inference-time Scaling
235
Performance
Inference-time Scaling
Fine-tuning Scaling
Pre-training Scaling
Compute
Fig. 5.14: Scaling for pre-training, ﬁne-tuning and inference stages [Briski, 2025].
problems, such as complex math problems [Snell et al., 2025]. Unlike pre-training and ﬁne-
tuning scaling, which focuses on improving LLMs via parameter updates, inference-time scaling
improves these models during inference without further training. This includes a large variety of
methods which scale LLMs in different dimensions, such as ensembling multiple model outputs,
increasing context length, adopting more aggressive decoding algorithms, and using external tools
to extend model capabilities.
While inference-time scaling is wide-ranging, in this section we consider those methods that
incorporate more compute into inference (called inference-time compute scaling). Here is a list of
inference-time (test-time) compute scaling methods, organized by category:
• Context Scaling. It involves scaling the input or context to improve generation (or poten-
tially scale the output).
• Search Scaling. It involves increasing computational effort during decoding.
• Output Ensembling. It involves combining multiple model outputs.
• Generating and Verifying Thinking Paths. It involves guiding LLMs to generate and
verify thinking paths for solving complex reasoning problems.
We will describe these methods in the following subsections.
5.3.1
Context Scaling
Context scaling improves LLM performance by extending the input to the model. A straight-
forward approach is to incorporate more helpful context during inference, allowing the model to236
Inference
condition its predictions on more prior information. One example is few-shot prompting. It aug-
ments the context with multiple input-output examples, and so the model can learn task behavior
implicitly from these examples without parameter updates. On top of few-shot prompting, we can
use chain-of-thought prompting to encourage the model to produce intermediate reasoning steps
before ﬁnal answers. Note that chain-of-thought prompting is one of the most important methods
in addressing reasoning problems. By explicitly providing intermediate steps in problem-solving,
we can prompt the model to break down complex tasks into simpler sub-tasks, which is found to
be very beneﬁcial for generating accurate and interpretable outputs.
Beyond extending the prompt with examples or reasoning steps, another approach to context
scaling involves dynamically incorporating external knowledge. This is often achieved through
RAG. RAG systems ﬁrst retrieve relevant document snippets from a large collection of documents
or a database based on the current input. These retrieved pieces of information are then added to
the context provided to the LLM. This essentially expands the context to include timely or spe-
cialized external knowledge. By doing so, the model grounds its responses in speciﬁc knowledge
found in the external source. The LLM thus can generate responses that are not only relevant to
the input but also factually accurate and up-to-date.
However, as the context grows, these methods often suffer from the constraints of ﬁnite con-
text window length. While model architectures and techniques (like efﬁcient attention models)
are continually evolving to support longer contexts, processing extremely long inputs still poses
challenges. Increased computational cost is one factor. More critically, when the context window
becomes very large, the model might struggle to attend effectively to the most relevant informa-
tion (e.g., the “lost in the middle” phenomenon). Therefore, effective context scaling is not just
about adding more information, but also about strategically selecting, structuring, and presenting
the most pertinent information within the model’s processing capabilities.
Here we omit the detailed discussion of these methods, as they have already been covered
in previous chapters. See Chapters 2 and 3 for more details, including prompting, RAG, and
long-sequence modeling methods.
5.3.2
Search Scaling
In LLMs, decoding is a search process that aims to efﬁciently ﬁnd the best output sequence given
the input sequence. Search scaling (or decoding scaling) typically involves two aspects: scaling
the output length and scaling the search space.
Scaling the output length refers to increasing the number of tokens generated during inference.
This is especially important in tasks that require long-form generation, such as story writing. More
recently, generating outputs with long thinking paths has shown strong performance in math prob-
lem solving and code generation. For example, encouraging the model to generate long thinking
paths before producing the ﬁnal answers has been found to be very beneﬁcial in performing com-
plex reasoning. This idea has been widely used in developing recent LLMs for reasoning, such as
OpenAI [2024]’s o1 and Deepseek [2025]’s R1. We will discuss more about output length scaling
in Section 5.3.4.
Scaling the search space, on the other hand, refers to expanding the set of candidate output
sequences considered during search, so that higher-quality outputs can be found. As discussed in
Section 5.1.3, a simple example is that in beam search we increase the beam width to allow more
candidate sequences to be explored in parallel at each decoding step. This increases the chance5.3 Inference-time Scaling
237
of discovering better outputs, especially in tasks where the optimal solution is not immediately
apparent from local decisions.
In addition to decoding algorithm adjustments, it is also possible to explore compact structures
to encode a large number of outputs. For example, we can construct and navigate a tree or graph
of reasoning steps [Yao et al., 2024]. In this paradigm, each node represents a partial solution
or intermediate step, and edges represent transitions between reasoning states. Such structured
search enables the model to consider multiple paths simultaneously. Another related direction is
Monte Carlo tree search-inspired decoding, where the model stochastically explores and scores
different paths based on learned heuristics or external reward models.
Search scaling is a very general idea, and it is often implicitly involved in the design of search
procedures that exploit search structure, heuristics, and model uncertainty. Many of the above
methods have been discussed previously, though they were not originally developed with scal-
ing as their primary goal. However, search scaling inherently comes with computational costs.
Increasing beam width, for instance, directly translates to higher memory usage and longer infer-
ence times. In practice, there is often a point of diminishing returns, where further expansion of
the search space yields marginal improvements in output quality at a signiﬁcant computational ex-
pense. Therefore, an effective strategy often involves ﬁnding an optimal balance between scaling
and computational feasibility.
5.3.3
Output Ensembling
If we have multiple model outputs, it is often beneﬁcial to combine them to mitigate the impact
of individual model errors and synthesize a superior ﬁnal output. Each model might capture
different aspects of the underlying data distribution or possess unique strengths and weaknesses.
By ensembling, we can average out the noise or random errors present in individual predictions,
leading to a more stable and reliable outcome. In LLM ensembling, one of the simplest approaches
is to average the probability distributions over the next token from each model, and select the best
token using this averaged distribution. Or, if we regard the problem as a discrete decision-making
task, majority voting can be employed. More sophisticated methods might involve re-ranking
candidate outputs generated by different models based on a separate scoring function or even
using a meta-learner to intelligently combine the predictions.
The “scaling” from output ensembling comes at the cost of running multiple models or sam-
pling multiple outputs. This not only increases the latency of inference but also leads to the addi-
tional complexity of managing multiple models. But the quality of outputs does not continue to
improve indeﬁnitely as more models are added. In some cases, the beneﬁts of output ensembling
may diminish as the number of component models in the ensemble exceeds a certain threshold.
Instead, the beneﬁts of ensembling are generally greater when the individual models are diverse
(i.e., they make different errors), even if there are a relatively small number of component models.
Therefore, it is common practice to use a set of diverse LLMs which differ in their training data,
model architectures, or ﬁne-tuning objectives.
In LLMs, “scaling” often implies making things “bigger” for quality with more resources.
However, in addition to scaling up the quality, scaling can mean more. It can also signify scaling
up the robustness (making the system less prone to errors and more reliable) and exploration (cov-
ering a wider range of potential solutions). In output ensembling, these dimensions are naturally
integrated. For instance, the very act of averaging or voting across different model outputs is a238
Inference
direct strategy to scale up robustness against individual model failures. Furthermore, by intention-
ally including varied models, ensembling increases the chances of discovering novel or superior
solutions. In this sense, scaling is not limited to making models larger or running them longer —
it also means strategies for making inference more robust, exploratory, and adaptive.
5.3.4
Generating and Verifying Thinking Paths
So far, we have viewed inference-time scaling as a general class of methods for scaling various
aspects of inference, such as sequence length, model size, and/or search strategies. In fact, one
successful application is the use of inference-time scaling to enhance the reasoning capabilities of
LLMs. As we have seen, the reasoning performance of LLMs can be improved by using chain-
of-thought methods. We can therefore make use of the chain-of-thought prompts to generate
intermediate reasoning steps and reach a correct answer. However, reasoning problems are often
so complicated that we cannot obtain high-quality solutions by providing simple chain-of-thought
prompts. For example, when solving a math problem, we typically need to reason over a sequence
of steps. At each step, we need to work out some intermediate result, verify it, and then determine
what to do next. The reasoning path is not a ﬁxed pattern but a dynamically generated thinking
process that often involves trial-and-error, backtracking, and self-correction. This requires more
sophisticated prompting strategies or search algorithms to navigate such complex reasoning. In
this subsection, we focus on inference-scaling methods that go beyond simple chain-of-thought to
address complex reasoning problems more effectively.
At a high level, methods for scaling the reasoning of LLMs can be categorized into two classes:
• Training-free Methods. These methods aim to improve reasoning capabilities without
requiring any modiﬁcation or retraining of the pre-trained parameters. Instead, they focus
on techniques applied during inference, such as sophisticated prompting strategies (e.g.,
chain-of-thought) and algorithmic control over the reasoning process (e.g., search).
• Training-based Methods. These methods involve further training or ﬁne-tuning the model
parameters to explicitly improve reasoning abilities, such as supervised ﬁne-tuning on datasets
with reasoning examples (e.g., math problems with step-by-step solutions).
In the following, we ﬁrst discuss training-free methods, and then training-based methods.
5.3.4.1
Solution-level Search with Veriﬁers
Given an input sequence (e.g., a math problem), there are many possible output sequences (e.g.,
solutions to the problem). If we have a model to evaluate or verify each solution, we can select the
best one. This is the fundamental principle behind methods like best-of-N sampling, where mul-
tiple outputs are generated, and the optimal result is picked based on some selection mechanism.
Such a selection process can be viewed as a search problem, which involves two components:
• Search Algorithm. This deﬁnes the strategy used to explore the space of possible output
sequences (solutions) and generate a set of candidates. It can range from simple independent
sampling to more sophisticated search techniques as discussed in Section 5.1.3.5.3 Inference-time Scaling
239
Problem (x)Express (5 − 4i) − 2(3 + 6i) as a
complex number. Think step by step.
Step 1 (a1 )Now we distribute the 2 to the terms
in the parenthesis. 2(3 + 6i) = 6 + 12i.
Step 2 (a2 )So (5 − 4i) − 2(3 + 6i) is equivalent to
(5 − 4i) − (6 + 12i).
Step 3 (a3 )Now we subtract the terms.
5 − 4i − 6 − 12i = −1 − 16i.
x
That’s the answer.
x
You can also write it as −1 − 16i.
Answer −1 − 16i.
x
Step 4 (a4 )
Step 5 (a5 )
x
Solution with
A Reasoning Path
Fig. 5.15: Illustration of multi-step reasoning. This example is from the PRM800K dataset [Lightman et al., 2024].
Given a math problem, the LLM is prompted to generate a thinking path (or reasoning path) consisting of several
reasoning steps. Each step addresses a sub-problem based on the results of the previous steps. The answer to the
original problem is contained in the last step.
• Veriﬁer. This is a model or function responsible for evaluating the quality, correctness, or
utility of each candidate solution generated by the search algorithm. It provides a score, a
probability, or a judgment that allows the system to select the best among the candidates.
The veriﬁer can be another LLM, or even a set of predeﬁned rules or heuristics.
Given an input problem x, we deﬁne that an output solution y can be represented as a sequence
of reasoning steps:
y = (a1 , a2 , ▷▷▷, anr )
(5.37)
where ai is the i-th reasoning step, and anr is the last step which should contain the answer to the
problem. See Figure 5.15 for an example of a multi-step reasoning path.
The search algorithm can efﬁciently generate a set of candidate solutions
Dc = {y1 , ▷▷▷, yK }
(5.38)
Then, we can use a veriﬁer, which evaluates each solution by the function V (y), to score the
candidates in Dc . The ﬁnal output is the best candidate selected by the veriﬁer
ŷ = arg max V (y)
y∈Dc
(5.39)240
Inference
Although verifying the entire reasoning path is possible, a simpler alternative is to verify only
the ﬁnal reasoning step. In this way the veriﬁer function V (y) is simpliﬁed to depend solely on
the ﬁnal answer contained within anr . This can be achieved in various ways, depending on the
nature of the problem and the expected answer format.
• For some math and coding problems, we can use off-the-shelf tools as veriﬁers. Exam-
ples include proof checkers for mathematical theorems, interpreters or compilers for code
execution, and unit test systems for verifying program correctness against predeﬁned test
cases.
• If there is labeled data for evaluating the answer, such as human preference data, we can
train a reward model on such data. The learned reward model is then used as the veriﬁer
which assigns a scalar score to each candidate answer.
• If there are no existing systems or suitable reward models, we can use another LLM to act
as the veriﬁer. This LLM is prompted to assess the quality of the candidate answer. It could
potentially be a more capable model, or the same LLM used with a speciﬁc “evaluator”
prompt.
• Alternatively, simpler heuristic-based veriﬁers can be designed. A commonly used approach
is to employ majority voting, where the most frequently occurring answer among a set of
candidates is selected.
Based on these veriﬁers, we can search to obtain a set of candidate solutions for selection.
One simple strategy, which is often referred to as parallel scaling [Brown et al., 2024; Snell et al.,
2024], involves generating K candidate solutions by running the base LLM K times indepen-
dently. In this process, we can adjust the temperature in sampling to control the diversity in the
outputs. The veriﬁer then assesses each of these K complete solutions, and the one with the high-
est score is selected as the ﬁnal output. This is conceptually very similar to best-of-N sampling,
which in previous chapters we primarily described as a method of selecting the best one from a
set of sampled outputs using a reward model.
Another approach is sequential scaling, which builds a sequence of solutions incrementally
[Gou et al., 2024; Zhang et al., 2024]. It starts with an initial solution generated by the LLM with
prompting. Then, we use a veriﬁer (often the same LLM) to evaluate the solution. This can be seen
as a critique stage. The output of this stage is some form of feedback, such as textual critiques
pinpointing errors or suggesting improvements, numerical scores reﬂecting solution quality, or
even a revised plan or intermediate step to guide the next generation. This feedback, along with
the original problem and the current solution, is then used to prompt the LLM to generate a
potentially improved solution. This can be seen as a reﬁne stage. This critique-reﬁne cycle can be
repeated, forming an iterative loop:
yk+1 = Rene(x, yk , Feedback(yk ))
(5.40)
where Feedback(yk ) represents the feedback from the veriﬁer. The Rene(·) function generates
the improved solution yk+1 by prompting the LLM with the original problem x, the previous
solution yk , and this feedback. The process can be iterated for K times, or until the solution5.3 Inference-time Scaling
241
Solution 1
y1
g
Samplin
Problem
x
Solution 2
y2
Solution 3
y3
(a) Parallel Scaling
Self-reﬁnement
Problem
x
Solution 1
y1
Solution 2
y2
Solution 3
y3
(b) Sequential Scaling
Fig. 5.16: Illustrations of parallel scaling and sequential scaling. In parallel scaling, we obtain multiple solutions by
running the LLM several times independently. In sequential scaling, the LLM generates an initial solution. Then, we
use the LLM to reﬁne it iteratively, with each reﬁnement yielding a new, possibly better solution.
quality, as assessed by the veriﬁer, converges to a satisfactory level. This iterative framework,
where a solution is progressively improved through cycles of generation, evaluation (critique), and
revision, is precisely what constitutes self-reﬁnement [Shinn et al., 2023; Madaan et al., 2024]. In
such scenarios, the role of the veriﬁeris not just to pick the best complete solution from a static
set, but to actively guide the generation process itself.
See Figure 5.16 for illustrations of parallel scaling and sequential scaling. Note that there
are other ways to perform search and obtain different sets of candidate solutions. One alternative
method is to organize search as a tree structure. This 