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"
  }
][
  {
    "header": {
      "version": 1,
      "block_number": 0,
      "previous_hash": "0000000000000000000000000000000000000000000000000000000000000000",
      "merkle_root": "0000000000000000000000000000000000000000000000000000000000000000",
      "timestamp": "2025-06-02T15:57:39.702095535Z",
      "validator": {
        "method": "atous",
        "identifier": "atous-node",
        "public_key_hash": "022f54d3feabe29313c545a0c43c76dd40caedbb98d55aeff1ff21fc85c3b668"
      },
      "extra": {}
    },
    "transactions": [],
    "signature": "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"
  }
]mod network;
mod p2p;

pub use network::NetworkNodeIsolator;
pub use p2p::P2PNetworkIsolator; use crate::NodeIsolator;
use anyhow::Result;
use async_trait::async_trait;
use std::collections::HashSet;
use tokio::sync::RwLock;

#[derive(Default)]
pub struct NetworkNodeIsolator {
    isolated_nodes: RwLock<HashSet<String>>,
}

impl NetworkNodeIsolator {
    pub fn new() -> Self {
        Self::default()
    }
}

#[async_trait]
impl NodeIsolator for NetworkNodeIsolator {
    async fn isolate_node(&self, node_id: &str) -> Result<()> {
        let mut isolated = self.isolated_nodes.write().await;
        isolated.insert(node_id.to_string());
        Ok(())
    }

    async fn remove_isolation(&self, node_id: &str) -> Result<()> {
        let mut isolated = self.isolated_nodes.write().await;
        isolated.remove(node_id);
        Ok(())
    }

    async fn is_node_isolated(&self, node_id: &str) -> Result<bool> {
        let isolated = self.isolated_nodes.read().await;
        Ok(isolated.contains(node_id))
    }
} use crate::NodeIsolator;
use anyhow::Result;
use libp2p::{
    PeerId,
    Multiaddr,
};
use network::{
    Network as NetworkService,
    types::NetworkEvent,
};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::{Mutex, RwLock};

const ISOLATION_TOPIC: &str = "node-isolation";

struct SyncNetwork {
    network: Arc<Mutex<NetworkService>>,
}

impl SyncNetwork {
    fn new(network: NetworkService) -> Self {
        SyncNetwork {
            network: Arc::new(Mutex::new(network))
        }
    }

    async fn next_event(&self) -> Option<NetworkEvent> {
        let mut network = self.network.lock().await;
        network.next_event().await
    }

    async fn publish(&self, topic: &str, data: Vec<u8>) -> Result<()> {
        let mut network = self.network.lock().await;
        network.publish(topic, data)
            .map(|_| ())
            .map_err(|e| anyhow::anyhow!("Network error: {}", e))
    }

    async fn add_address(&self, peer_id: PeerId, addr: impl Into<Multiaddr>) {
        let mut network = self.network.lock().await;
        network.add_address(peer_id, addr.into());
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum IsolationMessage {
    IsolateNode {
        node_id: String,
        reason: String,
        timestamp: u64,
    },
    RemoveIsolation {
        node_id: String,
        timestamp: u64,
    },
}

pub struct P2PNetworkIsolator {
    network: Arc<SyncNetwork>,
    isolated_nodes: Arc<RwLock<HashSet<String>>>,
    peer_map: Arc<RwLock<HashMap<String, PeerId>>>,
}

impl P2PNetworkIsolator {
    pub fn new(network: NetworkService) -> Self {
        let isolator = P2PNetworkIsolator {
            network: Arc::new(SyncNetwork::new(network)),
            isolated_nodes: Arc::new(RwLock::new(HashSet::new())),
            peer_map: Arc::new(RwLock::new(HashMap::new())),
        };

        isolator.subscribe_to_isolation_topic();
        isolator
    }

    fn subscribe_to_isolation_topic(&self) {
        let network = self.network.clone();
        let isolated_nodes = self.isolated_nodes.clone();
        let peer_map = self.peer_map.clone();

        tokio::spawn(async move {
            loop {
                if let Some(event) = network.next_event().await {
                    match event {
                        NetworkEvent::MessageReceived(_topic, data, _source) => {
                            if let Ok(isolation_msg) = serde_json::from_slice::<IsolationMessage>(&data) {
                                match isolation_msg {
                                    IsolationMessage::IsolateNode { node_id, .. } => {
                                        let mut nodes = isolated_nodes.write().await;
                                        nodes.insert(node_id.clone());
                                        
                                        if let Some(peer_id) = peer_map.read().await.get(&node_id) {
                                            if let Ok(addr) = "/ip4/0.0.0.0/tcp/0".parse::<Multiaddr>() {
                                                let _ = network.add_address(*peer_id, addr).await;
                                            }
                                        }
                                    }
                                    IsolationMessage::RemoveIsolation { node_id, .. } => {
                                        let mut nodes = isolated_nodes.write().await;
                                        nodes.remove(&node_id);
                                    }
                                }
                            }
                        }
                        NetworkEvent::PeerDiscovered(peer_id) => {
                            let mut peer_map = peer_map.write().await;
                            peer_map.insert(peer_id.to_string(), peer_id);
                        }
                        _ => {}
                    }
                }
            }
        });
    }

    async fn broadcast_isolation_message(&self, message: IsolationMessage) -> Result<()> {
        let msg_data = serde_json::to_vec(&message)?;
        self.network.publish(ISOLATION_TOPIC, msg_data).await?;
        Ok(())
    }
}

#[async_trait::async_trait]
impl NodeIsolator for P2PNetworkIsolator {
    async fn isolate_node(&self, node_id: &str) -> Result<()> {
        let message = IsolationMessage::IsolateNode {
            node_id: node_id.to_string(),
            reason: "Node flagged for malicious behavior".to_string(),
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        };

        self.broadcast_isolation_message(message).await?;

        let mut isolated = self.isolated_nodes.write().await;
        isolated.insert(node_id.to_string());

        if let Some(peer_id) = self.peer_map.read().await.get(node_id) {
            if let Ok(addr) = "/ip4/0.0.0.0/tcp/0".parse::<Multiaddr>() {
                let _ = self.network.add_address(*peer_id, addr).await;
            }
        }

        Ok(())
    }

    async fn remove_isolation(&self, node_id: &str) -> Result<()> {
        let message = IsolationMessage::RemoveIsolation {
            node_id: node_id.to_string(),
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        };

        self.broadcast_isolation_message(message).await?;

        let mut isolated = self.isolated_nodes.write().await;
        isolated.remove(node_id);

        Ok(())
    }

    async fn is_node_isolated(&self, node_id: &str) -> Result<bool> {
        let isolated = self.isolated_nodes.read().await;
        Ok(isolated.contains(node_id))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use libp2p::identity::Keypair;

    #[tokio::test]
    async fn test_sync_network_operations() {
        let keypair = Keypair::generate_ed25519();
        let network = NetworkService::new(&keypair).await.unwrap();
        let sync_network = SyncNetwork::new(network);

        // Test publish
        let result = sync_network.publish("test-topic", b"test".to_vec()).await;
        assert!(result.is_ok());

        // Test add_address
        let peer_id = PeerId::random();
        let addr = "/ip4/127.0.0.1/tcp/1234".parse::<Multiaddr>().unwrap();
        sync_network.add_address(peer_id, addr).await;

        // Test next_event
        let event = sync_network.next_event().await;
        assert!(event.is_none()); // Since we haven't set up any real network activity
    }

    #[tokio::test]
    async fn test_p2p_network_isolator() {
        let keypair = Keypair::generate_ed25519();
        let network = NetworkService::new(&keypair).await.unwrap();
        let isolator = P2PNetworkIsolator::new(network);

        // Test isolation
        let node_id = "test-node";
        let result = isolator.isolate_node(node_id).await;
        assert!(result.is_ok());

        // Test is_isolated
        let is_isolated = isolator.is_node_isolated(node_id).await;
        assert!(is_isolated.unwrap());

        // Test remove isolation
        let result = isolator.remove_isolation(node_id).await;
        assert!(result.is_ok());
        let is_isolated = isolator.is_node_isolated(node_id).await;
        assert!(!is_isolated.unwrap());
    }
} use crate::{Flag, FlagStorage};
use anyhow::Result;
use blake3::Hash;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Block {
    timestamp: u64,
    prev_hash: String,
    merkle_root: String,
    flags: Vec<Flag>,
    nonce: u64,
}

impl Block {
    fn new(prev_hash: String, flags: Vec<Flag>) -> Self {
        let timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        let mut block = Block {
            timestamp,
            prev_hash,
            merkle_root: String::new(),
            flags,
            nonce: 0,
        };
        
        block.merkle_root = block.calculate_merkle_root();
        block
    }

    fn calculate_merkle_root(&self) -> String {
        let mut hashes: Vec<Hash> = self.flags
            .iter()
            .map(|flag| {
                let serialized = serde_json::to_vec(flag).unwrap();
                blake3::hash(&serialized)
            })
            .collect();

        while hashes.len() > 1 {
            let mut new_hashes = Vec::new();
            for chunk in hashes.chunks(2) {
                let mut hasher = blake3::Hasher::new();
                hasher.update(chunk[0].as_bytes());
                if chunk.len() > 1 {
                    hasher.update(chunk[1].as_bytes());
                } else {
                    hasher.update(chunk[0].as_bytes()); // Duplicate last hash if odd number
                }
                new_hashes.push(hasher.finalize());
            }
            hashes = new_hashes;
        }

        hex::encode(hashes[0].as_bytes())
    }

    fn calculate_hash(&self) -> String {
        let serialized = serde_json::to_vec(self).unwrap();
        let hash = blake3::hash(&serialized);
        hex::encode(hash.as_bytes())
    }
}

pub struct BlockchainStorage {
    chain: Arc<RwLock<Vec<Block>>>,
    pending_flags: Arc<RwLock<Vec<Flag>>>,
    max_flags_per_block: usize,
}

impl BlockchainStorage {
    pub fn new(max_flags_per_block: usize) -> Self {
        let genesis_block = Block::new(
            "0000000000000000000000000000000000000000000000000000000000000000".to_string(),
            Vec::new(),
        );

        let mut chain = Vec::new();
        chain.push(genesis_block);

        BlockchainStorage {
            chain: Arc::new(RwLock::new(chain)),
            pending_flags: Arc::new(RwLock::new(Vec::new())),
            max_flags_per_block,
        }
    }

    async fn create_new_block(&self) -> Result<()> {
        let mut pending = self.pending_flags.write().await;
        if pending.is_empty() {
            return Ok(());
        }

        let chain = self.chain.read().await;
        let prev_hash = chain.last().unwrap().calculate_hash();
        let drain_amount = std::cmp::min(pending.len(), self.max_flags_per_block);
        let flags: Vec<_> = pending.drain(..drain_amount).collect();
        
        let new_block = Block::new(prev_hash, flags);
        drop(chain);

        let mut chain = self.chain.write().await;
        chain.push(new_block);
        
        Ok(())
    }

    pub async fn verify_chain(&self) -> Result<bool> {
        let chain = self.chain.read().await;
        
        for i in 1..chain.len() {
            let current = &chain[i];
            let previous = &chain[i - 1];
            
            // Verify previous hash
            if current.prev_hash != previous.calculate_hash() {
                return Ok(false);
            }
            
            // Verify merkle root
            if current.merkle_root != current.calculate_merkle_root() {
                return Ok(false);
            }
        }
        
        Ok(true)
    }
}

#[async_trait::async_trait]
impl FlagStorage for BlockchainStorage {
    async fn store_flag(&self, flag: Flag) -> Result<()> {
        let mut pending = self.pending_flags.write().await;
        pending.push(flag);
        
        if pending.len() >= self.max_flags_per_block {
            drop(pending);
            self.create_new_block().await?;
        }
        
        Ok(())
    }

    async fn get_flags_for_node(&self, node_id: &str) -> Result<Vec<Flag>> {
        let chain = self.chain.read().await;
        let pending = self.pending_flags.read().await;
        
        let mut flags = Vec::new();
        
        // Check blocks
        for block in chain.iter() {
            flags.extend(
                block
                    .flags
                    .iter()
                    .filter(|f| f.suspect_id == node_id)
                    .cloned(),
            );
        }
        
        // Check pending flags
        flags.extend(
            pending
                .iter()
                .filter(|f| f.suspect_id == node_id)
                .cloned(),
        );
        
        Ok(flags)
    }

    async fn get_flags_in_timerange(&self, start: u64, end: u64) -> Result<Vec<Flag>> {
        let chain = self.chain.read().await;
        let pending = self.pending_flags.read().await;
        
        let mut flags = Vec::new();
        
        // Check blocks
        for block in chain.iter() {
            if block.timestamp >= start && block.timestamp <= end {
                flags.extend(block.flags.clone());
            }
        }
        
        // Check pending flags
        flags.extend(
            pending
                .iter()
                .filter(|f| f.timestamp >= start && f.timestamp <= end)
                .cloned(),
        );
        
        Ok(flags)
    }
} use crate::{Flag, FlagStorage};
use anyhow::Result;
use std::collections::HashMap;
use tokio::sync::RwLock;

pub struct InMemoryFlagStorage {
    flags: RwLock<HashMap<String, Vec<Flag>>>,
}

impl InMemoryFlagStorage {
    pub fn new() -> Self {
        InMemoryFlagStorage {
            flags: RwLock::new(HashMap::new()),
        }
    }
}

#[async_trait::async_trait]
impl FlagStorage for InMemoryFlagStorage {
    async fn store_flag(&self, flag: Flag) -> Result<()> {
        let mut flags = self.flags.write().await;
        flags
            .entry(flag.suspect_id.clone())
            .or_insert_with(Vec::new)
            .push(flag);
        Ok(())
    }

    async fn get_flags_for_node(&self, node_id: &str) -> Result<Vec<Flag>> {
        let flags = self.flags.read().await;
        Ok(flags
            .get(node_id)
            .cloned()
            .unwrap_or_else(Vec::new))
    }

    async fn get_flags_in_timerange(&self, start: u64, end: u64) -> Result<Vec<Flag>> {
        let flags = self.flags.read().await;
        let mut result = Vec::new();

        for flag_list in flags.values() {
            result.extend(
                flag_list
                    .iter()
                    .filter(|f| f.timestamp >= start && f.timestamp <= end)
                    .cloned(),
            );
        }

        Ok(result)
    }
} mod blockchain;
mod memory;

pub use blockchain::BlockchainStorage;
pub use memory::InMemoryFlagStorage; use crate::{Flag, FlagStorage};
use anyhow::Result;
use async_trait::async_trait;
use redis::{aio::Connection, AsyncCommands};
use serde_json;
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct RedisFlagStorage {
    connection: Arc<Mutex<Connection>>,
    prefix: String,
}

impl RedisFlagStorage {
    pub async fn new(redis_url: &str, prefix: &str) -> Result<Self> {
        let client = redis::Client::open(redis_url)?;
        let connection = client.get_async_connection().await?;
        
        Ok(Self {
            connection: Arc::new(Mutex::new(connection)),
            prefix: prefix.to_string(),
        })
    }

    fn get_node_key(&self, node_id: &str) -> String {
        format!("{}:node:{}", self.prefix, node_id)
    }

    fn get_time_key(&self, timestamp: u64) -> String {
        format!("{}:time:{}", self.prefix, timestamp)
    }
}

#[async_trait]
impl FlagStorage for RedisFlagStorage {
    async fn store_flag(&self, flag: Flag) -> Result<()> {
        let mut conn = self.connection.lock().await;
        let flag_json = serde_json::to_string(&flag)?;
        
        // Store by node ID
        let node_key = self.get_node_key(&flag.suspect_id);
        conn.lpush(&node_key, &flag_json).await?;
        
        // Store by timestamp
        let time_key = self.get_time_key(flag.timestamp);
        conn.lpush(&time_key, &flag_json).await?;
        
        // Set expiration for time-based key (30 days)
        conn.expire(&time_key, 2592000).await?;
        
        Ok(())
    }

    async fn get_flags_for_node(&self, node_id: &str) -> Result<Vec<Flag>> {
        let mut conn = self.connection.lock().await;
        let node_key = self.get_node_key(node_id);
        
        let flags_json: Vec<String> = conn.lrange(&node_key, 0, -1).await?;
        let flags = flags_json
            .into_iter()
            .filter_map(|json| serde_json::from_str(&json).ok())
            .collect();
        
        Ok(flags)
    }

    async fn get_flags_in_timerange(&self, start: u64, end: u64) -> Result<Vec<Flag>> {
        let mut conn = self.connection.lock().await;
        let mut flags = Vec::new();
        
        for timestamp in start..=end {
            let time_key = self.get_time_key(timestamp);
            let flags_json: Vec<String> = conn.lrange(&time_key, 0, -1).await?;
            flags.extend(
                flags_json
                    .into_iter()
                    .filter_map(|json| serde_json::from_str(&json).ok())
            );
        }
        
        Ok(flags)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{FlagReason, SeverityLevel};
    use std::time::{SystemTime, UNIX_EPOCH};

    async fn create_test_flag() -> Flag {
        Flag::new(
            "test_flagger".to_string(),
            "test_suspect".to_string(),
            "test_evidence_hash".to_string(),
            FlagReason::MaliciousActivity,
            SeverityLevel::High,
            vec![0; 64],
        )
    }

    #[tokio::test]
    async fn test_redis_storage() -> Result<()> {
        let storage = RedisFlagStorage::new("redis://127.0.0.1/", "test").await?;
        let flag = create_test_flag().await;
        
        // Test storing flag
        storage.store_flag(flag.clone()).await?;
        
        // Test retrieving flags for node
        let node_flags = storage.get_flags_for_node(&flag.suspect_id).await?;
        assert_eq!(node_flags.len(), 1);
        assert_eq!(node_flags[0].flagger_id, flag.flagger_id);
        
        // Test time range query
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let flags = storage
            .get_flags_in_timerange(current_time - 3600, current_time + 3600)
            .await?;
        assert_eq!(flags.len(), 1);
        
        Ok(())
    }
} // use anyhow::Result;
// use libp2p::identity::Keypair;
// pub use network::Network;

// pub async fn create_test_network() -> Result<Network> {
//     let keypair = Keypair::generate_ed25519();
//     Network::new(&keypair).await
// } use crate::{
    BlockchainStorage, ConsensusValidator, Flag, FlagReason, FlagSystemConfig,
    FlagSystemManager, InMemoryFlagStorage, MockNodeIsolator, SeverityLevel,
};
use anyhow::Result;
use std::sync::Arc;

async fn setup_test_system() -> Result<(FlagSystemManager, Arc<MockNodeIsolator>)> {
    let storage = Arc::new(InMemoryFlagStorage::new());
    let validator = Arc::new(ConsensusValidator::new(
        vec!["validator1".to_string(), "validator2".to_string()],
        2,
        Arc::new(crate::BasicFlagValidator::new(Default::default())),
    ));
    let isolator = Arc::new(MockNodeIsolator::new());
    
    let config = FlagSystemConfig {
        min_flags_for_isolation: 2,
        flag_window_seconds: 3600,
        immediate_isolation_severity: SeverityLevel::Critical,
        min_unique_flaggers: 2,
    };
    
    let manager = FlagSystemManager::new(
        config,
        storage,
        validator,
        isolator.clone(),
    );
    
    Ok((manager, isolator))
}

#[tokio::test]
async fn test_flag_processing_workflow() -> Result<()> {
    let (manager, isolator) = setup_test_system().await?;
    let suspect_id = "malicious_node";
    
    // Create flags from different flaggers
    let flag1 = Flag::new(
        "flagger1".to_string(),
        suspect_id.to_string(),
        "evidence_hash1".to_string(),
        FlagReason::MaliciousActivity,
        SeverityLevel::High,
        vec![0; 64],
    );
    
    let flag2 = Flag::new(
        "flagger2".to_string(),
        suspect_id.to_string(),
        "evidence_hash2".to_string(),
        FlagReason::MaliciousActivity,
        SeverityLevel::High,
        vec![0; 64],
    );
    
    // Process flags
    manager.process_flag(flag1, &[1, 2, 3]).await?;
    assert!(!isolator.is_node_isolated(suspect_id).await?);
    
    manager.process_flag(flag2, &[4, 5, 6]).await?;
    assert!(isolator.is_node_isolated(suspect_id).await?);
    
    Ok(())
}

#[tokio::test]
async fn test_immediate_isolation() -> Result<()> {
    let (manager, isolator) = setup_test_system().await?;
    let suspect_id = "critical_node";
    
    // Create a flag with critical severity
    let flag = Flag::new(
        "flagger1".to_string(),
        suspect_id.to_string(),
        "evidence_hash".to_string(),
        FlagReason::MaliciousActivity,
        SeverityLevel::Critical,
        vec![0; 64],
    );
    
    // Process flag
    manager.process_flag(flag, &[1, 2, 3]).await?;
    assert!(isolator.is_node_isolated(suspect_id).await?);
    
    Ok(())
}

#[tokio::test]
async fn test_flag_expiration() -> Result<()> {
    let (manager, isolator) = setup_test_system().await?;
    let suspect_id = "expired_node";
    
    // Create old flags
    let flag1 = Flag::new(
        "flagger1".to_string(),
        suspect_id.to_string(),
        "evidence_hash1".to_string(),
        FlagReason::MaliciousActivity,
        SeverityLevel::High,
        vec![0; 64],
    );
    
    let flag2 = Flag::new(
        "flagger2".to_string(),
        suspect_id.to_string(),
        "evidence_hash2".to_string(),
        FlagReason::MaliciousActivity,
        SeverityLevel::High,
        vec![0; 64],
    );
    
    // Process flags
    manager.process_flag(flag1, &[1, 2, 3]).await?;
    manager.process_flag(flag2, &[4, 5, 6]).await?;
    
    // Check isolation status after window expiration
    tokio::time::sleep(tokio::time::Duration::from_secs(3601)).await;
    assert!(!isolator.is_node_isolated(suspect_id).await?);
    
    Ok(())
}

#[tokio::test]
async fn test_blockchain_persistence() -> Result<()> {
    let storage = Arc::new(BlockchainStorage::new(5));
    let validator = Arc::new(ConsensusValidator::new(
        vec!["validator1".to_string()],
        1,
        Arc::new(crate::BasicFlagValidator::new(Default::default())),
    ));
    let isolator = Arc::new(MockNodeIsolator::new());
    
    let config = FlagSystemConfig {
        min_flags_for_isolation: 1,
        flag_window_seconds: 3600,
        immediate_isolation_severity: SeverityLevel::Critical,
        min_unique_flaggers: 1,
    };
    
    let manager = FlagSystemManager::new(
        config,
        storage.clone(),
        validator,
        isolator,
    );
    
    // Create and process a flag
    let flag = Flag::new(
        "flagger1".to_string(),
        "persistent_node".to_string(),
        "evidence_hash".to_string(),
        FlagReason::MaliciousActivity,
        SeverityLevel::High,
        vec![0; 64],
    );
    
    manager.process_flag(flag.clone(), &[1, 2, 3]).await?;
    
    // Verify flag persistence
    let stored_flags = storage.get_flags_for_node("persistent_node").await?;
    assert_eq!(stored_flags.len(), 1);
    assert_eq!(stored_flags[0].flagger_id, flag.flagger_id);
    
    Ok(())
} use crate::{Flag, FlagStorage, FlagValidator, NodeIsolator};
use anyhow::Result;
use async_trait::async_trait;
use std::collections::{HashMap, HashSet};
use std::sync::Arc;
use tokio::sync::RwLock;

/// Mock implementation of FlagStorage for testing
pub struct MockFlagStorage {
    flags: Arc<RwLock<HashMap<String, Vec<Flag>>>>,
}

impl MockFlagStorage {
    pub fn new() -> Self {
        Self {
            flags: Arc::new(RwLock::new(HashMap::new())),
        }
    }
}

#[async_trait]
impl FlagStorage for MockFlagStorage {
    async fn store_flag(&self, flag: Flag) -> Result<()> {
        let mut flags = self.flags.write().await;
        flags
            .entry(flag.suspect_id.clone())
            .or_insert_with(Vec::new)
            .push(flag);
        Ok(())
    }

    async fn get_flags_for_node(&self, node_id: &str) -> Result<Vec<Flag>> {
        let flags = self.flags.read().await;
        Ok(flags.get(node_id).cloned().unwrap_or_default())
    }

    async fn get_flags_in_timerange(&self, start: u64, end: u64) -> Result<Vec<Flag>> {
        let flags = self.flags.read().await;
        let mut result = Vec::new();
        for flag_list in flags.values() {
            result.extend(
                flag_list
                    .iter()
                    .filter(|f| f.timestamp >= start && f.timestamp <= end)
                    .cloned(),
            );
        }
        Ok(result)
    }
}

/// Mock implementation of FlagValidator for testing
pub struct MockFlagValidator {
    valid_flags: Arc<RwLock<HashSet<String>>>,
}

impl MockFlagValidator {
    pub fn new() -> Self {
        Self {
            valid_flags: Arc::new(RwLock::new(HashSet::new())),
        }
    }

    pub async fn set_valid(&self, flag_id: &str, is_valid: bool) {
        let mut valid_flags = self.valid_flags.write().await;
        if is_valid {
            valid_flags.insert(flag_id.to_string());
        } else {
            valid_flags.remove(flag_id);
        }
    }
}

#[async_trait]
impl FlagValidator for MockFlagValidator {
    async fn validate_flag(&self, flag: &Flag, _evidence: &[u8]) -> Result<bool> {
        let valid_flags = self.valid_flags.read().await;
        Ok(valid_flags.contains(&format!("{}:{}", flag.flagger_id, flag.suspect_id)))
    }
}

/// Mock implementation of NodeIsolator for testing
pub struct MockNodeIsolator {
    isolated_nodes: Arc<RwLock<HashSet<String>>>,
}

impl MockNodeIsolator {
    pub fn new() -> Self {
        Self {
            isolated_nodes: Arc::new(RwLock::new(HashSet::new())),
        }
    }
}

#[async_trait]
impl NodeIsolator for MockNodeIsolator {
    async fn isolate_node(&self, node_id: &str) -> Result<()> {
        let mut nodes = self.isolated_nodes.write().await;
        nodes.insert(node_id.to_string());
        Ok(())
    }

    async fn remove_isolation(&self, node_id: &str) -> Result<()> {
        let mut nodes = self.isolated_nodes.write().await;
        nodes.remove(node_id);
        Ok(())
    }

    async fn is_node_isolated(&self, node_id: &str) -> Result<bool> {
        let nodes = self.isolated_nodes.read().await;
        Ok(nodes.contains(node_id))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{FlagReason, SeverityLevel};

    async fn create_test_flag(flagger: &str, suspect: &str) -> Flag {
        Flag::new(
            flagger.to_string(),
            suspect.to_string(),
            "test_evidence_hash".to_string(),
            FlagReason::MaliciousActivity,
            SeverityLevel::High,
            vec![0; 64], // Dummy signature
        )
    }

    #[tokio::test]
    async fn test_mock_storage() -> Result<()> {
        let storage = MockFlagStorage::new();
        let flag = create_test_flag("flagger1", "suspect1").await;
        
        storage.store_flag(flag.clone()).await?;
        let flags = storage.get_flags_for_node("suspect1").await?;
        assert_eq!(flags.len(), 1);
        assert_eq!(flags[0].flagger_id, "flagger1");
        
        Ok(())
    }

    #[tokio::test]
    async fn test_mock_validator() -> Result<()> {
        let validator = MockFlagValidator::new();
        let flag = create_test_flag("flagger1", "suspect1").await;
        
        validator.set_valid("flagger1:suspect1", true).await;
        assert!(validator.validate_flag(&flag, &[]).await?);
        
        validator.set_valid("flagger1:suspect1", false).await;
        assert!(!validator.validate_flag(&flag, &[]).await?);
        
        Ok(())
    }

    #[tokio::test]
    async fn test_mock_isolator() -> Result<()> {
        let isolator = MockNodeIsolator::new();
        
        assert!(!isolator.is_node_isolated("node1").await?);
        isolator.isolate_node("node1").await?;
        assert!(isolator.is_node_isolated("node1").await?);
        isolator.remove_isolation("node1").await?;
        assert!(!isolator.is_node_isolated("node1").await?);
        
        Ok(())
    }
} // use crate::{
//     BlockchainStorage, ConsensusValidator, Flag, FlagReason, FlagSystemConfig,
//     FlagSystemManager, FlagValidator, P2PNetworkIsolator, SeverityLevel,
//     FlagStorage, NodeIsolator,
// };
// use anyhow::Result;
// use ed25519_dalek::SigningKey;
// use rand::thread_rng;
// use async_trait::async_trait;
// use std::sync::Arc;

// mod mocks;

// // Test helper functions
// pub(crate) async fn create_test_validator(
//     validator_count: usize,
//     required_votes: usize,
// ) -> (ConsensusValidator, Vec<SigningKey>) {
//     let mut validators = Vec::new();
//     let mut test_keys = Vec::new();
    
//     for i in 0..validator_count {
//         let key = SigningKey::generate(&mut thread_rng());
//         validators.push(format!("validator{}", i));
//         test_keys.push(key);
//     }
    
//     let base_validator = Arc::new(create_test_base_validator());
//     (ConsensusValidator::new(validators, required_votes, base_validator), test_keys)
// }

// pub(crate) fn create_test_base_validator() -> impl FlagValidator {
//     struct TestValidator;
    
//     #[async_trait]
//     impl FlagValidator for TestValidator {
//         async fn validate_flag(&self, _flag: &Flag, _evidence: &[u8]) -> Result<bool> {
//             Ok(true)
//         }
//     }
    
//     TestValidator
// }

// pub(crate) fn create_test_flag_with_key(key: &SigningKey) -> Flag {
//     Flag::new(
//         "test_flagger".to_string(),
//         "test_suspect".to_string(),
//         "test_evidence_hash".to_string(),
//         FlagReason::MaliciousActivity,
//         SeverityLevel::High,
//         vec![0; 64], // Dummy signature
//     )
// }

// mod blockchain_tests {
//     use super::*;

//     #[tokio::test]
//     async fn test_blockchain_storage() -> Result<()> {
//         let storage = BlockchainStorage::new(5);
        
//         // Create test flags
//         let flags = create_test_flags(10);
        
//         // Store flags
//         for flag in flags.iter() {
//             storage.store_flag(flag.clone()).await?;
//         }
        
//         // Verify chain integrity
//         assert!(storage.verify_chain().await?);
        
//         // Test retrieval
//         let node_flags = storage.get_flags_for_node("node1").await?;
//         assert_eq!(node_flags.len(), 5);
        
//         Ok(())
//     }

//     #[tokio::test]
//     async fn test_flag_storage_consistency() -> Result<()> {
//         let storage = BlockchainStorage::new(4);
//         let flags = create_test_flags(4);
        
//         // Store flags
//         for flag in flags.iter() {
//             storage.store_flag(flag.clone()).await?;
//         }
        
//         // Verify chain integrity
//         assert!(storage.verify_chain().await?);
        
//         // Verify flags are stored correctly
//         for flag in flags {
//             let stored_flags = storage.get_flags_for_node(&flag.suspect_id).await?;
//             assert!(stored_flags.iter().any(|f| f.evidence_hash == flag.evidence_hash));
//         }
        
//         Ok(())
//     }
// }

// mod consensus_tests {
//     use super::*;

//     #[tokio::test]
//     async fn test_consensus_validation() -> Result<()> {
//         let (validator, test_keys) = create_test_validator(3, 2).await;
//         let flag = create_test_flag_with_key(&test_keys[0]);
        
//         // First validation should add initial vote
//         assert!(!validator.validate_flag(&flag, b"test evidence").await?);
        
//         // Add second vote
//         validator.add_vote("test_flag", "validator2", true).await?;
        
//         // Now should have consensus
//         assert!(validator.validate_flag(&flag, b"test evidence").await?);
        
//         Ok(())
//     }

//     #[tokio::test]
//     async fn test_consensus_rejection() -> Result<()> {
//         let (validator, test_keys) = create_test_validator(3, 2).await;
//         let flag = create_test_flag_with_key(&test_keys[0]);
        
//         // Add rejecting votes
//         validator.add_vote("test_flag", "validator1", false).await?;
//         validator.add_vote("test_flag", "validator2", false).await?;
        
//         // Should be rejected
//         assert!(!validator.validate_flag(&flag, b"test evidence").await?);
        
//         Ok(())
//     }
// }

// mod network_tests {
//     use super::*;

//     #[tokio::test]
//     async fn test_network_isolation() -> Result<()> {
//         let keypair = Keypair::generate_ed25519();
//         let network = Network::new(&keypair).await?;
//         let isolator = P2PNetworkIsolator::new(network);
        
//         // Test isolation
//         isolator.isolate_node("malicious_node").await?;
//         assert!(isolator.is_node_isolated("malicious_node").await?);
        
//         // Test removal
//         isolator.remove_isolation("malicious_node").await?;
//         assert!(!isolator.is_node_isolated("malicious_node").await?);
        
//         Ok(())
//     }

//     #[tokio::test]
//     async fn test_isolation_propagation() -> Result<()> {
//         let keypair1 = Keypair::generate_ed25519();
//         let keypair2 = Keypair::generate_ed25519();
//         let network1 = Network::new(&keypair1).await?;
//         let network2 = Network::new(&keypair2).await?;
        
//         let isolator1 = P2PNetworkIsolator::new(network1);
//         let isolator2 = P2PNetworkIsolator::new(network2);
        
//         // Isolate node on first network
//         isolator1.isolate_node("malicious_node").await?;
        
//         // Wait for propagation
//         tokio::time::sleep(Duration::from_millis(100)).await;
        
//         // Check if second network received isolation
//         assert!(isolator2.is_node_isolated("malicious_node").await?);
        
//         Ok(())
//     }
// }

// mod integration_tests {
//     use super::*;

//     #[tokio::test]
//     async fn test_complete_flag_system() -> Result<()> {
//         let keypair = Keypair::generate_ed25519();
//         let network = Network::new(&keypair).await?;
//         let storage = Arc::new(BlockchainStorage::new(5));
//         let isolator = Arc::new(P2PNetworkIsolator::new(network));
//         let (validator, test_keys): (ConsensusValidator, Vec<SigningKey>) = create_test_validator(3, 2).await;
//         let validator = Arc::new(validator);
        
//         let config = FlagSystemConfig {
//             min_flags_for_isolation: 2,
//             flag_window_seconds: 3600,
//             immediate_isolation_severity: SeverityLevel::Critical,
//             min_unique_flaggers: 2,
//         };
        
//         let manager = FlagSystemManager::new(
//             config,
//             storage.clone(),
//             validator.clone(),
//             isolator.clone(),
//         );
        
//         // Create and process flags
//         let flag1 = create_test_flag_with_key(&test_keys[0]);
//         let flag2 = create_test_flag_with_key(&test_keys[1]);
        
//         // Process flags
//         manager.process_flag(flag1.clone(), b"test evidence").await?;
//         manager.process_flag(flag2.clone(), b"test evidence").await?;
        
//         // Verify storage
//         let stored_flags = storage.get_flags_for_node(&flag1.suspect_id).await?;
//         assert_eq!(stored_flags.len(), 2);
        
//         // Verify isolation
//         assert!(isolator.is_node_isolated(&flag1.suspect_id).await?);
        
//         Ok(())
//     }

//     #[tokio::test]
//     async fn test_stress_concurrent_flags() -> Result<()> {
//         let keypair = Keypair::generate_ed25519();
//         let network = Network::new(&keypair).await?;
//         let storage = Arc::new(BlockchainStorage::new(10));
//         let isolator = Arc::new(P2PNetworkIsolator::new(network));
//         let (validator, test_keys): (ConsensusValidator, Vec<SigningKey>) = create_test_validator(5, 3).await;
//         let validator = Arc::new(validator);
        
//         let config = FlagSystemConfig {
//             min_flags_for_isolation: 3,
//             flag_window_seconds: 3600,
//             immediate_isolation_severity: SeverityLevel::Critical,
//             min_unique_flaggers: 2,
//         };
        
//         let manager = Arc::new(FlagSystemManager::new(
//             config,
//             storage.clone(),
//             validator.clone(),
//             isolator.clone(),
//         ));
        
//         // Create multiple concurrent flag processing tasks
//         let mut handles = Vec::new();
//         for i in 0..10 {
//             let manager = manager.clone();
//             let key = test_keys[i % test_keys.len()].clone();
//             let handle = tokio::spawn(async move {
//                 let flag = create_test_flag_with_key(&key);
//                 manager.process_flag(flag, b"test evidence").await
//             });
//             handles.push(handle);
//         }
        
//         // Wait for all tasks to complete
//         for handle in handles {
//             handle.await??;
//         }
        
//         // Verify final state
//         assert!(storage.verify_chain().await?);
        
//         Ok(())
//     }
// }

// // Helper functions for tests
// fn create_test_flags(count: usize) -> Vec<Flag> {
//     let mut flags = Vec::new();
//     for i in 0..count {
//         flags.push(Flag::new(
//             format!("flagger{}", i),
//             format!("node{}", i % 2),
//             format!("evidence_hash_{}", i),
//             FlagReason::MaliciousActivity,
//             SeverityLevel::High,
//             vec![0; 64], // Dummy signature
//         ));
//     }
//     flags
// } use crate::{Flag, FlagReason, FlagSystemManager, SeverityLevel};
use anyhow::Result;
use serde::{Deserialize, Serialize};
use warp::{Filter, Rejection, Reply};
use std::sync::Arc;

/// Request body for creating a new flag
#[derive(Debug, Deserialize)]
pub struct CreateFlagRequest {
    /// ID of the node being flagged
    pub suspect_id: String,
    /// Evidence supporting the flag
    pub evidence: Vec<u8>,
    /// Reason for flagging
    pub reason: FlagReason,
    /// Severity level of the flag
    pub severity: SeverityLevel,
}

/// Response for flag creation
#[derive(Debug, Serialize)]
pub struct CreateFlagResponse {
    /// ID of the created flag
    pub flag_id: String,
    /// Whether the flag led to node isolation
    pub isolation_triggered: bool,
}

/// Response for flag queries
#[derive(Debug, Serialize)]
pub struct FlagQueryResponse {
    /// List of flags matching the query
    pub flags: Vec<Flag>,
    /// Total number of flags
    pub total: usize,
}

/// API handler for the flag system
pub struct FlagSystemApi {
    manager: Arc<FlagSystemManager>,
}

impl FlagSystemApi {
    pub fn new(manager: Arc<FlagSystemManager>) -> Self {
        Self { manager }
    }

    /// Create routes for the flag system API
    pub fn routes(&self) -> impl Filter<Extract = impl Reply, Error = Rejection> + Clone {
        let api = Arc::new(self.clone());
        
        // POST /api/flags - Create a new flag
        let create_flag = warp::path!("api" / "flags")
            .and(warp::post())
            .and(warp::body::json())
            .and(with_api(api.clone()))
            .and_then(handle_create_flag);
        
        // GET /api/flags/node/{node_id} - Get flags for a specific node
        let get_node_flags = warp::path!("api" / "flags" / "node" / String)
            .and(warp::get())
            .and(with_api(api.clone()))
            .and_then(handle_get_node_flags);
        
        // GET /api/flags/timerange - Get flags within a time range
        let get_timerange_flags = warp::path!("api" / "flags" / "timerange")
            .and(warp::get())
            .and(warp::query::<TimeRangeQuery>())
            .and(with_api(api.clone()))
            .and_then(handle_get_timerange_flags);
        
        create_flag
            .or(get_node_flags)
            .or(get_timerange_flags)
            .with(warp::cors().allow_any_origin())
    }
}

#[derive(Debug, Deserialize)]
struct TimeRangeQuery {
    start: u64,
    end: u64,
}

fn with_api(
    api: Arc<FlagSystemApi>,
) -> impl Filter<Extract = (Arc<FlagSystemApi>,), Error = std::convert::Infallible> + Clone {
    warp::any().map(move || api.clone())
}

async fn handle_create_flag(
    request: CreateFlagRequest,
    api: Arc<FlagSystemApi>,
) -> Result<impl Reply, Rejection> {
    let flag = Flag::new(
        "system".to_string(), // Replace with actual flagger ID from auth
        request.suspect_id,
        hex::encode(&request.evidence),
        request.reason,
        request.severity,
        vec![0; 64], // Replace with actual signature
    );
    
    let isolation_triggered = api.manager.process_flag(flag.clone(), &request.evidence).await
        .map_err(|_| warp::reject::custom(ApiError))?;
    
    let response = CreateFlagResponse {
        flag_id: format!("{}:{}", flag.flagger_id, flag.timestamp),
        isolation_triggered,
    };
    
    Ok(warp::reply::json(&response))
}

async fn handle_get_node_flags(
    node_id: String,
    api: Arc<FlagSystemApi>,
) -> Result<impl Reply, Rejection> {
    let flags = api.manager
        .storage
        .get_flags_for_node(&node_id)
        .await
        .map_err(|_| warp::reject::custom(ApiError))?;
    
    let response = FlagQueryResponse {
        total: flags.len(),
        flags,
    };
    
    Ok(warp::reply::json(&response))
}

async fn handle_get_timerange_flags(
    query: TimeRangeQuery,
    api: Arc<FlagSystemApi>,
) -> Result<impl Reply, Rejection> {
    let flags = api.manager
        .storage
        .get_flags_in_timerange(query.start, query.end)
        .await
        .map_err(|_| warp::reject::custom(ApiError))?;
    
    let response = FlagQueryResponse {
        total: flags.len(),
        flags,
    };
    
    Ok(warp::reply::json(&response))
}

#[derive(Debug)]
struct ApiError;
impl warp::reject::Reject for ApiError {} use crate::SeverityLevel;
use serde::{Deserialize, Serialize};
use anyhow::{Context, Result};
use std::path::Path;
use std::fs;

/// Configuration for the flag system
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FlagSystemConfig {
    /// Minimum number of flags needed for automatic isolation
    pub min_flags_for_isolation: u32,
    /// Time window (in seconds) to consider flags for isolation
    pub flag_window_seconds: u64,
    /// Minimum severity level that triggers immediate isolation
    pub immediate_isolation_severity: SeverityLevel,
    /// Required number of unique flaggers for isolation
    pub min_unique_flaggers: u32,
    /// Storage configuration
    pub storage: StorageConfig,
    /// Validation configuration
    pub validation: ValidationConfig,
    /// API configuration
    pub api: ApiConfig,
}

/// Storage configuration options
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StorageConfig {
    /// Storage type (memory, redis, blockchain)
    pub storage_type: StorageType,
    /// Redis configuration if using Redis storage
    pub redis: Option<RedisConfig>,
    /// Blockchain configuration if using blockchain storage
    pub blockchain: Option<BlockchainConfig>,
}

/// Available storage types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum StorageType {
    Memory,
    Redis,
    Blockchain,
}

/// Redis storage configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RedisConfig {
    /// Redis connection URL
    pub url: String,
    /// Key prefix for Redis storage
    pub prefix: String,
    /// Connection pool size
    pub pool_size: u32,
}

/// Blockchain storage configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockchainConfig {
    /// Maximum flags per block
    pub max_flags_per_block: u32,
    /// Block creation interval in seconds
    pub block_interval: u64,
}

/// Validation configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationConfig {
    /// Number of required validators
    pub required_validators: u32,
    /// List of validator node IDs
    pub validator_nodes: Vec<String>,
    /// Validation timeout in seconds
    pub validation_timeout: u64,
}

/// API configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ApiConfig {
    /// API bind address
    pub bind_address: String,
    /// API port
    pub port: u16,
    /// Enable CORS
    pub enable_cors: bool,
    /// Rate limiting configuration
    pub rate_limit: RateLimitConfig,
}

/// Rate limiting configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RateLimitConfig {
    /// Maximum requests per minute
    pub max_requests_per_minute: u32,
    /// Maximum concurrent requests
    pub max_concurrent_requests: u32,
}

impl FlagSystemConfig {
    /// Load configuration from a file
    pub fn from_file<P: AsRef<Path>>(path: P) -> Result<Self> {
        let content = fs::read_to_string(path)
            .context("Failed to read config file")?;
        let config: Self = serde_json::from_str(&content)
            .context("Failed to parse config file")?;
        config.validate()?;
        Ok(config)
    }

    /// Validate the configuration
    pub fn validate(&self) -> Result<()> {
        // Validate basic parameters
        if self.min_flags_for_isolation == 0 {
            anyhow::bail!("min_flags_for_isolation must be greater than 0");
        }
        if self.flag_window_seconds == 0 {
            anyhow::bail!("flag_window_seconds must be greater than 0");
        }
        if self.min_unique_flaggers == 0 {
            anyhow::bail!("min_unique_flaggers must be greater than 0");
        }
        if self.min_unique_flaggers > self.min_flags_for_isolation {
            anyhow::bail!("min_unique_flaggers cannot be greater than min_flags_for_isolation");
        }

        // Validate storage configuration
        match self.storage.storage_type {
            StorageType::Redis => {
                let redis = self.storage.redis.as_ref()
                    .ok_or_else(|| anyhow::anyhow!("Redis configuration is required for Redis storage"))?;
                if redis.pool_size == 0 {
                    anyhow::bail!("Redis pool_size must be greater than 0");
                }
            }
            StorageType::Blockchain => {
                let blockchain = self.storage.blockchain.as_ref()
                    .ok_or_else(|| anyhow::anyhow!("Blockchain configuration is required for blockchain storage"))?;
                if blockchain.max_flags_per_block == 0 {
                    anyhow::bail!("max_flags_per_block must be greater than 0");
                }
                if blockchain.block_interval == 0 {
                    anyhow::bail!("block_interval must be greater than 0");
                }
            }
            StorageType::Memory => {}
        }

        // Validate validation configuration
        if self.validation.required_validators == 0 {
            anyhow::bail!("required_validators must be greater than 0");
        }
        if self.validation.validator_nodes.is_empty() {
            anyhow::bail!("validator_nodes cannot be empty");
        }
        if self.validation.required_validators as usize > self.validation.validator_nodes.len() {
            anyhow::bail!("required_validators cannot be greater than the number of validator nodes");
        }
        if self.validation.validation_timeout == 0 {
            anyhow::bail!("validation_timeout must be greater than 0");
        }

        // Validate API configuration
        if self.api.port == 0 {
            anyhow::bail!("API port cannot be 0");
        }
        if self.api.rate_limit.max_requests_per_minute == 0 {
            anyhow::bail!("max_requests_per_minute must be greater than 0");
        }
        if self.api.rate_limit.max_concurrent_requests == 0 {
            anyhow::bail!("max_concurrent_requests must be greater than 0");
        }

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn create_test_config() -> FlagSystemConfig {
        FlagSystemConfig {
            min_flags_for_isolation: 2,
            flag_window_seconds: 3600,
            immediate_isolation_severity: SeverityLevel::Critical,
            min_unique_flaggers: 2,
            storage: StorageConfig {
                storage_type: StorageType::Memory,
                redis: None,
                blockchain: None,
            },
            validation: ValidationConfig {
                required_validators: 2,
                validator_nodes: vec!["node1".to_string(), "node2".to_string(), "node3".to_string()],
                validation_timeout: 30,
            },
            api: ApiConfig {
                bind_address: "127.0.0.1".to_string(),
                port: 8080,
                enable_cors: true,
                rate_limit: RateLimitConfig {
                    max_requests_per_minute: 60,
                    max_concurrent_requests: 10,
                },
            },
        }
    }

    #[test]
    fn test_valid_config() {
        let config = create_test_config();
        assert!(config.validate().is_ok());
    }

    #[test]
    fn test_invalid_min_flags() {
        let mut config = create_test_config();
        config.min_flags_for_isolation = 0;
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_invalid_window() {
        let mut config = create_test_config();
        config.flag_window_seconds = 0;
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_invalid_unique_flaggers() {
        let mut config = create_test_config();
        config.min_unique_flaggers = config.min_flags_for_isolation + 1;
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_invalid_redis_config() {
        let mut config = create_test_config();
        config.storage.storage_type = StorageType::Redis;
        assert!(config.validate().is_err());

        config.storage.redis = Some(RedisConfig {
            url: "redis://localhost".to_string(),
            prefix: "test".to_string(),
            pool_size: 0,
        });
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_invalid_blockchain_config() {
        let mut config = create_test_config();
        config.storage.storage_type = StorageType::Blockchain;
        assert!(config.validate().is_err());

        config.storage.blockchain = Some(BlockchainConfig {
            max_flags_per_block: 0,
            block_interval: 60,
        });
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_invalid_validation_config() {
        let mut config = create_test_config();
        config.validation.required_validators = 0;
        assert!(config.validate().is_err());

        config.validation.required_validators = 2;
        config.validation.validator_nodes.clear();
        assert!(config.validate().is_err());

        config.validation.validator_nodes = vec!["node1".to_string()];
        config.validation.required_validators = 2;
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_invalid_api_config() {
        let mut config = create_test_config();
        config.api.port = 0;
        assert!(config.validate().is_err());

        config.api.port = 8080;
        config.api.rate_limit.max_requests_per_minute = 0;
        assert!(config.validate().is_err());

        config.api.rate_limit.max_requests_per_minute = 60;
        config.api.rate_limit.max_concurrent_requests = 0;
        assert!(config.validate().is_err());
    }
} use crate::{Flag, FlagValidator};
use anyhow::Result;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

/// Represents a validation vote from a node
#[derive(Debug, Clone)]
struct ValidationVote {
    node_id: String,
    approved: bool,
    _timestamp: u64, // Prefixed with underscore to suppress unused field warning
}

/// Consensus-based flag validator that requires multiple nodes to approve flags
pub struct ConsensusValidator {
    validators: Arc<RwLock<Vec<String>>>,
    required_votes: usize,
    votes: Arc<RwLock<HashMap<String, Vec<ValidationVote>>>>,
    base_validator: Arc<dyn FlagValidator + Send + Sync>,
}

impl ConsensusValidator {
    pub fn new(
        validators: Vec<String>,
        required_votes: usize,
        base_validator: Arc<dyn FlagValidator + Send + Sync>,
    ) -> Self {
        ConsensusValidator {
            validators: Arc::new(RwLock::new(validators)),
            required_votes,
            votes: Arc::new(RwLock::new(HashMap::new())),
            base_validator,
        }
    }

    /// Add a vote for a flag
    pub async fn add_vote(&self, flag_id: &str, node_id: &str, approved: bool) -> Result<bool> {
        let validators = self.validators.read().await;
        if !validators.contains(&node_id.to_string()) {
            return Err(anyhow::anyhow!("Node is not a validator"));
        }

        let vote = ValidationVote {
            node_id: node_id.to_string(),
            approved,
            _timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        };

        let mut votes = self.votes.write().await;
        let flag_votes = votes.entry(flag_id.to_string()).or_insert_with(Vec::new);
        
        // Check if node has already voted
        if flag_votes.iter().any(|v| v.node_id == node_id) {
            return Err(anyhow::anyhow!("Node has already voted"));
        }

        flag_votes.push(vote);

        // Check if we have enough votes
        let approval_count = flag_votes.iter().filter(|v| v.approved).count();
        let rejection_count = flag_votes.len() - approval_count;

        Ok(approval_count >= self.required_votes || rejection_count >= self.required_votes)
    }

    /// Check if a flag has reached consensus
    async fn check_consensus(&self, flag_id: &str) -> Result<bool> {
        let votes = self.votes.read().await;
        if let Some(flag_votes) = votes.get(flag_id) {
            let approval_count = flag_votes.iter().filter(|v| v.approved).count();
            Ok(approval_count >= self.required_votes)
        } else {
            Ok(false)
        }
    }
}

#[async_trait::async_trait]
impl FlagValidator for ConsensusValidator {
    async fn validate_flag(&self, flag: &Flag, evidence: &[u8]) -> Result<bool> {
        // First validate using base validator
        if !self.base_validator.validate_flag(flag, evidence).await? {
            return Ok(false);
        }

        // Generate a unique ID for this flag
        let flag_id = format!(
            "{}:{}:{}",
            flag.flagger_id,
            flag.suspect_id,
            flag.timestamp
        );

        // Check if we already have consensus
        if self.check_consensus(&flag_id).await? {
            return Ok(true);
        }

        // Add vote from this node
        self.add_vote(&flag_id, &flag.flagger_id, true).await?;

        // Return current consensus state
        self.check_consensus(&flag_id).await
    }
} use anyhow::{Context, Result};
use ring::digest::{Context as DigestContext, SHA256};
use serde::{Deserialize, Serialize};
use std::time::{SystemTime, UNIX_EPOCH};

/// Evidence type for flag verification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Evidence {
    /// Raw evidence data
    pub data: Vec<u8>,
    /// Evidence metadata
    pub metadata: EvidenceMetadata,
    /// Evidence hash
    pub hash: String,
    /// Evidence signatures
    pub signatures: Vec<EvidenceSignature>,
}

/// Metadata for evidence
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvidenceMetadata {
    /// Timestamp when evidence was collected
    pub timestamp: u64,
    /// Type of evidence
    pub evidence_type: EvidenceType,
    /// Source of evidence
    pub source: String,
    /// Additional metadata fields
    pub additional_fields: std::collections::HashMap<String, String>,
}

/// Types of evidence
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum EvidenceType {
    NetworkTraffic,
    SystemLog,
    Metrics,
    Behavioral,
    Custom(String),
}

/// Signature for evidence verification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvidenceSignature {
    /// Signer ID
    pub signer_id: String,
    /// Signature data
    pub signature: Vec<u8>,
    /// Signature algorithm
    pub algorithm: SignatureAlgorithm,
    /// Timestamp of signing
    pub timestamp: u64,
}

/// Supported signature algorithms
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SignatureAlgorithm {
    Ed25519,
    ECDSA,
    RSA,
}

/// Evidence verifier for validating evidence
pub struct EvidenceVerifier {
    required_signatures: usize,
    trusted_signers: Vec<String>,
    min_evidence_age: u64,
    max_evidence_age: u64,
}

impl EvidenceVerifier {
    pub fn new(
        required_signatures: usize,
        trusted_signers: Vec<String>,
        min_evidence_age: u64,
        max_evidence_age: u64,
    ) -> Self {
        Self {
            required_signatures,
            trusted_signers,
            min_evidence_age,
            max_evidence_age,
        }
    }

    /// Verify evidence validity
    pub fn verify_evidence(&self, evidence: &Evidence) -> Result<bool> {
        // Verify evidence hash
        let calculated_hash = self.calculate_evidence_hash(&evidence.data)?;
        if calculated_hash != evidence.hash {
            return Ok(false);
        }

        // Verify evidence age
        let current_time = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let evidence_age = current_time - evidence.metadata.timestamp;
        if evidence_age < self.min_evidence_age || evidence_age > self.max_evidence_age {
            return Ok(false);
        }

        // Verify signatures
        let valid_signatures = evidence
            .signatures
            .iter()
            .filter(|sig| self.verify_signature(sig, &evidence.hash))
            .count();

        if valid_signatures < self.required_signatures {
            return Ok(false);
        }

        Ok(true)
    }

    /// Calculate evidence hash
    fn calculate_evidence_hash(&self, data: &[u8]) -> Result<String> {
        let mut context = DigestContext::new(&SHA256);
        context.update(data);
        let digest = context.finish();
        Ok(hex::encode(digest.as_ref()))
    }

    /// Verify signature validity
    fn verify_signature(&self, signature: &EvidenceSignature, evidence_hash: &str) -> bool {
        // Check if signer is trusted
        if !self.trusted_signers.contains(&signature.signer_id) {
            return false;
        }

        // Verify signature based on algorithm
        match signature.algorithm {
            SignatureAlgorithm::Ed25519 => self.verify_ed25519(signature, evidence_hash),
            SignatureAlgorithm::ECDSA => self.verify_ecdsa(signature, evidence_hash),
            SignatureAlgorithm::RSA => self.verify_rsa(signature, evidence_hash),
        }
    }

    fn verify_ed25519(&self, signature: &EvidenceSignature, evidence_hash: &str) -> bool {
        // Implement Ed25519 signature verification
        // This is a placeholder - implement actual verification logic
        true
    }

    fn verify_ecdsa(&self, signature: &EvidenceSignature, evidence_hash: &str) -> bool {
        // Implement ECDSA signature verification
        // This is a placeholder - implement actual verification logic
        true
    }

    fn verify_rsa(&self, signature: &EvidenceSignature, evidence_hash: &str) -> bool {
        // Implement RSA signature verification
        // This is a placeholder - implement actual verification logic
        true
    }
}

/// Evidence builder for creating evidence
pub struct EvidenceBuilder {
    data: Vec<u8>,
    metadata: EvidenceMetadata,
    signatures: Vec<EvidenceSignature>,
}

impl EvidenceBuilder {
    pub fn new(data: Vec<u8>, evidence_type: EvidenceType, source: String) -> Self {
        let metadata = EvidenceMetadata {
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            evidence_type,
            source,
            additional_fields: std::collections::HashMap::new(),
        };

        Self {
            data,
            metadata,
            signatures: Vec::new(),
        }
    }

    pub fn add_metadata_field(&mut self, key: String, value: String) -> &mut Self {
        self.metadata.additional_fields.insert(key, value);
        self
    }

    pub fn add_signature(
        &mut self,
        signer_id: String,
        signature: Vec<u8>,
        algorithm: SignatureAlgorithm,
    ) -> &mut Self {
        let signature = EvidenceSignature {
            signer_id,
            signature,
            algorithm,
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        };
        self.signatures.push(signature);
        self
    }

    pub fn build(&self) -> Result<Evidence> {
        let mut context = DigestContext::new(&SHA256);
        context.update(&self.data);
        let hash = hex::encode(context.finish().as_ref());

        Ok(Evidence {
            data: self.data.clone(),
            metadata: self.metadata.clone(),
            hash,
            signatures: self.signatures.clone(),
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn create_test_evidence() -> Evidence {
        let mut builder = EvidenceBuilder::new(
            vec![1, 2, 3, 4, 5],
            EvidenceType::NetworkTraffic,
            "test_source".to_string(),
        );

        builder
            .add_metadata_field("test_key".to_string(), "test_value".to_string())
            .add_signature(
                "signer1".to_string(),
                vec![0; 64],
                SignatureAlgorithm::Ed25519,
            );

        builder.build().unwrap()
    }

    #[test]
    fn test_evidence_verification() {
        let evidence = create_test_evidence();
        let verifier = EvidenceVerifier::new(
            1,
            vec!["signer1".to_string()],
            0,
            u64::MAX,
        );

        assert!(verifier.verify_evidence(&evidence).unwrap());
    }

    #[test]
    fn test_evidence_hash() {
        let evidence = create_test_evidence();
        let verifier = EvidenceVerifier::new(1, vec![], 0, u64::MAX);
        
        let calculated_hash = verifier.calculate_evidence_hash(&evidence.data).unwrap();
        assert_eq!(calculated_hash, evidence.hash);
    }

    #[test]
    fn test_invalid_evidence_age() {
        let evidence = create_test_evidence();
        let verifier = EvidenceVerifier::new(
            1,
            vec!["signer1".to_string()],
            u64::MAX, // Set minimum age to maximum possible value
            u64::MAX,
        );

        assert!(!verifier.verify_evidence(&evidence).unwrap());
    }

    #[test]
    fn test_untrusted_signer() {
        let evidence = create_test_evidence();
        let verifier = EvidenceVerifier::new(
            1,
            vec!["other_signer".to_string()],
            0,
            u64::MAX,
        );

        assert!(!verifier.verify_evidence(&evidence).unwrap());
    }

    #[test]
    fn test_insufficient_signatures() {
        let evidence = create_test_evidence();
        let verifier = EvidenceVerifier::new(
            2, // Require 2 signatures
            vec!["signer1".to_string()],
            0,
            u64::MAX,
        );

        assert!(!verifier.verify_evidence(&evidence).unwrap());
    }
} use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::time::{SystemTime, UNIX_EPOCH};

/// Represents the severity level of a flag
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize, PartialOrd, Ord)]
pub enum SeverityLevel {
    Low,
    Medium,
    High,
    Critical,
}

/// Represents the reason for flagging a node
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub enum FlagReason {
    MalformedData,
    NonResponsive,
    ConsensusViolation,
    UnexpectedBehavior,
    MaliciousActivity,
    Custom(String),
}

/// Represents a flag in the system
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Flag {
    /// Unique identifier of the node that created the flag
    pub flagger_id: String,
    /// Unique identifier of the suspicious node
    pub suspect_id: String,
    /// Unix timestamp when the flag was created
    pub timestamp: u64,
    /// Hash of the evidence supporting the flag
    pub evidence_hash: String,
    /// Reason for flagging
    pub reason: FlagReason,
    /// Severity level of the flag
    pub severity: SeverityLevel,
    /// Digital signature of the flagger
    pub signature: Vec<u8>,
}

impl Flag {
    pub fn new(
        flagger_id: String,
        suspect_id: String,
        evidence_hash: String,
        reason: FlagReason,
        severity: SeverityLevel,
        signature: Vec<u8>,
    ) -> Self {
        let timestamp = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs();

        Flag {
            flagger_id,
            suspect_id,
            timestamp,
            evidence_hash,
            reason,
            severity,
            signature,
        }
    }
}

/// Trait for flag validation
#[async_trait::async_trait]
pub trait FlagValidator {
    /// Validates a flag and its evidence
    async fn validate_flag(&self, flag: &Flag, evidence: &[u8]) -> Result<bool>;
}

/// Trait for flag storage
#[async_trait::async_trait]
pub trait FlagStorage {
    /// Stores a new flag
    async fn store_flag(&self, flag: Flag) -> Result<()>;
    /// Retrieves flags for a specific node
    async fn get_flags_for_node(&self, node_id: &str) -> Result<Vec<Flag>>;
    /// Retrieves all flags within a time range
    async fn get_flags_in_timerange(&self, start: u64, end: u64) -> Result<Vec<Flag>>;
}

/// Trait for node isolation
#[async_trait::async_trait]
pub trait NodeIsolator {
    /// Isolates a node from the network
    async fn isolate_node(&self, node_id: &str) -> Result<()>;
    /// Removes isolation from a node
    async fn remove_isolation(&self, node_id: &str) -> Result<()>;
    /// Checks if a node is isolated
    async fn is_node_isolated(&self, node_id: &str) -> Result<bool>;
}

/// Configuration for the flag system
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FlagSystemConfig {
    /// Minimum number of flags needed for automatic isolation
    pub min_flags_for_isolation: u32,
    /// Time window (in seconds) to consider flags for isolation
    pub flag_window_seconds: u64,
    /// Minimum severity level that triggers immediate isolation
    pub immediate_isolation_severity: SeverityLevel,
    /// Required number of unique flaggers for isolation
    pub min_unique_flaggers: u32,
}

mod manager;
mod storage;
mod isolator;
mod validator;
mod consensus;
mod tests;

pub use manager::FlagSystemManager;
pub use storage::{BlockchainStorage, InMemoryFlagStorage};
pub use isolator::{NetworkNodeIsolator, P2PNetworkIsolator};
pub use validator::BasicFlagValidator;
pub use consensus::ConsensusValidator; use crate::{Flag, FlagStorage, FlagSystemConfig, FlagValidator, NodeIsolator};
use anyhow::{Context, Result};
use std::sync::Arc;
use tokio::sync::RwLock;


/// Manages the flag system operations
pub struct FlagSystemManager {
    config: FlagSystemConfig,
    storage: Arc<dyn FlagStorage + Send + Sync>,
    validator: Arc<dyn FlagValidator + Send + Sync>,
    isolator: Arc<dyn NodeIsolator + Send + Sync>,
    isolated_nodes: Arc<RwLock<Vec<String>>>,
}

impl FlagSystemManager {
    pub fn new(
        config: FlagSystemConfig,
        storage: Arc<dyn FlagStorage + Send + Sync>,
        validator: Arc<dyn FlagValidator + Send + Sync>,
        isolator: Arc<dyn NodeIsolator + Send + Sync>,
    ) -> Self {
        FlagSystemManager {
            config,
            storage,
            validator,
            isolator,
            isolated_nodes: Arc::new(RwLock::new(Vec::new())),
        }
    }

    /// Process a new flag and decide if isolation is needed
    pub async fn process_flag(&self, flag: Flag, evidence: &[u8]) -> Result<bool> {
        // Validate the flag
        let is_valid = self
            .validator
            .validate_flag(&flag, evidence)
            .await
            .context("Failed to validate flag")?;

        if !is_valid {
            return Ok(false);
        }

        // Store the valid flag
        self.storage
            .store_flag(flag.clone())
            .await
            .context("Failed to store flag")?;

        // Check if immediate isolation is needed based on severity
        println!("Flag severity: {:?}", flag.severity);
        println!("Immediate isolation severity: {:?}", self.config.immediate_isolation_severity);
        if flag.severity >= self.config.immediate_isolation_severity {
            self.isolate_node(&flag.suspect_id).await?;
            return Ok(true);
        }

        // Check accumulated flags
        self.check_accumulated_flags(&flag.suspect_id).await
    }

    /// Check if a node should be isolated based on accumulated flags
    async fn check_accumulated_flags(&self, node_id: &str) -> Result<bool> {
        let current_time = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let window_start = current_time.saturating_sub(self.config.flag_window_seconds);

        // Get flags within the time window
        let flags = self
            .storage
            .get_flags_in_timerange(window_start, current_time)
            .await
            .context("Failed to retrieve flags")?;

        // Filter flags for the specific node
        let node_flags: Vec<_> = flags
            .into_iter()
            .filter(|f| f.suspect_id == node_id)
            .collect();

        // Count unique flaggers
        let unique_flaggers: std::collections::HashSet<_> =
            node_flags.iter().map(|f| &f.flagger_id).collect();

        if node_flags.len() >= self.config.min_flags_for_isolation as usize
            && unique_flaggers.len() >= self.config.min_unique_flaggers as usize
        {
            self.isolate_node(node_id).await?;
            return Ok(true);
        }

        Ok(false)
    }

    /// Isolate a node
    async fn isolate_node(&self, node_id: &str) -> Result<()> {
        // Check if already isolated
        if self
            .isolator
            .is_node_isolated(node_id)
            .await
            .context("Failed to check node isolation status")?
        {
            return Ok(());
        }

        // Isolate the node
        self.isolator
            .isolate_node(node_id)
            .await
            .context("Failed to isolate node")?;

        // Add to isolated nodes list
        let mut isolated_nodes = self.isolated_nodes.write().await;
        if !isolated_nodes.contains(&node_id.to_string()) {
            isolated_nodes.push(node_id.to_string());
        }

        Ok(())
    }

    /// Remove isolation from a node
    pub async fn remove_isolation(&self, node_id: &str) -> Result<()> {
        self.isolator
            .remove_isolation(node_id)
            .await
            .context("Failed to remove node isolation")?;

        // Remove from isolated nodes list
        let mut isolated_nodes = self.isolated_nodes.write().await;
        isolated_nodes.retain(|id| id != node_id);

        Ok(())
    }

    /// Get list of currently isolated nodes
    pub async fn get_isolated_nodes(&self) -> Vec<String> {
        self.isolated_nodes.read().await.clone()
    }
} use lazy_static::lazy_static;
use prometheus::{
    Counter, Gauge, Histogram, HistogramOpts, IntCounter, IntGauge, Opts, Registry,
};
use std::sync::Arc;

lazy_static! {
    pub static ref REGISTRY: Registry = Registry::new();
    
    // Flag processing metrics
    pub static ref FLAGS_PROCESSED: IntCounter = IntCounter::new(
        "flag_system_flags_processed_total",
        "Total number of flags processed"
    ).unwrap();
    
    pub static ref FLAGS_VALIDATED: IntCounter = IntCounter::new(
        "flag_system_flags_validated_total",
        "Total number of flags validated"
    ).unwrap();
    
    pub static ref FLAGS_REJECTED: IntCounter = IntCounter::new(
        "flag_system_flags_rejected_total",
        "Total number of flags rejected"
    ).unwrap();
    
    // Node isolation metrics
    pub static ref NODES_ISOLATED: IntGauge = IntGauge::new(
        "flag_system_nodes_isolated",
        "Current number of isolated nodes"
    ).unwrap();
    
    pub static ref ISOLATION_ACTIONS: IntCounter = IntCounter::new(
        "flag_system_isolation_actions_total",
        "Total number of isolation actions taken"
    ).unwrap();
    
    // Storage metrics
    pub static ref STORAGE_OPERATIONS: IntCounter = IntCounter::new(
        "flag_system_storage_operations_total",
        "Total number of storage operations"
    ).unwrap();
    
    pub static ref STORAGE_ERRORS: IntCounter = IntCounter::new(
        "flag_system_storage_errors_total",
        "Total number of storage operation errors"
    ).unwrap();
    
    // Performance metrics
    pub static ref FLAG_PROCESSING_TIME: Histogram = Histogram::with_opts(
        HistogramOpts::new(
            "flag_system_processing_duration_seconds",
            "Time taken to process flags"
        )
    ).unwrap();
    
    pub static ref VALIDATION_TIME: Histogram = Histogram::with_opts(
        HistogramOpts::new(
            "flag_system_validation_duration_seconds",
            "Time taken to validate flags"
        )
    ).unwrap();
}

/// Initialize metrics collection
pub fn init_metrics() {
    // Register all metrics
    REGISTRY.register(Box::new(FLAGS_PROCESSED.clone())).unwrap();
    REGISTRY.register(Box::new(FLAGS_VALIDATED.clone())).unwrap();
    REGISTRY.register(Box::new(FLAGS_REJECTED.clone())).unwrap();
    REGISTRY.register(Box::new(NODES_ISOLATED.clone())).unwrap();
    REGISTRY.register(Box::new(ISOLATION_ACTIONS.clone())).unwrap();
    REGISTRY.register(Box::new(STORAGE_OPERATIONS.clone())).unwrap();
    REGISTRY.register(Box::new(STORAGE_ERRORS.clone())).unwrap();
    REGISTRY.register(Box::new(FLAG_PROCESSING_TIME.clone())).unwrap();
    REGISTRY.register(Box::new(VALIDATION_TIME.clone())).unwrap();
}

/// Metrics collector for the flag system
pub struct MetricsCollector {
    registry: Arc<Registry>,
}

impl MetricsCollector {
    pub fn new() -> Self {
        init_metrics();
        Self {
            registry: Arc::new(REGISTRY.clone()),
        }
    }
    
    pub fn record_flag_processed(&self) {
        FLAGS_PROCESSED.inc();
    }
    
    pub fn record_flag_validated(&self, success: bool) {
        if success {
            FLAGS_VALIDATED.inc();
        } else {
            FLAGS_REJECTED.inc();
        }
    }
    
    pub fn record_node_isolated(&self) {
        NODES_ISOLATED.inc();
        ISOLATION_ACTIONS.inc();
    }
    
    pub fn record_node_unisolated(&self) {
        NODES_ISOLATED.dec();
    }
    
    pub fn record_storage_operation(&self, success: bool) {
        STORAGE_OPERATIONS.inc();
        if !success {
            STORAGE_ERRORS.inc();
        }
    }
    
    pub fn start_flag_processing_timer(&self) -> prometheus::HistogramTimer {
        FLAG_PROCESSING_TIME.start_timer()
    }
    
    pub fn start_validation_timer(&self) -> prometheus::HistogramTimer {
        VALIDATION_TIME.start_timer()
    }
    
    pub fn get_registry(&self) -> Arc<Registry> {
        self.registry.clone()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_metrics_collection() {
        let collector = MetricsCollector::new();
        
        // Test flag processing metrics
        collector.record_flag_processed();
        assert_eq!(FLAGS_PROCESSED.get(), 1);
        
        collector.record_flag_validated(true);
        assert_eq!(FLAGS_VALIDATED.get(), 1);
        
        collector.record_flag_validated(false);
        assert_eq!(FLAGS_REJECTED.get(), 1);
        
        // Test isolation metrics
        collector.record_node_isolated();
        assert_eq!(NODES_ISOLATED.get(), 1);
        assert_eq!(ISOLATION_ACTIONS.get(), 1);
        
        collector.record_node_unisolated();
        assert_eq!(NODES_ISOLATED.get(), 0);
        
        // Test storage metrics
        collector.record_storage_operation(true);
        assert_eq!(STORAGE_OPERATIONS.get(), 1);
        assert_eq!(STORAGE_ERRORS.get(), 0);
        
        collector.record_storage_operation(false);
        assert_eq!(STORAGE_OPERATIONS.get(), 2);
        assert_eq!(STORAGE_ERRORS.get(), 1);
    }
} use crate::NodeIsolator;
use anyhow::Result;
use libp2p::{
    gossipsub::{GossipsubMessage, MessageId},
    kad::KademliaEvent,
    PeerId,
};
use network::{NetworkBehaviour, NetworkEvent, NetworkService};
use serde::{Deserialize, Serialize};
use std::{collections::HashSet, sync::Arc};
use tokio::sync::RwLock;

const ISOLATION_TOPIC: &str = "node-isolation";

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum IsolationMessage {
    IsolateNode {
        node_id: String,
        reason: String,
        timestamp: u64,
    },
    RemoveIsolation {
        node_id: String,
        timestamp: u64,
    },
}

pub struct P2PNetworkIsolator {
    network: Arc<NetworkService>,
    isolated_nodes: Arc<RwLock<HashSet<String>>>,
    peer_map: Arc<RwLock<HashMap<String, PeerId>>>,
}

impl P2PNetworkIsolator {
    pub fn new(network: Arc<NetworkService>) -> Self {
        let isolator = P2PNetworkIsolator {
            network,
            isolated_nodes: Arc::new(RwLock::new(HashSet::new())),
            peer_map: Arc::new(RwLock::new(HashMap::new())),
        };

        // Subscribe to isolation topic
        isolator.subscribe_to_isolation_topic();
        
        isolator
    }

    fn subscribe_to_isolation_topic(&self) {
        let network = self.network.clone();
        let isolated_nodes = self.isolated_nodes.clone();
        let peer_map = self.peer_map.clone();

        tokio::spawn(async move {
            let mut network_events = network.event_stream().await;

            while let Some(event) = network_events.next().await {
                match event {
                    NetworkEvent::GossipsubMessage { message, .. } => {
                        if let Ok(isolation_msg) = serde_json::from_slice::<IsolationMessage>(&message.data) {
                            match isolation_msg {
                                IsolationMessage::IsolateNode { node_id, .. } => {
                                    let mut nodes = isolated_nodes.write().await;
                                    nodes.insert(node_id.clone());
                                    
                                    // Disconnect from peer if we have their PeerId
                                    if let Some(peer_id) = peer_map.read().await.get(&node_id) {
                                        network.disconnect_peer(*peer_id).await;
                                    }
                                }
                                IsolationMessage::RemoveIsolation { node_id, .. } => {
                                    let mut nodes = isolated_nodes.write().await;
                                    nodes.remove(&node_id);
                                }
                            }
                        }
                    }
                    NetworkEvent::KademliaEvent(KademliaEvent::RoutingUpdated { peer, .. }) => {
                        // Update peer mapping
                        if let Some(node_id) = network.get_node_id_for_peer(peer).await {
                            let mut peer_map = peer_map.write().await;
                            peer_map.insert(node_id, peer);
                        }
                    }
                    _ => {}
                }
            }
        });
    }

    async fn broadcast_isolation_message(&self, message: IsolationMessage) -> Result<()> {
        let msg_data = serde_json::to_vec(&message)?;
        self.network.publish(ISOLATION_TOPIC, msg_data).await?;
        Ok(())
    }
}

#[async_trait::async_trait]
impl NodeIsolator for P2PNetworkIsolator {
    async fn isolate_node(&self, node_id: &str) -> Result<()> {
        // Create isolation message
        let message = IsolationMessage::IsolateNode {
            node_id: node_id.to_string(),
            reason: "Node flagged for malicious behavior".to_string(),
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        };

        // Broadcast isolation message
        self.broadcast_isolation_message(message).await?;

        // Add to local isolation set
        let mut isolated = self.isolated_nodes.write().await;
        isolated.insert(node_id.to_string());

        // Disconnect from peer if we have their PeerId
        if let Some(peer_id) = self.peer_map.read().await.get(node_id) {
            self.network.disconnect_peer(*peer_id).await;
        }

        Ok(())
    }

    async fn remove_isolation(&self, node_id: &str) -> Result<()> {
        // Create removal message
        let message = IsolationMessage::RemoveIsolation {
            node_id: node_id.to_string(),
            timestamp: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        };

        // Broadcast removal message
        self.broadcast_isolation_message(message).await?;

        // Remove from local isolation set
        let mut isolated = self.isolated_nodes.write().await;
        isolated.remove(node_id);

        Ok(())
    }

    async fn is_node_isolated(&self, node_id: &str) -> Result<bool> {
        let isolated = self.isolated_nodes.read().await;
        Ok(isolated.contains(node_id))
    }
} use crate::{Flag, FlagValidator};
use anyhow::Result;
use ed25519_dalek::{Signature, Verifier, VerifyingKey};

/// Basic implementation of flag validation
pub struct BasicFlagValidator {
    /// Map of node IDs to their public keys for signature verification
    node_keys: std::collections::HashMap<String, VerifyingKey>,
}

impl BasicFlagValidator {
    pub fn new(node_keys: std::collections::HashMap<String, VerifyingKey>) -> Self {
        BasicFlagValidator { node_keys }
    }

    /// Verify the signature on a flag
    fn verify_signature(&self, flag: &Flag) -> Result<bool> {
        let flagger_key = self.node_keys.get(&flag.flagger_id).ok_or_else(|| {
            anyhow::anyhow!("No public key found for flagger: {}", flag.flagger_id)
        })?;

        // Create message to verify (concatenate relevant fields)
        let msg = format!(
            "{}:{}:{}:{}:{}:{:?}",
            flag.flagger_id,
            flag.suspect_id,
            flag.timestamp,
            flag.evidence_hash,
            format!("{:?}", flag.reason),
            flag.severity
        );

        // Convert signature bytes to ed25519 Signature
        let sig_array: [u8; 64] = flag.signature.as_slice().try_into().map_err(|_| {
            anyhow::anyhow!("Invalid signature length")
        })?;
        let signature = Signature::from_bytes(&sig_array);

        // Verify the signature
        Ok(flagger_key.verify(msg.as_bytes(), &signature).is_ok())
    }

    /// Verify that the evidence hash matches the provided evidence
    fn verify_evidence(&self, flag: &Flag, evidence: &[u8]) -> bool {
        let evidence_hash = blake3::hash(evidence);
        hex::encode(evidence_hash.as_bytes()) == flag.evidence_hash
    }
}

#[async_trait::async_trait]
impl FlagValidator for BasicFlagValidator {
    async fn validate_flag(&self, flag: &Flag, evidence: &[u8]) -> Result<bool> {
        // Verify the signature first
        if !self.verify_signature(flag)? {
            return Ok(false);
        }

        // Verify the evidence hash
        if !self.verify_evidence(flag, evidence) {
            return Ok(false);
        }

        // Additional validation could be added here:
        // - Check if the flagger is authorized to flag
        // - Validate the evidence format/content
        // - Check for rate limiting on flags
        // - Verify the severity level is appropriate for the evidence

        Ok(true)
    }
} apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: falsepub mod send_encrypt_message; pub mod send_encrypt_message;

fn main() {
    println!("Hello, world!");
}use libp2p::PeerId;
use anyhow::{Result, anyhow};

pub fn send_encrypted_message(message: String, peer_id: PeerId) -> Result<()> {
    
    println!("Enviando mensagem: {} para peer: {}", message, peer_id);
    Ok(())
}

pub fn verify_message(message: &[u8]) -> Result<()> {
    
    println!("Verificando mensagem de {} bytes", message.len());
    Ok(())
}

#[allow(dead_code)]
fn encrypt_message(message: String) -> Result<Vec<u8>> {

    Ok(message.as_bytes().to_vec())
}

#[allow(dead_code)]
fn decrypt_message(message: &[u8]) -> Result<String> {

    let result = String::from_utf8(message.to_vec())
        .map_err(|e| anyhow!("Erro ao decodificar mensagem: {}", e))?;
    Ok(result)
}use libp2p::PeerId;
use message::send_encrypt_message::send_encrypted_message;

#[test]
fn test_send_message() {
    let message = "Hello, world!";
    let peer_id = PeerId::random();
    let result = send_encrypted_message(message.to_string(), peer_id);
    assert!(result.is_ok());
}

#[test]
fn test_receive_message() {
    let message = "Test message";
    let peer_id = PeerId::random();
    let result = send_encrypted_message(message.to_string(), peer_id);
    assert!(result.is_ok());
}# Fluentd configuration for Atous Network

# Input plugins - receive logs via forward protocol
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Add timestamp and hostname to all logs
<filter **>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    timestamp ${time}
  </record>
</filter>

# Output all logs to stdout with structured format
<match **>
  @type stdout
  format json
</match> FROM fluentd:v1.16-1

# Use root account to use gem and apk
USER root

# Install necessary gems
RUN gem install fluent-plugin-elasticsearch fluent-plugin-prometheus

# Copy fluent configuration
COPY conf/fluent.conf /fluentd/etc/

# Create directories for logs
RUN mkdir -p /var/log/fluentd

# Change ownership to fluent user
RUN chown -R fluent:fluent /var/log/fluentd

# Switch back to fluent user
USER fluent

# Expose ports
EXPOSE 24224 24224/udp

# Start fluentd
CMD ["fluentd", "-c", "/fluentd/etc/fluent.conf", "-v"] apiVersion: 1

providers:
  - name: 'default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /etc/grafana/provisioning/dashboardsapiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      timeInterval: "30s"global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

scrape_configs:
  # ATOUS Security Application
  - job_name: 'atous-security'
    static_configs:
      - targets: ['atous:8080']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s

  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Redis monitoring
  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  # Node exporter (if available)
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']use criterion::{black_box, criterion_group, criterion_main, Criterion};
use libp2p::{gossipsub, identity::Keypair, PeerId};
use network::crdt::CrdtManager;
use yrs::{Text, Transact};
// use std::sync::{Arc, Mutex};

fn crdt_setup() -> CrdtManager {
    let keypair = Keypair::generate_ed25519();
    let _peer_id = PeerId::from(keypair.public());
    let _gossipsub: gossipsub::Behaviour<gossipsub::IdentityTransform> = gossipsub::Behaviour::new(
        gossipsub::MessageAuthenticity::Signed(keypair),
        Default::default()
    ).expect("Failed to create gossipsub");
    
    CrdtManager::new()
}

fn criterion_benchmark(c: &mut Criterion) {
    c.bench_function("crdt_document_creation", |b| {
        b.iter(|| {
            let mut manager = crdt_setup();
            black_box(manager.get_or_create_document(black_box("bench-doc")));
        })
    });

    c.bench_function("crdt_small_update", |b| {
        let mut manager = crdt_setup();
        let doc_id = "bench-small-update";
        
        b.iter(|| {
            black_box(manager.apply_local_update(black_box(doc_id), |text| {
                let mut txn = text.transact_mut();
                text.insert(&mut txn, 0, "small update");
            })).expect("Failed to apply update");
        })
    });

    c.bench_function("crdt_large_update", |b| {
        let mut manager = crdt_setup();
        let doc_id = "bench-large-update";
        let large_text = "Large text update ".repeat(100); // About 1.8KB
        
        b.iter(|| {
            black_box(manager.apply_local_update(black_box(doc_id), |text| {
                let mut txn = text.transact_mut();
                text.insert(&mut txn, 0, &large_text);
            })).expect("Failed to apply update");
        })
    });

    c.bench_function("crdt_apply_remote", |b| {
        let mut manager1 = crdt_setup();
        let mut manager2 = crdt_setup();
        let doc_id = "bench-remote-update";

        let update = manager1.apply_local_update(doc_id, |text| {
            let mut txn = text.transact_mut();
            text.insert(&mut txn, 0, "Remote update test");
        }).expect("Failed to create update");
        
        b.iter(|| {
            let update_clone = update.clone();
            black_box(manager2.apply_remote_update(black_box(doc_id), black_box(&update_clone)))
                .expect("Failed to apply remote update");
        })
    });
}
criterion_group!(benches, criterion_benchmark);
criterion_main!(benches); // use network::energy_manager::{EnergyManager, EnergyMetrics};
use std::time::Duration;
use std::sync::Arc;
use warp::{Filter, Rejection, Reply};
use tokio::sync::Mutex;
use serde::Serialize;
use log::info;
use network::energy_manager::EnergyManager;

#[derive(Debug, Clone, Serialize)]
struct EnergyMetricsResponse {
    node_id: String,
    current_consumption: f64,
    total_savings: f64,
    history: Vec<f64>,
    forecast: Option<f64>,
    last_updated: chrono::DateTime<chrono::Utc>,
    active_connections: usize,
}

#[derive(Debug, Clone, Serialize)]
struct NetworkEnergyResponse {
    average_energy_saved: f64,
    node_count: usize,
    nodes: Vec<EnergyMetricsResponse>,
}

struct ApiState {
    energy_manager: Arc<Mutex<EnergyManager>>,
}

async fn get_energy_metrics(state: Arc<ApiState>) -> Result<impl Reply, Rejection> {
    let energy_manager = state.energy_manager.lock().await;
    
    // Simulate nodes
    let node_ids = vec!["node-1", "node-2", "local"];
    let mut nodes = Vec::new();
    let mut average_energy_saved = 0.0;
    
    for node_id in node_ids {
        if let Some(metrics) = energy_manager.get_latest_node_metrics(node_id).await {
            let node_metrics = EnergyMetricsResponse {
                node_id: node_id.to_string(),
                current_consumption: metrics.current_consumption,
                total_savings: metrics.total_savings,
                history: metrics.history.clone(),
                forecast: metrics.forecast,
                last_updated: metrics.last_updated,
                active_connections: metrics.active_connections,
            };
            
            average_energy_saved += metrics.total_savings;
            nodes.push(node_metrics);
        }
    }
    
    if !nodes.is_empty() {
        average_energy_saved /= nodes.len() as f64;
    }
    
    let response = NetworkEnergyResponse {
        average_energy_saved,
        node_count: nodes.len(),
        nodes,
    };
    
    Ok(warp::reply::json(&response))
}

async fn get_total_energy_saved(state: Arc<ApiState>) -> Result<impl Reply, Rejection> {
    let energy_manager = state.energy_manager.lock().await;
    let total_saved = energy_manager.get_total_energy_saved().await;
    
    Ok(warp::reply::json(&serde_json::json!({
        "energy_saved_percent": total_saved,
    })))
}

async fn get_node_status(node_id: String, state: Arc<ApiState>) -> Result<impl Reply, Rejection> {
    let energy_manager = state.energy_manager.lock().await;
    
    let node_metrics = match energy_manager.get_latest_node_metrics(&node_id).await {
        Some(metrics) => EnergyMetricsResponse {
            node_id: node_id.to_string(),
            current_consumption: metrics.current_consumption,
            total_savings: metrics.total_savings,
            history: metrics.history.clone(),
            forecast: metrics.forecast,
            last_updated: metrics.last_updated,
            active_connections: metrics.active_connections,
        },
        None => {
            return Ok(warp::reply::json(&serde_json::json!({
                "error": "Node not found",
                "node_id": node_id
            })));
        }
    };
    
    Ok(warp::reply::json(&node_metrics))
}

#[tokio::main]
async fn main() {
    env_logger::init();
    
    // Initialize the energy manager
    let mut energy_manager = EnergyManager::new()
        .with_sampling_interval(Duration::from_secs(15))
        .with_low_power_threshold(0.7);
    
    // Simulate some metrics
    energy_manager.record_tx_bytes(1500);
    energy_manager.record_rx_bytes(2500);
    
    // Capture metrics for different nodes
    energy_manager.capture_metrics("node-1", 8).await;
    energy_manager.capture_metrics("node-2", 12).await;
    energy_manager.capture_metrics("local", 5).await;
    
    // Setup API state
    let api_state = Arc::new(ApiState {
        energy_manager: Arc::new(Mutex::new(energy_manager)),
    });
    
    // Create API routes
    let api_state_filter = warp::any().map(move || api_state.clone());
    
    // Endpoint: GET /api/metrics/energy
    let energy_route = warp::path!("api" / "metrics" / "energy")
        .and(warp::get())
        .and(api_state_filter.clone())
        .and_then(get_energy_metrics);
    
    // Endpoint: GET /api/metrics/energy/total
    let energy_total_route = warp::path!("api" / "metrics" / "energy" / "total")
        .and(warp::get())
        .and(api_state_filter.clone())
        .and_then(get_total_energy_saved);
    
    // Endpoint: GET /api/node/:id/status
    let node_status_route = warp::path!("api" / "node" / String / "status")
        .and(warp::get())
        .and(api_state_filter.clone())
        .and_then(|id, state| get_node_status(id, state));
    
    // Combine routes
    let routes = energy_route
        .or(energy_total_route)
        .or(node_status_route);
    
    // Start server
    let port = 8081;
    info!("Starting energy API server on port {}...", port);
    warp::serve(routes).run(([0, 0, 0, 0], port)).await;
} use network::energy_manager::EnergyManager;
use std::time::Duration;
use std::sync::Arc;
use warp::{Filter, Rejection, Reply};
use tokio::sync::Mutex;
use serde::Serialize;
use log::{info, debug};
use env_logger;
use chrono;

#[derive(Debug, Clone, Serialize)]
struct EnergyMetricsResponse {
    node_id: String,
    current_consumption: f64,
    total_savings: f64,
    history: Vec<f64>,
    forecast: Option<f64>,
    last_updated: chrono::DateTime<chrono::Utc>,
    active_connections: usize,
}

#[derive(Debug, Clone, Serialize)]
struct NetworkEnergyResponse {
    average_energy_saved: f64,
    node_count: usize,
    nodes: Vec<EnergyMetricsResponse>,
}

struct ApiState {
    energy_manager: Arc<Mutex<EnergyManager>>,
}
